{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "uuid": "f2bd8cf9-2eb4-478d-85a0-f86f2feb36ee"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "    \n",
    "def Precision(y_true, y_pred):\n",
    "    \"\"\"精确率\"\"\"\n",
    "    tp= K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))  # true positives\n",
    "    pp= K.sum(K.round(K.clip(y_pred, 0, 1))) # predicted positives\n",
    "    precision = tp/ (pp+ K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def Recall(y_true, y_pred):\n",
    "    \"\"\"召回率\"\"\"\n",
    "    tp = K.sum(K.round(K.clip(y_true * y_pred, 0, 1))) # true positives\n",
    "    pp = K.sum(K.round(K.clip(y_true, 0, 1))) # possible positives\n",
    "    recall = tp / (pp + K.epsilon())\n",
    "    return recall\n",
    " \n",
    "def F1(y_true, y_pred):\n",
    "    \"\"\"F1-score\"\"\"\n",
    "    precision = Precision(y_true, y_pred)\n",
    "    recall = Recall(y_true, y_pred)\n",
    "    f1 = 2 * ((precision * recall) / (precision + recall + K.epsilon()))\n",
    "    return f1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "uuid": "57057b62-5e9b-4ab4-a778-5f0cf23934d7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:CUDA GPU available, you can set `kashgari.config.use_cudnn_cell = True` to use CuDNNCell. This will speed up the training, but will make model incompatible with CPU device.\n",
      "WARNING:root:\n",
      "╭─────────────────────────────────────────────────────────────────────────╮\n",
      "│ ◎ ○ ○ ░░░░░░░░░░░░░░░░░░░░░  Important Message  ░░░░░░░░░░░░░░░░░░░░░░░░│\n",
      "├─────────────────────────────────────────────────────────────────────────┤\n",
      "│                                                                         │\n",
      "│              We renamed again for consistency and clarity.              │\n",
      "│                   From now on, it is all `kashgari`.                    │\n",
      "│  Changelog: https://github.com/BrikerMan/Kashgari/releases/tag/v1.0.0   │\n",
      "│                                                                         │\n",
      "│         | Backend          | pypi version   | desc           |          │\n",
      "│         | ---------------- | -------------- | -------------- |          │\n",
      "│         | TensorFlow 2.x   | kashgari 2.x.x | coming soon    |          │\n",
      "│         | TensorFlow 1.14+ | kashgari 1.x.x |                |          │\n",
      "│         | Keras            | kashgari 0.x.x | legacy version |          │\n",
      "│                                                                         │\n",
      "╰─────────────────────────────────────────────────────────────────────────╯\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import kashgari\n",
    "from kashgari.tasks.labeling import BiLSTM_CRF_Model\n",
    "from kashgari.embeddings import BERTEmbedding, BareEmbedding\n",
    "from kashgari.embeddings import NumericFeaturesEmbedding, StackedEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "uuid": "940a7c02-e130-442e-b093-f1092bde9bb9"
   },
   "outputs": [],
   "source": [
    "flb2id = {\n",
    "    'O': 1,\n",
    "    'Time': 2,\n",
    "    'Person': 3,\n",
    "    'Location': 4,\n",
    "    'Money': 5,\n",
    "    'Things': 6,\n",
    "    'Count': 7\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "uuid": "a28aa4e2-459e-40d0-9958-2c0d82354f93"
   },
   "outputs": [],
   "source": [
    "trig2id = {\n",
    "    'O': 1,\n",
    "    'Steal': 2,\n",
    "    'Draw': 3,\n",
    "    'Consume': 4,\n",
    "    'Sale': 5,\n",
    "    'Volunteer': 6\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "uuid": "68adf509-b474-45a0-8d61-c3171af6ce9a"
   },
   "outputs": [],
   "source": [
    "# 初始化训练数据\n",
    "train_x = []\n",
    "# train_islb = []\n",
    "train_flb = []\n",
    "# train_istrig = []\n",
    "# train_trig= []\n",
    "train_steal = []\n",
    "train_draw = []\n",
    "train_consume = []\n",
    "train_sale = []\n",
    "train_volun = []\n",
    "train_steal_dis = []\n",
    "train_draw_dis = []\n",
    "train_consume_dis = []\n",
    "train_sale_dis = []\n",
    "train_volun_dis = []\n",
    "train_y = []\n",
    "\n",
    "with open('train_zi.txt', 'r', encoding='utf-8') as f:\n",
    "    tmp_x = []\n",
    "    tmp_flb = []\n",
    "    # tmp_islb = []\n",
    "    tmp_trig = []\n",
    "    # tmp_istrig = []\n",
    "    tmp_steal = []\n",
    "    tmp_draw = []\n",
    "    tmp_consume = []\n",
    "    tmp_sale = []\n",
    "    tmp_volun = []\n",
    "    tmp_steal_dis = []\n",
    "    tmp_draw_dis = []\n",
    "    tmp_consume_dis = []\n",
    "    tmp_sale_dis = []\n",
    "    tmp_volun_dis = []\n",
    "    tmp_yy = []\n",
    "    for i in f.readlines():\n",
    "        if i == '\\n':\n",
    "            train_x.append(tmp_x)\n",
    "            train_flb.append(tmp_flb)\n",
    "            # train_islb.append(tmp_islb)\n",
    "            # train_trig.append(tmp_trig)\n",
    "            # train_istrig.append(tmp_istrig)\n",
    "            train_y.append(tmp_yy)\n",
    "            # 处理事件标签\n",
    "            if 2 in tmp_trig:\n",
    "                idx = tmp_trig.index(2)\n",
    "                cnt = 0\n",
    "                for i in tmp_flb:\n",
    "                    if i != 1:\n",
    "                        tmp_steal.append(2)\n",
    "                    else:\n",
    "                        tmp_steal.append(1)\n",
    "                    tmp_steal_dis.append(abs(idx-cnt)+1)\n",
    "                    cnt = cnt + 1\n",
    "            else:\n",
    "                for i in tmp_flb:\n",
    "                    tmp_steal.append(1)\n",
    "                    tmp_steal_dis.append(256)\n",
    "            train_steal.append(tmp_steal)\n",
    "            train_steal_dis.append(tmp_steal_dis)\n",
    "            if 3 in tmp_trig:\n",
    "                idx = tmp_trig.index(3)\n",
    "                cnt = 0\n",
    "                for i in tmp_flb:\n",
    "                    if i != 1:\n",
    "                        tmp_draw.append(2)\n",
    "                    else:\n",
    "                        tmp_draw.append(1)\n",
    "                    tmp_draw_dis.append(abs(idx-cnt)+1)\n",
    "                    cnt = cnt + 1\n",
    "            else:\n",
    "                for i in tmp_flb:\n",
    "                    tmp_draw.append(1)\n",
    "                    tmp_draw_dis.append(256)\n",
    "            train_draw.append(tmp_draw)\n",
    "            train_draw_dis.append(tmp_draw_dis)\n",
    "            if 4 in tmp_trig:\n",
    "                idx = tmp_trig.index(4)\n",
    "                cnt = 0\n",
    "                for i in tmp_flb:\n",
    "                    if i != 1:\n",
    "                        tmp_consume.append(2)\n",
    "                    else:\n",
    "                        tmp_consume.append(1)\n",
    "                    tmp_consume_dis.append(abs(idx-cnt)+1)\n",
    "                    cnt = cnt + 1\n",
    "            else:\n",
    "                for i in tmp_flb:\n",
    "                    tmp_consume.append(1)\n",
    "                    tmp_consume_dis.append(256)\n",
    "            train_consume.append(tmp_consume)\n",
    "            train_consume_dis.append(tmp_consume_dis)\n",
    "            if 5 in tmp_trig:\n",
    "                idx = tmp_trig.index(5)\n",
    "                cnt = 0\n",
    "                for i in tmp_flb:\n",
    "                    if i != 1:\n",
    "                        tmp_sale.append(2)\n",
    "                    else:\n",
    "                        tmp_sale.append(1)\n",
    "                    tmp_sale_dis.append(abs(idx-cnt)+1)\n",
    "                    cnt = cnt + 1\n",
    "            else:\n",
    "                for i in tmp_flb:\n",
    "                    tmp_sale.append(1)\n",
    "                    tmp_sale_dis.append(256)\n",
    "            train_sale.append(tmp_sale)\n",
    "            train_sale_dis.append(tmp_sale_dis)\n",
    "            if 6 in tmp_trig:\n",
    "                idx = tmp_trig.index(6)\n",
    "                cnt = 0\n",
    "                for i in tmp_flb:\n",
    "                    if i != 1:\n",
    "                        tmp_volun.append(2)\n",
    "                    else:\n",
    "                        tmp_volun.append(1)\n",
    "                    tmp_volun_dis.append(abs(idx-cnt)+1)\n",
    "                    cnt = cnt + 1\n",
    "            else:\n",
    "                for i in tmp_flb:\n",
    "                    tmp_volun.append(1)\n",
    "                    tmp_volun_dis.append(256)\n",
    "            train_volun.append(tmp_volun)\n",
    "            train_volun_dis.append(tmp_volun_dis)\n",
    "            tmp_x = []\n",
    "            tmp_flb = []\n",
    "            # tmp_islb = []\n",
    "            tmp_trig = []\n",
    "            # tmp_istrig = []\n",
    "            tmp_steal = []\n",
    "            tmp_draw = []\n",
    "            tmp_consume = []\n",
    "            tmp_sale = []\n",
    "            tmp_volun = []\n",
    "            tmp_steal_dis = []\n",
    "            tmp_draw_dis = []\n",
    "            tmp_consume_dis = []\n",
    "            tmp_sale_dis = []\n",
    "            tmp_volun_dis = []\n",
    "            tmp_yy = []\n",
    "        else:\n",
    "            x, y = i.split(' ')\n",
    "            tmp_x.append(x)\n",
    "            tmp_list = y.split('-')\n",
    "            if len(tmp_list) == 1:\n",
    "                tmp_flb.append(1)\n",
    "                # tmp_islb.append(1)\n",
    "                tmp_trig.append(1)\n",
    "                # tmp_istrig.append(1)\n",
    "                tmp_yy.append(tmp_list[0].replace('\\n', ''))\n",
    "            elif len(tmp_list) == 2:\n",
    "                # tmp_str = tmp_list[0] + '-' + tmp_list[1].replace('\\n', '')\n",
    "                tmp_flb.append(1)\n",
    "                # tmp_islb.append(1)\n",
    "                tmp_trig.append(trig2id[tmp_list[1].replace('\\n', '')])\n",
    "                # tmp_istrig.append(2)\n",
    "                tmp_yy.append('O')\n",
    "                # 处理label\n",
    "            else:\n",
    "                tmp_str = tmp_list[0] + '-' + tmp_list[1].replace('\\n', '')\n",
    "                tmp_flb.append(flb2id[tmp_list[1].replace('\\n', '')])\n",
    "                # tmp_islb.append(2)\n",
    "                tmp_trig.append(1)\n",
    "                # tmp_istrig.append(1)\n",
    "                if len(tmp_list) == 3:\n",
    "                    tmp_str = tmp_list[0] + '-' + tmp_list[1] +\\\n",
    "                    tmp_list[2].replace('\\n', '')\n",
    "                    tmp_yy.append(tmp_str)\n",
    "                elif len(tmp_list) == 4:\n",
    "                    tmp_str = tmp_list[0] + '-' + tmp_list[3].replace('\\n', '') +\\\n",
    "                    tmp_list[2]\n",
    "                    tmp_yy.append(tmp_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "uuid": "96ffb949-6240-45fc-8690-11653329244c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['3', '、', '2', '0', '1', '3', '年', '1', '2', '月', '1', '0', '日', '下', '午', '，', '被', '告', '人', '卿', '某', '某', '在', '江', '津', '区', '双', '福', '群', '光', '电', '子', '厂', '宿', '舍', 'X', '—', 'X', 'X', '号', '寝', '室', '内', '，', '盗', '走', '同', '寝', '室', '居', '住', '的', '凌', '某', '的', '联', '想', '牌', '笔', '记', '本', '电', '脑', '一', '台', '。']\n",
      "[1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 3, 3, 3, 1, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 3, 1, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 1]\n",
      "[1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[45, 44, 43, 42, 41, 40, 39, 38, 37, 36, 35, 34, 33, 32, 31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 18, 17, 16, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22]\n",
      "[256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256]\n",
      "[256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256]\n",
      "[256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256]\n",
      "[256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256]\n",
      "['O', 'O', 'B-TimeSteal', 'I-TimeSteal', 'I-TimeSteal', 'I-TimeSteal', 'I-TimeSteal', 'I-TimeSteal', 'I-TimeSteal', 'I-TimeSteal', 'I-TimeSteal', 'I-TimeSteal', 'I-TimeSteal', 'I-TimeSteal', 'I-TimeSteal', 'O', 'O', 'O', 'O', 'B-ThiefSteal', 'I-ThiefSteal', 'I-ThiefSteal', 'O', 'B-LocationSteal', 'I-LocationSteal', 'I-LocationSteal', 'I-LocationSteal', 'I-LocationSteal', 'I-LocationSteal', 'I-LocationSteal', 'I-LocationSteal', 'I-LocationSteal', 'I-LocationSteal', 'I-LocationSteal', 'I-LocationSteal', 'I-LocationSteal', 'I-LocationSteal', 'I-LocationSteal', 'I-LocationSteal', 'I-LocationSteal', 'I-LocationSteal', 'I-LocationSteal', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-VictimSteal', 'I-VictimSteal', 'O', 'B-ThingsSteal', 'I-ThingsSteal', 'I-ThingsSteal', 'I-ThingsSteal', 'I-ThingsSteal', 'I-ThingsSteal', 'I-ThingsSteal', 'I-ThingsSteal', 'B-CountSteal', 'I-CountSteal', 'O']\n"
     ]
    }
   ],
   "source": [
    "print(train_x[1110])\n",
    "print(train_flb[1110])\n",
    "print(train_steal[1110])\n",
    "print(train_draw[1110])\n",
    "print(train_consume[1110])\n",
    "print(train_sale[1110])\n",
    "print(train_volun[1110])\n",
    "print(train_steal_dis[1110])\n",
    "print(train_draw_dis[1110])\n",
    "print(train_consume_dis[1110])\n",
    "print(train_sale_dis[1110])\n",
    "print(train_volun_dis[1110])\n",
    "print(train_y[1110])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "uuid": "48164dc0-75e9-4a6e-be88-f35bc462fa7d"
   },
   "outputs": [],
   "source": [
    "# 初始化测试数据\n",
    "test_x = []\n",
    "# train_islb = []\n",
    "test_flb = []\n",
    "# train_istrig = []\n",
    "# train_trig= []\n",
    "test_steal = []\n",
    "test_draw = []\n",
    "test_consume = []\n",
    "test_sale = []\n",
    "test_volun = []\n",
    "test_steal_dis = []\n",
    "test_draw_dis = []\n",
    "test_consume_dis = []\n",
    "test_sale_dis = []\n",
    "test_volun_dis = []\n",
    "test_y = []\n",
    "\n",
    "with open('test_zi.txt', 'r', encoding='utf-8') as f:\n",
    "    tmp_x = []\n",
    "    tmp_flb = []\n",
    "    # tmp_islb = []\n",
    "    tmp_trig = []\n",
    "    # tmp_istrig = []\n",
    "    tmp_steal = []\n",
    "    tmp_draw = []\n",
    "    tmp_consume = []\n",
    "    tmp_sale = []\n",
    "    tmp_volun = []\n",
    "    tmp_steal_dis = []\n",
    "    tmp_draw_dis = []\n",
    "    tmp_consume_dis = []\n",
    "    tmp_sale_dis = []\n",
    "    tmp_volun_dis = []\n",
    "    tmp_yy = []\n",
    "    for i in f.readlines():\n",
    "        if i == '\\n':\n",
    "            test_x.append(tmp_x)\n",
    "            test_flb.append(tmp_flb)\n",
    "            # train_islb.append(tmp_islb)\n",
    "            # train_trig.append(tmp_trig)\n",
    "            # train_istrig.append(tmp_istrig)\n",
    "            test_y.append(tmp_yy)\n",
    "            # 处理事件标签\n",
    "            if 2 in tmp_trig:\n",
    "                idx = tmp_trig.index(2)\n",
    "                cnt = 0\n",
    "                for i in tmp_flb:\n",
    "                    if i != 1:\n",
    "                        tmp_steal.append(2)\n",
    "                    else:\n",
    "                        tmp_steal.append(1)\n",
    "                    tmp_steal_dis.append(abs(idx-cnt)+1)\n",
    "                    cnt = cnt + 1\n",
    "            else:\n",
    "                for i in tmp_flb:\n",
    "                    tmp_steal.append(1)\n",
    "                    tmp_steal_dis.append(256)\n",
    "            test_steal.append(tmp_steal)\n",
    "            test_steal_dis.append(tmp_steal_dis)\n",
    "            if 3 in tmp_trig:\n",
    "                idx = tmp_trig.index(3)\n",
    "                cnt = 0\n",
    "                for i in tmp_flb:\n",
    "                    if i != 1:\n",
    "                        tmp_draw.append(2)\n",
    "                    else:\n",
    "                        tmp_draw.append(1)\n",
    "                    tmp_draw_dis.append(abs(idx-cnt)+1)\n",
    "                    cnt = cnt + 1\n",
    "            else:\n",
    "                for i in tmp_flb:\n",
    "                    tmp_draw.append(1)\n",
    "                    tmp_draw_dis.append(256)\n",
    "            test_draw.append(tmp_draw)\n",
    "            test_draw_dis.append(tmp_draw_dis)\n",
    "            if 4 in tmp_trig:\n",
    "                idx = tmp_trig.index(4)\n",
    "                cnt = 0\n",
    "                for i in tmp_flb:\n",
    "                    if i != 1:\n",
    "                        tmp_consume.append(2)\n",
    "                    else:\n",
    "                        tmp_consume.append(1)\n",
    "                    tmp_consume_dis.append(abs(idx-cnt)+1)\n",
    "                    cnt = cnt + 1\n",
    "            else:\n",
    "                for i in tmp_flb:\n",
    "                    tmp_consume.append(1)\n",
    "                    tmp_consume_dis.append(256)\n",
    "            test_consume.append(tmp_consume)\n",
    "            test_consume_dis.append(tmp_consume_dis)\n",
    "            if 5 in tmp_trig:\n",
    "                idx = tmp_trig.index(5)\n",
    "                cnt = 0\n",
    "                for i in tmp_flb:\n",
    "                    if i != 1:\n",
    "                        tmp_sale.append(2)\n",
    "                    else:\n",
    "                        tmp_sale.append(1)\n",
    "                    tmp_sale_dis.append(abs(idx-cnt)+1)\n",
    "                    cnt = cnt + 1\n",
    "            else:\n",
    "                for i in tmp_flb:\n",
    "                    tmp_sale.append(1)\n",
    "                    tmp_sale_dis.append(256)\n",
    "            test_sale.append(tmp_sale)\n",
    "            test_sale_dis.append(tmp_sale_dis)\n",
    "            if 6 in tmp_trig:\n",
    "                idx = tmp_trig.index(6)\n",
    "                cnt = 0\n",
    "                for i in tmp_flb:\n",
    "                    if i != 1:\n",
    "                        tmp_volun.append(2)\n",
    "                    else:\n",
    "                        tmp_volun.append(1)\n",
    "                    tmp_volun_dis.append(abs(idx-cnt)+1)\n",
    "                    cnt = cnt + 1\n",
    "            else:\n",
    "                for i in tmp_flb:\n",
    "                    tmp_volun.append(1)\n",
    "                    tmp_volun_dis.append(256)\n",
    "            test_volun.append(tmp_volun)\n",
    "            test_volun_dis.append(tmp_volun_dis)\n",
    "            tmp_x = []\n",
    "            tmp_flb = []\n",
    "            # tmp_islb = []\n",
    "            tmp_trig = []\n",
    "            # tmp_istrig = []\n",
    "            tmp_steal = []\n",
    "            tmp_draw = []\n",
    "            tmp_consume = []\n",
    "            tmp_sale = []\n",
    "            tmp_volun = []\n",
    "            tmp_steal_dis = []\n",
    "            tmp_draw_dis = []\n",
    "            tmp_consume_dis = []\n",
    "            tmp_sale_dis = []\n",
    "            tmp_volun_dis = []\n",
    "            tmp_yy = []\n",
    "        else:\n",
    "            x, y = i.split(' ')\n",
    "            tmp_x.append(x)\n",
    "            tmp_list = y.split('-')\n",
    "            if len(tmp_list) == 1:\n",
    "                tmp_flb.append(1)\n",
    "                # tmp_islb.append(1)\n",
    "                tmp_trig.append(1)\n",
    "                # tmp_istrig.append(1)\n",
    "                tmp_yy.append(tmp_list[0].replace('\\n', ''))\n",
    "            elif len(tmp_list) == 2:\n",
    "                # tmp_str = tmp_list[0] + '-' + tmp_list[1].replace('\\n', '')\n",
    "                tmp_flb.append(1)\n",
    "                # tmp_islb.append(1)\n",
    "                tmp_trig.append(trig2id[tmp_list[1].replace('\\n', '')])\n",
    "                # tmp_istrig.append(2)\n",
    "                tmp_yy.append('O')\n",
    "                # 处理label\n",
    "            else:\n",
    "                tmp_str = tmp_list[0] + '-' + tmp_list[1].replace('\\n', '')\n",
    "                tmp_flb.append(flb2id[tmp_list[1].replace('\\n', '')])\n",
    "                # tmp_islb.append(2)\n",
    "                tmp_trig.append(1)\n",
    "                # tmp_istrig.append(1)\n",
    "                if len(tmp_list) == 3:\n",
    "                    tmp_str = tmp_list[0] + '-' + tmp_list[1] +\\\n",
    "                    tmp_list[2].replace('\\n', '')\n",
    "                    tmp_yy.append(tmp_str)\n",
    "                elif len(tmp_list) == 4:\n",
    "                    tmp_str = tmp_list[0] + '-' + tmp_list[3].replace('\\n', '') +\\\n",
    "                    tmp_list[2]\n",
    "                    tmp_yy.append(tmp_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "uuid": "29203e89-d469-455a-946d-6399677e7467"
   },
   "outputs": [],
   "source": [
    "# 初始化验证数据\n",
    "dev_x = []\n",
    "# train_islb = []\n",
    "dev_flb = []\n",
    "# train_istrig = []\n",
    "# train_trig= []\n",
    "dev_steal = []\n",
    "dev_draw = []\n",
    "dev_consume = []\n",
    "dev_sale = []\n",
    "dev_volun = []\n",
    "dev_steal_dis = []\n",
    "dev_draw_dis = []\n",
    "dev_consume_dis = []\n",
    "dev_sale_dis = []\n",
    "dev_volun_dis = []\n",
    "dev_y = []\n",
    "\n",
    "with open('dev_zi.txt', 'r', encoding='utf-8') as f:\n",
    "    tmp_x = []\n",
    "    tmp_flb = []\n",
    "    # tmp_islb = []\n",
    "    tmp_trig = []\n",
    "    # tmp_istrig = []\n",
    "    tmp_steal = []\n",
    "    tmp_draw = []\n",
    "    tmp_consume = []\n",
    "    tmp_sale = []\n",
    "    tmp_volun = []\n",
    "    tmp_steal_dis = []\n",
    "    tmp_draw_dis = []\n",
    "    tmp_consume_dis = []\n",
    "    tmp_sale_dis = []\n",
    "    tmp_volun_dis = []\n",
    "    tmp_yy = []\n",
    "    for i in f.readlines():\n",
    "        if i == '\\n':\n",
    "            dev_x.append(tmp_x)\n",
    "            dev_flb.append(tmp_flb)\n",
    "            # train_islb.append(tmp_islb)\n",
    "            # train_trig.append(tmp_trig)\n",
    "            # train_istrig.append(tmp_istrig)\n",
    "            dev_y.append(tmp_yy)\n",
    "            # 处理事件标签\n",
    "            if 2 in tmp_trig:\n",
    "                idx = tmp_trig.index(2)\n",
    "                cnt = 0\n",
    "                for i in tmp_flb:\n",
    "                    if i != 1:\n",
    "                        tmp_steal.append(2)\n",
    "                    else:\n",
    "                        tmp_steal.append(1)\n",
    "                    tmp_steal_dis.append(abs(idx-cnt)+1)\n",
    "                    cnt = cnt + 1\n",
    "            else:\n",
    "                for i in tmp_flb:\n",
    "                    tmp_steal.append(1)\n",
    "                    tmp_steal_dis.append(256)\n",
    "            dev_steal.append(tmp_steal)\n",
    "            dev_steal_dis.append(tmp_steal_dis)\n",
    "            if 3 in tmp_trig:\n",
    "                idx = tmp_trig.index(3)\n",
    "                cnt = 0\n",
    "                for i in tmp_flb:\n",
    "                    if i != 1:\n",
    "                        tmp_draw.append(2)\n",
    "                    else:\n",
    "                        tmp_draw.append(1)\n",
    "                    tmp_draw_dis.append(abs(idx-cnt)+1)\n",
    "                    cnt = cnt + 1\n",
    "            else:\n",
    "                for i in tmp_flb:\n",
    "                    tmp_draw.append(1)\n",
    "                    tmp_draw_dis.append(256)\n",
    "            dev_draw.append(tmp_draw)\n",
    "            dev_draw_dis.append(tmp_draw_dis)\n",
    "            if 4 in tmp_trig:\n",
    "                idx = tmp_trig.index(4)\n",
    "                cnt = 0\n",
    "                for i in tmp_flb:\n",
    "                    if i != 1:\n",
    "                        tmp_consume.append(2)\n",
    "                    else:\n",
    "                        tmp_consume.append(1)\n",
    "                    tmp_consume_dis.append(abs(idx-cnt)+1)\n",
    "                    cnt = cnt + 1\n",
    "            else:\n",
    "                for i in tmp_flb:\n",
    "                    tmp_consume.append(1)\n",
    "                    tmp_consume_dis.append(256)\n",
    "            dev_consume.append(tmp_consume)\n",
    "            dev_consume_dis.append(tmp_consume_dis)\n",
    "            if 5 in tmp_trig:\n",
    "                idx = tmp_trig.index(5)\n",
    "                cnt = 0\n",
    "                for i in tmp_flb:\n",
    "                    if i != 1:\n",
    "                        tmp_sale.append(2)\n",
    "                    else:\n",
    "                        tmp_sale.append(1)\n",
    "                    tmp_sale_dis.append(abs(idx-cnt)+1)\n",
    "                    cnt = cnt + 1\n",
    "            else:\n",
    "                for i in tmp_flb:\n",
    "                    tmp_sale.append(1)\n",
    "                    tmp_sale_dis.append(256)\n",
    "            dev_sale.append(tmp_sale)\n",
    "            dev_sale_dis.append(tmp_sale_dis)\n",
    "            if 6 in tmp_trig:\n",
    "                idx = tmp_trig.index(6)\n",
    "                cnt = 0\n",
    "                for i in tmp_flb:\n",
    "                    if i != 1:\n",
    "                        tmp_volun.append(2)\n",
    "                    else:\n",
    "                        tmp_volun.append(1)\n",
    "                    tmp_volun_dis.append(abs(idx-cnt)+1)\n",
    "                    cnt = cnt + 1\n",
    "            else:\n",
    "                for i in tmp_flb:\n",
    "                    tmp_volun.append(1)\n",
    "                    tmp_volun_dis.append(256)\n",
    "            dev_volun.append(tmp_volun)\n",
    "            dev_volun_dis.append(tmp_volun_dis)\n",
    "            tmp_x = []\n",
    "            tmp_flb = []\n",
    "            # tmp_islb = []\n",
    "            tmp_trig = []\n",
    "            # tmp_istrig = []\n",
    "            tmp_steal = []\n",
    "            tmp_draw = []\n",
    "            tmp_consume = []\n",
    "            tmp_sale = []\n",
    "            tmp_volun = []\n",
    "            tmp_steal_dis = []\n",
    "            tmp_draw_dis = []\n",
    "            tmp_consume_dis = []\n",
    "            tmp_sale_dis = []\n",
    "            tmp_volun_dis = []\n",
    "            tmp_yy = []\n",
    "        else:\n",
    "            x, y = i.split(' ')\n",
    "            tmp_x.append(x)\n",
    "            tmp_list = y.split('-')\n",
    "            if len(tmp_list) == 1:\n",
    "                tmp_flb.append(1)\n",
    "                # tmp_islb.append(1)\n",
    "                tmp_trig.append(1)\n",
    "                # tmp_istrig.append(1)\n",
    "                tmp_yy.append(tmp_list[0].replace('\\n', ''))\n",
    "            elif len(tmp_list) == 2:\n",
    "                # tmp_str = tmp_list[0] + '-' + tmp_list[1].replace('\\n', '')\n",
    "                tmp_flb.append(1)\n",
    "                # tmp_islb.append(1)\n",
    "                tmp_trig.append(trig2id[tmp_list[1].replace('\\n', '')])\n",
    "                # tmp_istrig.append(2)\n",
    "                tmp_yy.append('O')\n",
    "                # 处理label\n",
    "            else:\n",
    "                tmp_str = tmp_list[0] + '-' + tmp_list[1].replace('\\n', '')\n",
    "                tmp_flb.append(flb2id[tmp_list[1].replace('\\n', '')])\n",
    "                # tmp_islb.append(2)\n",
    "                tmp_trig.append(1)\n",
    "                # tmp_istrig.append(1)\n",
    "                if len(tmp_list) == 3:\n",
    "                    tmp_str = tmp_list[0] + '-' + tmp_list[1] +\\\n",
    "                    tmp_list[2].replace('\\n', '')\n",
    "                    tmp_yy.append(tmp_str)\n",
    "                elif len(tmp_list) == 4:\n",
    "                    tmp_str = tmp_list[0] + '-' + tmp_list[3].replace('\\n', '') +\\\n",
    "                    tmp_list[2]\n",
    "                    tmp_yy.append(tmp_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "uuid": "e66dfe39-09bd-496a-99e5-822aec57fd8d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7fee49165320>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7fee49165320>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7fee4b1f24e0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7fee4b1f24e0>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fee4b0c75c0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fee4b0c75c0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fee4b173b38>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fee4b173b38>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fee49d53c18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fee49d53c18>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fee49d451d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fee49d451d0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fee49def1d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fee49def1d0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fee49b637b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fee49b637b8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fee49a4d7f0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fee49a4d7f0>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fee4988e6a0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fee4988e6a0>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fee499f8fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fee499f8fd0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fee498d9b00>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fee498d9b00>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fee49916e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fee49916e10>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fee495f99e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fee495f99e8>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fee49414d30>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fee49414d30>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fee4954a978>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fee4954a978>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fee49469c88>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fee49469c88>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fee45acd0b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fee45acd0b8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fee45b24550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fee45b24550>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fee4591deb8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fee4591deb8>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fee45a147f0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fee45a147f0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fee45962c88>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fee45962c88>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fee45771ef0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fee45771ef0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fee45604978>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fee45604978>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fee45457ba8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fee45457ba8>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fee45676898>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fee45676898>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fee454b9320>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fee454b9320>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fee4553e9b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fee4553e9b0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fee4518e080>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fee4518e080>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fee44ff7c50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fee44ff7c50>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fee4510fba8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fee4510fba8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fee44dfa5c0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fee44dfa5c0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fee44fdf710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fee44fdf710>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fee44cc5198>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fee44cc5198>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fee44b36550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fee44b36550>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fee49bc7320>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fee49bc7320>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fee49affeb8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fee49affeb8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fee49e42eb8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fee49e42eb8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fee44825e48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fee44825e48>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fee446639e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fee446639e8>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fee44797438>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fee44797438>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fee444bb710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fee444bb710>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fee44418860>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fee44418860>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fee443ade80>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fee443ade80>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fee4419f588>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fee4419f588>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fee4418d0f0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fee4418d0f0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fee441e89e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fee441e89e8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fee44226f28>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fee44226f28>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fee43e924e0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fee43e924e0>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fee43ce2e80>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fee43ce2e80>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fee43efdbe0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fee43efdbe0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fee43cc1128>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fee43cc1128>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fee43ab3400>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fee43ab3400>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fee43a36518>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fee43a36518>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fee438039b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fee438039b0>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fee43a13c88>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fee43a13c88>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fee4360a4a8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fee4360a4a8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fee4354a2e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fee4354a2e8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fee4354a4a8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fee4354a4a8>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fee433bc1d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fee433bc1d0>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fee435cc860>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fee435cc860>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fee433a52b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fee433a52b0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fee4494b4a8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fee4494b4a8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:seq_len: 256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method NonMaskingLayer.call of <kashgari.layers.non_masking_layer.NonMaskingLayer object at 0x7fee3857cd30>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method NonMaskingLayer.call of <kashgari.layers.non_masking_layer.NonMaskingLayer object at 0x7fee3857cd30>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    }
   ],
   "source": [
    "bert_embed = BERTEmbedding('chinese_L-12_H-768_A-12',\n",
    "                           task=kashgari.LABELING,\n",
    "                           sequence_length=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "uuid": "c0ac7d2d-76fd-4a38-8994-7f397db42d61"
   },
   "outputs": [],
   "source": [
    "text_embedding = BareEmbedding(task=kashgari.LABELING,\n",
    "                               sequence_length=256)\n",
    "flb_embedding = NumericFeaturesEmbedding(feature_count=7,\n",
    "                                         feature_name='flb',\n",
    "                                         sequence_length=256)\n",
    "\n",
    "islb_embedding = NumericFeaturesEmbedding(feature_count=2,\n",
    "                                         feature_name='islb',\n",
    "                                         sequence_length=256)\n",
    "\n",
    "trig_embedding = NumericFeaturesEmbedding(feature_count=6,\n",
    "                                         feature_name='trig',\n",
    "                                         sequence_length=256)\n",
    "\n",
    "istrig_embedding = NumericFeaturesEmbedding(feature_count=2,\n",
    "                                         feature_name='istrig',\n",
    "                                         sequence_length=256)\n",
    "\n",
    "steal_embedding = NumericFeaturesEmbedding(feature_count=2,\n",
    "                                         feature_name='steal',\n",
    "                                         sequence_length=256)\n",
    "\n",
    "draw_embedding = NumericFeaturesEmbedding(feature_count=2,\n",
    "                                         feature_name='draw',\n",
    "                                         sequence_length=256)\n",
    "\n",
    "consume_embedding = NumericFeaturesEmbedding(feature_count=2,\n",
    "                                         feature_name='consume',\n",
    "                                         sequence_length=256)\n",
    "\n",
    "sale_embedding = NumericFeaturesEmbedding(feature_count=2,\n",
    "                                         feature_name='sale',\n",
    "                                         sequence_length=256)\n",
    "\n",
    "volun_embedding = NumericFeaturesEmbedding(feature_count=2,\n",
    "                                         feature_name='volun',\n",
    "                                         sequence_length=256)\n",
    "\n",
    "steal_dis_embedding = NumericFeaturesEmbedding(feature_count=256,\n",
    "                                         feature_name='steal_dis',\n",
    "                                         sequence_length=256)\n",
    "\n",
    "draw_dis_embedding = NumericFeaturesEmbedding(feature_count=256,\n",
    "                                         feature_name='draw_dis',\n",
    "                                         sequence_length=256)\n",
    "\n",
    "consume_dis_embedding = NumericFeaturesEmbedding(feature_count=256,\n",
    "                                         feature_name='consume_dis',\n",
    "                                         sequence_length=256)\n",
    "\n",
    "sale_dis_embedding = NumericFeaturesEmbedding(feature_count=256,\n",
    "                                         feature_name='sale_dis',\n",
    "                                         sequence_length=256)\n",
    "\n",
    "volun_dis_embedding = NumericFeaturesEmbedding(feature_count=256,\n",
    "                                         feature_name='volun_dis',\n",
    "                                         sequence_length=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "uuid": "4a9498b2-dc86-4f7d-a09f-d911c3d76d31"
   },
   "outputs": [],
   "source": [
    "stack_embedding = StackedEmbedding([\n",
    "    bert_embed,\n",
    "    flb_embedding,\n",
    "    steal_embedding,\n",
    "    draw_embedding,\n",
    "    consume_embedding,\n",
    "    sale_embedding,\n",
    "    volun_embedding,\n",
    "    steal_dis_embedding,\n",
    "    draw_dis_embedding,\n",
    "    consume_dis_embedding,\n",
    "    sale_dis_embedding,\n",
    "    volun_dis_embedding\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "uuid": "3a142a52-acda-4efd-be13-17e1825776e4"
   },
   "outputs": [],
   "source": [
    "train_fx = (train_x, train_flb, train_steal, train_draw, train_consume, train_sale, train_volun,\n",
    "            train_steal_dis, train_draw_dis, train_consume_dis, train_sale_dis, train_volun_dis)\n",
    "test_fx = (test_x, test_flb, test_steal, test_draw, test_consume, test_sale, test_volun,\n",
    "           test_steal_dis, test_draw_dis, test_consume_dis, test_sale_dis, test_volun_dis)\n",
    "dev_fx = (dev_x, dev_flb, dev_steal, dev_draw, dev_consume, dev_sale, dev_volun,\n",
    "          dev_steal_dis, dev_draw_dis, train_consume_dis, train_sale_dis, train_volun_dis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "uuid": "c880c653-df96-4863-9339-8af1456b97d9"
   },
   "outputs": [],
   "source": [
    "# stack_embedding.analyze_corpus(train_fx, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "uuid": "5a7666e0-ce1c-4347-b0d6-9cab74412eba"
   },
   "outputs": [],
   "source": [
    "ten_fold_x = train_x + dev_x\n",
    "ten_fold_flb = train_flb + dev_flb\n",
    "ten_fold_steal = train_steal + dev_steal\n",
    "ten_fold_draw = train_draw + dev_draw\n",
    "ten_fold_consume = train_consume + dev_consume\n",
    "ten_fold_sale = train_sale + dev_sale\n",
    "ten_fold_volun = train_volun + dev_volun\n",
    "ten_fold_steal_dis = train_steal_dis + dev_steal_dis\n",
    "ten_fold_draw_dis = train_draw_dis + dev_draw_dis\n",
    "ten_fold_consume_dis = train_consume_dis + dev_consume_dis\n",
    "ten_fold_sale_dis = train_sale_dis + dev_sale_dis\n",
    "ten_fold_volun_dis = train_volun_dis + dev_volun_dis\n",
    "ten_fold_y = train_y + dev_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "uuid": "bc3242ca-fb4c-41b7-801d-4aab48bf24ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5171\n"
     ]
    }
   ],
   "source": [
    "print(len(ten_fold_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "uuid": "0695535f-576a-487b-af61-8aea447ba650"
   },
   "outputs": [],
   "source": [
    "model = BiLSTM_CRF_Model(embedding=stack_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "uuid": "72b83b77-603c-475b-989f-46b3035784b8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:CuDNN enabled, this will speed up the training, but will make model incompatible with CPU device.\n"
     ]
    }
   ],
   "source": [
    "kashgari.config.use_cudnn_cell = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "uuid": "3da21f0c-d72c-4b2e-a199-3e363f294e4c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method CRF.call of <kashgari.layers.crf.CRF object at 0x7fee3826a080>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method CRF.call of <kashgari.layers.crf.CRF object at 0x7fee3826a080>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "Model: \"model_19\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Input-Token (InputLayer)        [(None, 256)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Input-Segment (InputLayer)      [(None, 256)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Token (TokenEmbedding [(None, 256, 768), ( 16226304    Input-Token[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Segment (Embedding)   (None, 256, 768)     1536        Input-Segment[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Token-Segment (Add)   (None, 256, 768)     0           Embedding-Token[0][0]            \n",
      "                                                                 Embedding-Segment[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Position (PositionEmb (None, 256, 768)     196608      Embedding-Token-Segment[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Dropout (Dropout)     (None, 256, 768)     0           Embedding-Position[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Norm (LayerNormalizat (None, 256, 768)     1536        Embedding-Dropout[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-MultiHeadSelfAttentio (None, 256, 768)     2362368     Embedding-Norm[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-MultiHeadSelfAttentio (None, 256, 768)     0           Encoder-1-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-MultiHeadSelfAttentio (None, 256, 768)     0           Embedding-Norm[0][0]             \n",
      "                                                                 Encoder-1-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-MultiHeadSelfAttentio (None, 256, 768)     1536        Encoder-1-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-FeedForward (FeedForw (None, 256, 768)     4722432     Encoder-1-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-FeedForward-Dropout ( (None, 256, 768)     0           Encoder-1-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-FeedForward-Add (Add) (None, 256, 768)     0           Encoder-1-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-1-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-FeedForward-Norm (Lay (None, 256, 768)     1536        Encoder-1-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-MultiHeadSelfAttentio (None, 256, 768)     2362368     Encoder-1-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-MultiHeadSelfAttentio (None, 256, 768)     0           Encoder-2-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-MultiHeadSelfAttentio (None, 256, 768)     0           Encoder-1-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-2-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-MultiHeadSelfAttentio (None, 256, 768)     1536        Encoder-2-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-FeedForward (FeedForw (None, 256, 768)     4722432     Encoder-2-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-FeedForward-Dropout ( (None, 256, 768)     0           Encoder-2-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-FeedForward-Add (Add) (None, 256, 768)     0           Encoder-2-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-2-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-FeedForward-Norm (Lay (None, 256, 768)     1536        Encoder-2-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-MultiHeadSelfAttentio (None, 256, 768)     2362368     Encoder-2-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-MultiHeadSelfAttentio (None, 256, 768)     0           Encoder-3-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-MultiHeadSelfAttentio (None, 256, 768)     0           Encoder-2-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-3-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-MultiHeadSelfAttentio (None, 256, 768)     1536        Encoder-3-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-FeedForward (FeedForw (None, 256, 768)     4722432     Encoder-3-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-FeedForward-Dropout ( (None, 256, 768)     0           Encoder-3-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-FeedForward-Add (Add) (None, 256, 768)     0           Encoder-3-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-3-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-FeedForward-Norm (Lay (None, 256, 768)     1536        Encoder-3-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-MultiHeadSelfAttentio (None, 256, 768)     2362368     Encoder-3-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-MultiHeadSelfAttentio (None, 256, 768)     0           Encoder-4-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-MultiHeadSelfAttentio (None, 256, 768)     0           Encoder-3-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-4-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-MultiHeadSelfAttentio (None, 256, 768)     1536        Encoder-4-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-FeedForward (FeedForw (None, 256, 768)     4722432     Encoder-4-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-FeedForward-Dropout ( (None, 256, 768)     0           Encoder-4-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-FeedForward-Add (Add) (None, 256, 768)     0           Encoder-4-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-4-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-FeedForward-Norm (Lay (None, 256, 768)     1536        Encoder-4-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-MultiHeadSelfAttentio (None, 256, 768)     2362368     Encoder-4-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-MultiHeadSelfAttentio (None, 256, 768)     0           Encoder-5-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-MultiHeadSelfAttentio (None, 256, 768)     0           Encoder-4-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-5-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-MultiHeadSelfAttentio (None, 256, 768)     1536        Encoder-5-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-FeedForward (FeedForw (None, 256, 768)     4722432     Encoder-5-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-FeedForward-Dropout ( (None, 256, 768)     0           Encoder-5-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-FeedForward-Add (Add) (None, 256, 768)     0           Encoder-5-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-5-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-FeedForward-Norm (Lay (None, 256, 768)     1536        Encoder-5-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-MultiHeadSelfAttentio (None, 256, 768)     2362368     Encoder-5-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-MultiHeadSelfAttentio (None, 256, 768)     0           Encoder-6-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-MultiHeadSelfAttentio (None, 256, 768)     0           Encoder-5-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-6-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-MultiHeadSelfAttentio (None, 256, 768)     1536        Encoder-6-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-FeedForward (FeedForw (None, 256, 768)     4722432     Encoder-6-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-FeedForward-Dropout ( (None, 256, 768)     0           Encoder-6-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-FeedForward-Add (Add) (None, 256, 768)     0           Encoder-6-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-6-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-FeedForward-Norm (Lay (None, 256, 768)     1536        Encoder-6-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-MultiHeadSelfAttentio (None, 256, 768)     2362368     Encoder-6-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-MultiHeadSelfAttentio (None, 256, 768)     0           Encoder-7-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-MultiHeadSelfAttentio (None, 256, 768)     0           Encoder-6-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-7-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-MultiHeadSelfAttentio (None, 256, 768)     1536        Encoder-7-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-FeedForward (FeedForw (None, 256, 768)     4722432     Encoder-7-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-FeedForward-Dropout ( (None, 256, 768)     0           Encoder-7-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-FeedForward-Add (Add) (None, 256, 768)     0           Encoder-7-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-7-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-FeedForward-Norm (Lay (None, 256, 768)     1536        Encoder-7-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-MultiHeadSelfAttentio (None, 256, 768)     2362368     Encoder-7-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-MultiHeadSelfAttentio (None, 256, 768)     0           Encoder-8-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-MultiHeadSelfAttentio (None, 256, 768)     0           Encoder-7-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-8-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-MultiHeadSelfAttentio (None, 256, 768)     1536        Encoder-8-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-FeedForward (FeedForw (None, 256, 768)     4722432     Encoder-8-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-FeedForward-Dropout ( (None, 256, 768)     0           Encoder-8-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-FeedForward-Add (Add) (None, 256, 768)     0           Encoder-8-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-8-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-FeedForward-Norm (Lay (None, 256, 768)     1536        Encoder-8-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-MultiHeadSelfAttentio (None, 256, 768)     2362368     Encoder-8-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-MultiHeadSelfAttentio (None, 256, 768)     0           Encoder-9-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-MultiHeadSelfAttentio (None, 256, 768)     0           Encoder-8-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-9-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-MultiHeadSelfAttentio (None, 256, 768)     1536        Encoder-9-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-FeedForward (FeedForw (None, 256, 768)     4722432     Encoder-9-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-FeedForward-Dropout ( (None, 256, 768)     0           Encoder-9-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-FeedForward-Add (Add) (None, 256, 768)     0           Encoder-9-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-9-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-FeedForward-Norm (Lay (None, 256, 768)     1536        Encoder-9-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-MultiHeadSelfAttenti (None, 256, 768)     2362368     Encoder-9-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-MultiHeadSelfAttenti (None, 256, 768)     0           Encoder-10-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-MultiHeadSelfAttenti (None, 256, 768)     0           Encoder-9-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-10-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-MultiHeadSelfAttenti (None, 256, 768)     1536        Encoder-10-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-FeedForward (FeedFor (None, 256, 768)     4722432     Encoder-10-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-FeedForward-Dropout  (None, 256, 768)     0           Encoder-10-FeedForward[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-FeedForward-Add (Add (None, 256, 768)     0           Encoder-10-MultiHeadSelfAttention\n",
      "                                                                 Encoder-10-FeedForward-Dropout[0]\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-FeedForward-Norm (La (None, 256, 768)     1536        Encoder-10-FeedForward-Add[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-MultiHeadSelfAttenti (None, 256, 768)     2362368     Encoder-10-FeedForward-Norm[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-MultiHeadSelfAttenti (None, 256, 768)     0           Encoder-11-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-MultiHeadSelfAttenti (None, 256, 768)     0           Encoder-10-FeedForward-Norm[0][0]\n",
      "                                                                 Encoder-11-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-MultiHeadSelfAttenti (None, 256, 768)     1536        Encoder-11-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-FeedForward (FeedFor (None, 256, 768)     4722432     Encoder-11-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-FeedForward-Dropout  (None, 256, 768)     0           Encoder-11-FeedForward[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-FeedForward-Add (Add (None, 256, 768)     0           Encoder-11-MultiHeadSelfAttention\n",
      "                                                                 Encoder-11-FeedForward-Dropout[0]\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-FeedForward-Norm (La (None, 256, 768)     1536        Encoder-11-FeedForward-Add[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-MultiHeadSelfAttenti (None, 256, 768)     2362368     Encoder-11-FeedForward-Norm[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-MultiHeadSelfAttenti (None, 256, 768)     0           Encoder-12-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-MultiHeadSelfAttenti (None, 256, 768)     0           Encoder-11-FeedForward-Norm[0][0]\n",
      "                                                                 Encoder-12-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-MultiHeadSelfAttenti (None, 256, 768)     1536        Encoder-12-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-FeedForward (FeedFor (None, 256, 768)     4722432     Encoder-12-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-FeedForward-Dropout  (None, 256, 768)     0           Encoder-12-FeedForward[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-FeedForward-Add (Add (None, 256, 768)     0           Encoder-12-MultiHeadSelfAttention\n",
      "                                                                 Encoder-12-FeedForward-Dropout[0]\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-FeedForward-Norm (La (None, 256, 768)     1536        Encoder-12-FeedForward-Add[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-Output (Concatenate)    (None, 256, 3072)    0           Encoder-9-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-10-FeedForward-Norm[0][0]\n",
      "                                                                 Encoder-11-FeedForward-Norm[0][0]\n",
      "                                                                 Encoder-12-FeedForward-Norm[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "input_flb (InputLayer)          [(None, 256)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_steal (InputLayer)        [(None, 256)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_draw (InputLayer)         [(None, 256)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_consume (InputLayer)      [(None, 256)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_sale (InputLayer)         [(None, 256)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_volun (InputLayer)        [(None, 256)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_steal_dis (InputLayer)    [(None, 256)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_draw_dis (InputLayer)     [(None, 256)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_consume_dis (InputLayer)  [(None, 256)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_sale_dis (InputLayer)     [(None, 256)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_volun_dis (InputLayer)    [(None, 256)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "non_masking_layer (NonMaskingLa (None, 256, 3072)    0           Encoder-Output[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "layer_embedding_flb (Embedding) (None, 256, 56)      448         input_flb[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "layer_embedding_steal (Embeddin (None, 256, 16)      48          input_steal[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "layer_embedding_draw (Embedding (None, 256, 16)      48          input_draw[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "layer_embedding_consume (Embedd (None, 256, 16)      48          input_consume[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_embedding_sale (Embedding (None, 256, 16)      48          input_sale[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "layer_embedding_volun (Embeddin (None, 256, 16)      48          input_volun[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "layer_embedding_steal_dis (Embe (None, 256, 2048)    526336      input_steal_dis[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "layer_embedding_draw_dis (Embed (None, 256, 2048)    526336      input_draw_dis[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "layer_embedding_consume_dis (Em (None, 256, 2048)    526336      input_consume_dis[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_embedding_sale_dis (Embed (None, 256, 2048)    526336      input_sale_dis[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "layer_embedding_volun_dis (Embe (None, 256, 2048)    526336      input_volun_dis[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "layer_concatenate (Concatenate) (None, 256, 13448)   0           non_masking_layer[0][0]          \n",
      "                                                                 layer_embedding_flb[0][0]        \n",
      "                                                                 layer_embedding_steal[0][0]      \n",
      "                                                                 layer_embedding_draw[0][0]       \n",
      "                                                                 layer_embedding_consume[0][0]    \n",
      "                                                                 layer_embedding_sale[0][0]       \n",
      "                                                                 layer_embedding_volun[0][0]      \n",
      "                                                                 layer_embedding_steal_dis[0][0]  \n",
      "                                                                 layer_embedding_draw_dis[0][0]   \n",
      "                                                                 layer_embedding_consume_dis[0][0]\n",
      "                                                                 layer_embedding_sale_dis[0][0]   \n",
      "                                                                 layer_embedding_volun_dis[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "layer_blstm (Bidirectional)     (None, 256, 256)     13903872    layer_concatenate[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_dense (Dense)             (None, 256, 64)      16448       layer_blstm[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "layer_crf_dense (Dense)         (None, 256, 42)      2730        layer_dense[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "layer_crf (CRF)                 (None, 256, 42)      1764        layer_crf_dense[0][0]            \n",
      "==================================================================================================\n",
      "Total params: 118,037,630\n",
      "Trainable params: 16,557,182\n",
      "Non-trainable params: 101,480,448\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"model_19\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Input-Token (InputLayer)        [(None, 256)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Input-Segment (InputLayer)      [(None, 256)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Token (TokenEmbedding [(None, 256, 768), ( 16226304    Input-Token[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Segment (Embedding)   (None, 256, 768)     1536        Input-Segment[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Token-Segment (Add)   (None, 256, 768)     0           Embedding-Token[0][0]            \n",
      "                                                                 Embedding-Segment[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Position (PositionEmb (None, 256, 768)     196608      Embedding-Token-Segment[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Dropout (Dropout)     (None, 256, 768)     0           Embedding-Position[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Norm (LayerNormalizat (None, 256, 768)     1536        Embedding-Dropout[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-MultiHeadSelfAttentio (None, 256, 768)     2362368     Embedding-Norm[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-MultiHeadSelfAttentio (None, 256, 768)     0           Encoder-1-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-MultiHeadSelfAttentio (None, 256, 768)     0           Embedding-Norm[0][0]             \n",
      "                                                                 Encoder-1-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-MultiHeadSelfAttentio (None, 256, 768)     1536        Encoder-1-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-FeedForward (FeedForw (None, 256, 768)     4722432     Encoder-1-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-FeedForward-Dropout ( (None, 256, 768)     0           Encoder-1-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-FeedForward-Add (Add) (None, 256, 768)     0           Encoder-1-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-1-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-FeedForward-Norm (Lay (None, 256, 768)     1536        Encoder-1-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-MultiHeadSelfAttentio (None, 256, 768)     2362368     Encoder-1-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-MultiHeadSelfAttentio (None, 256, 768)     0           Encoder-2-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-MultiHeadSelfAttentio (None, 256, 768)     0           Encoder-1-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-2-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-MultiHeadSelfAttentio (None, 256, 768)     1536        Encoder-2-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-FeedForward (FeedForw (None, 256, 768)     4722432     Encoder-2-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-FeedForward-Dropout ( (None, 256, 768)     0           Encoder-2-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-FeedForward-Add (Add) (None, 256, 768)     0           Encoder-2-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-2-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-FeedForward-Norm (Lay (None, 256, 768)     1536        Encoder-2-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-MultiHeadSelfAttentio (None, 256, 768)     2362368     Encoder-2-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-MultiHeadSelfAttentio (None, 256, 768)     0           Encoder-3-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-MultiHeadSelfAttentio (None, 256, 768)     0           Encoder-2-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-3-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-MultiHeadSelfAttentio (None, 256, 768)     1536        Encoder-3-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-FeedForward (FeedForw (None, 256, 768)     4722432     Encoder-3-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-FeedForward-Dropout ( (None, 256, 768)     0           Encoder-3-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-FeedForward-Add (Add) (None, 256, 768)     0           Encoder-3-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-3-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-FeedForward-Norm (Lay (None, 256, 768)     1536        Encoder-3-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-MultiHeadSelfAttentio (None, 256, 768)     2362368     Encoder-3-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-MultiHeadSelfAttentio (None, 256, 768)     0           Encoder-4-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-MultiHeadSelfAttentio (None, 256, 768)     0           Encoder-3-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-4-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-MultiHeadSelfAttentio (None, 256, 768)     1536        Encoder-4-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-FeedForward (FeedForw (None, 256, 768)     4722432     Encoder-4-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-FeedForward-Dropout ( (None, 256, 768)     0           Encoder-4-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-FeedForward-Add (Add) (None, 256, 768)     0           Encoder-4-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-4-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-FeedForward-Norm (Lay (None, 256, 768)     1536        Encoder-4-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-MultiHeadSelfAttentio (None, 256, 768)     2362368     Encoder-4-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-MultiHeadSelfAttentio (None, 256, 768)     0           Encoder-5-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-MultiHeadSelfAttentio (None, 256, 768)     0           Encoder-4-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-5-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-MultiHeadSelfAttentio (None, 256, 768)     1536        Encoder-5-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-FeedForward (FeedForw (None, 256, 768)     4722432     Encoder-5-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-FeedForward-Dropout ( (None, 256, 768)     0           Encoder-5-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-FeedForward-Add (Add) (None, 256, 768)     0           Encoder-5-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-5-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-FeedForward-Norm (Lay (None, 256, 768)     1536        Encoder-5-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-MultiHeadSelfAttentio (None, 256, 768)     2362368     Encoder-5-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-MultiHeadSelfAttentio (None, 256, 768)     0           Encoder-6-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-MultiHeadSelfAttentio (None, 256, 768)     0           Encoder-5-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-6-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-MultiHeadSelfAttentio (None, 256, 768)     1536        Encoder-6-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-FeedForward (FeedForw (None, 256, 768)     4722432     Encoder-6-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-FeedForward-Dropout ( (None, 256, 768)     0           Encoder-6-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-FeedForward-Add (Add) (None, 256, 768)     0           Encoder-6-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-6-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-FeedForward-Norm (Lay (None, 256, 768)     1536        Encoder-6-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-MultiHeadSelfAttentio (None, 256, 768)     2362368     Encoder-6-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-MultiHeadSelfAttentio (None, 256, 768)     0           Encoder-7-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-MultiHeadSelfAttentio (None, 256, 768)     0           Encoder-6-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-7-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-MultiHeadSelfAttentio (None, 256, 768)     1536        Encoder-7-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-FeedForward (FeedForw (None, 256, 768)     4722432     Encoder-7-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-FeedForward-Dropout ( (None, 256, 768)     0           Encoder-7-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-FeedForward-Add (Add) (None, 256, 768)     0           Encoder-7-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-7-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-FeedForward-Norm (Lay (None, 256, 768)     1536        Encoder-7-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-MultiHeadSelfAttentio (None, 256, 768)     2362368     Encoder-7-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-MultiHeadSelfAttentio (None, 256, 768)     0           Encoder-8-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-MultiHeadSelfAttentio (None, 256, 768)     0           Encoder-7-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-8-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-MultiHeadSelfAttentio (None, 256, 768)     1536        Encoder-8-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-FeedForward (FeedForw (None, 256, 768)     4722432     Encoder-8-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-FeedForward-Dropout ( (None, 256, 768)     0           Encoder-8-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-FeedForward-Add (Add) (None, 256, 768)     0           Encoder-8-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-8-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-FeedForward-Norm (Lay (None, 256, 768)     1536        Encoder-8-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-MultiHeadSelfAttentio (None, 256, 768)     2362368     Encoder-8-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-MultiHeadSelfAttentio (None, 256, 768)     0           Encoder-9-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-MultiHeadSelfAttentio (None, 256, 768)     0           Encoder-8-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-9-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-MultiHeadSelfAttentio (None, 256, 768)     1536        Encoder-9-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-FeedForward (FeedForw (None, 256, 768)     4722432     Encoder-9-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-FeedForward-Dropout ( (None, 256, 768)     0           Encoder-9-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-FeedForward-Add (Add) (None, 256, 768)     0           Encoder-9-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-9-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-FeedForward-Norm (Lay (None, 256, 768)     1536        Encoder-9-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-MultiHeadSelfAttenti (None, 256, 768)     2362368     Encoder-9-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-MultiHeadSelfAttenti (None, 256, 768)     0           Encoder-10-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-MultiHeadSelfAttenti (None, 256, 768)     0           Encoder-9-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-10-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-MultiHeadSelfAttenti (None, 256, 768)     1536        Encoder-10-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-FeedForward (FeedFor (None, 256, 768)     4722432     Encoder-10-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-FeedForward-Dropout  (None, 256, 768)     0           Encoder-10-FeedForward[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-FeedForward-Add (Add (None, 256, 768)     0           Encoder-10-MultiHeadSelfAttention\n",
      "                                                                 Encoder-10-FeedForward-Dropout[0]\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-FeedForward-Norm (La (None, 256, 768)     1536        Encoder-10-FeedForward-Add[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-MultiHeadSelfAttenti (None, 256, 768)     2362368     Encoder-10-FeedForward-Norm[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-MultiHeadSelfAttenti (None, 256, 768)     0           Encoder-11-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-MultiHeadSelfAttenti (None, 256, 768)     0           Encoder-10-FeedForward-Norm[0][0]\n",
      "                                                                 Encoder-11-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-MultiHeadSelfAttenti (None, 256, 768)     1536        Encoder-11-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-FeedForward (FeedFor (None, 256, 768)     4722432     Encoder-11-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-FeedForward-Dropout  (None, 256, 768)     0           Encoder-11-FeedForward[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-FeedForward-Add (Add (None, 256, 768)     0           Encoder-11-MultiHeadSelfAttention\n",
      "                                                                 Encoder-11-FeedForward-Dropout[0]\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-FeedForward-Norm (La (None, 256, 768)     1536        Encoder-11-FeedForward-Add[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-MultiHeadSelfAttenti (None, 256, 768)     2362368     Encoder-11-FeedForward-Norm[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-MultiHeadSelfAttenti (None, 256, 768)     0           Encoder-12-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-MultiHeadSelfAttenti (None, 256, 768)     0           Encoder-11-FeedForward-Norm[0][0]\n",
      "                                                                 Encoder-12-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-MultiHeadSelfAttenti (None, 256, 768)     1536        Encoder-12-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-FeedForward (FeedFor (None, 256, 768)     4722432     Encoder-12-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-FeedForward-Dropout  (None, 256, 768)     0           Encoder-12-FeedForward[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-FeedForward-Add (Add (None, 256, 768)     0           Encoder-12-MultiHeadSelfAttention\n",
      "                                                                 Encoder-12-FeedForward-Dropout[0]\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-FeedForward-Norm (La (None, 256, 768)     1536        Encoder-12-FeedForward-Add[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-Output (Concatenate)    (None, 256, 3072)    0           Encoder-9-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-10-FeedForward-Norm[0][0]\n",
      "                                                                 Encoder-11-FeedForward-Norm[0][0]\n",
      "                                                                 Encoder-12-FeedForward-Norm[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "input_flb (InputLayer)          [(None, 256)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_steal (InputLayer)        [(None, 256)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_draw (InputLayer)         [(None, 256)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_consume (InputLayer)      [(None, 256)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_sale (InputLayer)         [(None, 256)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_volun (InputLayer)        [(None, 256)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_steal_dis (InputLayer)    [(None, 256)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_draw_dis (InputLayer)     [(None, 256)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_consume_dis (InputLayer)  [(None, 256)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_sale_dis (InputLayer)     [(None, 256)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_volun_dis (InputLayer)    [(None, 256)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "non_masking_layer (NonMaskingLa (None, 256, 3072)    0           Encoder-Output[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "layer_embedding_flb (Embedding) (None, 256, 56)      448         input_flb[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "layer_embedding_steal (Embeddin (None, 256, 16)      48          input_steal[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "layer_embedding_draw (Embedding (None, 256, 16)      48          input_draw[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "layer_embedding_consume (Embedd (None, 256, 16)      48          input_consume[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_embedding_sale (Embedding (None, 256, 16)      48          input_sale[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "layer_embedding_volun (Embeddin (None, 256, 16)      48          input_volun[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "layer_embedding_steal_dis (Embe (None, 256, 2048)    526336      input_steal_dis[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "layer_embedding_draw_dis (Embed (None, 256, 2048)    526336      input_draw_dis[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "layer_embedding_consume_dis (Em (None, 256, 2048)    526336      input_consume_dis[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_embedding_sale_dis (Embed (None, 256, 2048)    526336      input_sale_dis[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "layer_embedding_volun_dis (Embe (None, 256, 2048)    526336      input_volun_dis[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "layer_concatenate (Concatenate) (None, 256, 13448)   0           non_masking_layer[0][0]          \n",
      "                                                                 layer_embedding_flb[0][0]        \n",
      "                                                                 layer_embedding_steal[0][0]      \n",
      "                                                                 layer_embedding_draw[0][0]       \n",
      "                                                                 layer_embedding_consume[0][0]    \n",
      "                                                                 layer_embedding_sale[0][0]       \n",
      "                                                                 layer_embedding_volun[0][0]      \n",
      "                                                                 layer_embedding_steal_dis[0][0]  \n",
      "                                                                 layer_embedding_draw_dis[0][0]   \n",
      "                                                                 layer_embedding_consume_dis[0][0]\n",
      "                                                                 layer_embedding_sale_dis[0][0]   \n",
      "                                                                 layer_embedding_volun_dis[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "layer_blstm (Bidirectional)     (None, 256, 256)     13903872    layer_concatenate[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_dense (Dense)             (None, 256, 64)      16448       layer_blstm[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "layer_crf_dense (Dense)         (None, 256, 42)      2730        layer_dense[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "layer_crf (CRF)                 (None, 256, 42)      1764        layer_crf_dense[0][0]            \n",
      "==================================================================================================\n",
      "Total params: 118,037,630\n",
      "Trainable params: 16,557,182\n",
      "Non-trainable params: 101,480,448\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.build_model(train_fx,\n",
    "                  train_y,\n",
    "                  dev_fx,\n",
    "                  dev_y)\n",
    "model.compile_model(metrics=[F1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "uuid": "11eddc89-7100-494e-95e0-af4318296b3a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 4/74 [>.............................] - ETA: 4:04 - loss: 424.0665 - F1: 0.1567"
     ]
    }
   ],
   "source": [
    "model.fit(train_fx,\n",
    "          train_y,\n",
    "          dev_fx,\n",
    "          dev_y,\n",
    "          epochs=1,\n",
    "          batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "uuid": "28c67369-b444-4571-aa94-15db021931c0"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "uuid": "687287fa-3d22-4b04-9fab-30121e91137d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71/71 [==============================] - 57s 807ms/step - loss: 61.5878 - F1: 0.2883 - val_loss: 720.4152 - val_F1: 0.9613\n",
      "73/73 [==============================] - 46s 631ms/step - loss: 4.0347 - F1: 0.3204 - val_loss: 717.0809 - val_F1: 0.9567\n",
      "73/73 [==============================] - 45s 620ms/step - loss: 1.6332 - F1: 0.3716 - val_loss: 715.5501 - val_F1: 0.9616\n",
      "73/73 [==============================] - 45s 620ms/step - loss: 0.9538 - F1: 0.4091 - val_loss: 715.8911 - val_F1: 0.9502\n",
      "73/73 [==============================] - 47s 640ms/step - loss: 0.6422 - F1: 0.4132 - val_loss: 712.1466 - val_F1: 0.9720\n",
      "73/73 [==============================] - 46s 624ms/step - loss: 0.4667 - F1: 0.4172 - val_loss: 714.0917 - val_F1: 0.9378\n",
      "73/73 [==============================] - 46s 627ms/step - loss: 0.3601 - F1: 0.4208 - val_loss: 714.5015 - val_F1: 0.9551\n",
      "73/73 [==============================] - 45s 619ms/step - loss: 0.2872 - F1: 0.4236 - val_loss: 717.7556 - val_F1: 0.9440\n",
      "73/73 [==============================] - 45s 614ms/step - loss: 0.2318 - F1: 0.4504 - val_loss: 712.2183 - val_F1: 0.9499\n",
      "73/73 [==============================] - 47s 643ms/step - loss: 5.3094 - F1: 0.4768 - val_loss: 699.0414 - val_F1: 0.9906\n",
      "71/71 [==============================] - 46s 643ms/step - loss: 0.5980 - F1: 0.4430 - val_loss: 702.5508 - val_F1: 0.9753\n",
      "73/73 [==============================] - 45s 620ms/step - loss: 0.1929 - F1: 0.4894 - val_loss: 700.8992 - val_F1: 0.9740\n",
      "73/73 [==============================] - 46s 630ms/step - loss: 0.1511 - F1: 0.4989 - val_loss: 702.4890 - val_F1: 0.9727\n",
      "73/73 [==============================] - 47s 638ms/step - loss: 0.1256 - F1: 0.5012 - val_loss: 700.9786 - val_F1: 0.9717\n",
      "73/73 [==============================] - 46s 625ms/step - loss: 0.1111 - F1: 0.5021 - val_loss: 701.2197 - val_F1: 0.9827\n",
      "73/73 [==============================] - 46s 631ms/step - loss: 0.0961 - F1: 0.5030 - val_loss: 697.2464 - val_F1: 0.9710\n",
      "73/73 [==============================] - 45s 622ms/step - loss: 0.0750 - F1: 0.5034 - val_loss: 702.8760 - val_F1: 0.9707\n",
      "73/73 [==============================] - 44s 608ms/step - loss: 0.0661 - F1: 0.5034 - val_loss: 709.4621 - val_F1: 0.9502\n",
      "73/73 [==============================] - 46s 626ms/step - loss: 0.0609 - F1: 0.5038 - val_loss: 693.9128 - val_F1: 0.9922\n",
      "73/73 [==============================] - 46s 632ms/step - loss: 0.6344 - F1: 0.4883 - val_loss: 691.7774 - val_F1: 1.0000\n",
      "71/71 [==============================] - 46s 653ms/step - loss: 0.0834 - F1: 0.5024 - val_loss: 697.3441 - val_F1: 0.9792\n",
      "73/73 [==============================] - 46s 634ms/step - loss: 0.0571 - F1: 0.5027 - val_loss: 695.6671 - val_F1: 0.9788\n",
      "73/73 [==============================] - 46s 634ms/step - loss: 0.0503 - F1: 0.5036 - val_loss: 696.9659 - val_F1: 0.9795\n",
      "73/73 [==============================] - 45s 617ms/step - loss: 0.0424 - F1: 0.5042 - val_loss: 695.9941 - val_F1: 0.9769\n",
      "73/73 [==============================] - 46s 624ms/step - loss: 0.0317 - F1: 0.5044 - val_loss: 696.4307 - val_F1: 0.9883\n",
      "73/73 [==============================] - 45s 617ms/step - loss: 0.0291 - F1: 0.5049 - val_loss: 692.6362 - val_F1: 0.9756\n",
      "73/73 [==============================] - 46s 629ms/step - loss: 0.0280 - F1: 0.5052 - val_loss: 697.3586 - val_F1: 0.9795\n",
      "73/73 [==============================] - 46s 635ms/step - loss: 0.0250 - F1: 0.5058 - val_loss: 705.9435 - val_F1: 0.9521\n",
      "73/73 [==============================] - 46s 629ms/step - loss: 0.0227 - F1: 0.5066 - val_loss: 688.7233 - val_F1: 1.0000\n",
      "73/73 [==============================] - 45s 611ms/step - loss: 0.0880 - F1: 0.4929 - val_loss: 689.1776 - val_F1: 1.0000\n",
      "71/71 [==============================] - 45s 631ms/step - loss: 0.0235 - F1: 0.5128 - val_loss: 694.4524 - val_F1: 0.9811\n",
      "73/73 [==============================] - 46s 635ms/step - loss: 0.0188 - F1: 0.5117 - val_loss: 693.0878 - val_F1: 0.9798\n",
      "73/73 [==============================] - 46s 626ms/step - loss: 0.0205 - F1: 0.5110 - val_loss: 694.6357 - val_F1: 0.9798\n",
      "73/73 [==============================] - 46s 629ms/step - loss: 0.0161 - F1: 0.5109 - val_loss: 693.2939 - val_F1: 0.9788\n",
      "73/73 [==============================] - 45s 616ms/step - loss: 0.0165 - F1: 0.5105 - val_loss: 694.3447 - val_F1: 0.9883\n",
      "73/73 [==============================] - 46s 633ms/step - loss: 0.0135 - F1: 0.5108 - val_loss: 690.4360 - val_F1: 0.9759\n",
      "73/73 [==============================] - 46s 630ms/step - loss: 0.0143 - F1: 0.5118 - val_loss: 694.7882 - val_F1: 0.9818\n",
      "73/73 [==============================] - 46s 626ms/step - loss: 0.0147 - F1: 0.5127 - val_loss: 704.1445 - val_F1: 0.9518\n",
      "73/73 [==============================] - 45s 619ms/step - loss: 0.0120 - F1: 0.5135 - val_loss: 686.7537 - val_F1: 1.0000\n",
      "73/73 [==============================] - 45s 619ms/step - loss: 0.0415 - F1: 0.4965 - val_loss: 687.3354 - val_F1: 1.0000\n",
      "71/71 [==============================] - 46s 642ms/step - loss: 0.0129 - F1: 0.5180 - val_loss: 692.7206 - val_F1: 0.9808\n",
      "73/73 [==============================] - 46s 624ms/step - loss: 0.0054 - F1: 0.5175 - val_loss: 690.9989 - val_F1: 0.9811\n",
      "73/73 [==============================] - 45s 616ms/step - loss: 0.0125 - F1: 0.5181 - val_loss: 692.7199 - val_F1: 0.9808\n",
      "73/73 [==============================] - 45s 615ms/step - loss: 0.0142 - F1: 0.5187 - val_loss: 691.6887 - val_F1: 0.9788\n",
      "73/73 [==============================] - 45s 612ms/step - loss: 0.0045 - F1: 0.5198 - val_loss: 692.7896 - val_F1: 0.9886\n",
      "73/73 [==============================] - 45s 619ms/step - loss: 0.0051 - F1: 0.5208 - val_loss: 689.0640 - val_F1: 0.9753\n",
      "73/73 [==============================] - 46s 627ms/step - loss: 0.0146 - F1: 0.5222 - val_loss: 693.1576 - val_F1: 0.9824\n",
      "73/73 [==============================] - 45s 622ms/step - loss: 0.0080 - F1: 0.5230 - val_loss: 702.2243 - val_F1: 0.9538\n",
      "73/73 [==============================] - 45s 616ms/step - loss: 0.0084 - F1: 0.5235 - val_loss: 685.2278 - val_F1: 1.0000\n",
      "73/73 [==============================] - 45s 618ms/step - loss: 0.0260 - F1: 0.5061 - val_loss: 685.9142 - val_F1: 1.0000\n",
      "71/71 [==============================] - 46s 655ms/step - loss: 0.0049 - F1: 0.5281 - val_loss: 691.3641 - val_F1: 0.9808\n",
      "73/73 [==============================] - 44s 609ms/step - loss: 0.0088 - F1: 0.5287 - val_loss: 689.4494 - val_F1: 0.9818\n",
      "73/73 [==============================] - 46s 628ms/step - loss: 0.0069 - F1: 0.5288 - val_loss: 691.2118 - val_F1: 0.9811\n",
      "73/73 [==============================] - 45s 616ms/step - loss: 0.0067 - F1: 0.5279 - val_loss: 690.0151 - val_F1: 0.9795\n",
      "73/73 [==============================] - 46s 625ms/step - loss: 0.0049 - F1: 0.5279 - val_loss: 691.3906 - val_F1: 0.9883\n",
      "73/73 [==============================] - 45s 618ms/step - loss: 0.0066 - F1: 0.5279 - val_loss: 687.3541 - val_F1: 0.9756\n",
      "73/73 [==============================] - 45s 619ms/step - loss: 0.0070 - F1: 0.5283 - val_loss: 691.9242 - val_F1: 0.9811\n",
      "73/73 [==============================] - 45s 620ms/step - loss: 0.0112 - F1: 0.5296 - val_loss: 700.6065 - val_F1: 0.9544\n",
      "73/73 [==============================] - 46s 630ms/step - loss: 0.0100 - F1: 0.5310 - val_loss: 683.7079 - val_F1: 1.0000\n",
      "73/73 [==============================] - 46s 633ms/step - loss: 0.0208 - F1: 0.5148 - val_loss: 684.4889 - val_F1: 1.0000\n",
      "71/71 [==============================] - 47s 660ms/step - loss: -6.2153e-04 - F1: 0.5358 - val_loss: 689.9666 - val_F1: 0.9808\n",
      "73/73 [==============================] - 46s 630ms/step - loss: -3.0796e-04 - F1: 0.5367 - val_loss: 687.9007 - val_F1: 0.9824\n",
      "73/73 [==============================] - 46s 629ms/step - loss: 5.0166e-04 - F1: 0.5374 - val_loss: 689.9829 - val_F1: 0.9808\n",
      "73/73 [==============================] - 46s 626ms/step - loss: -0.0021 - F1: 0.5383 - val_loss: 688.8515 - val_F1: 0.9792\n",
      "73/73 [==============================] - 46s 626ms/step - loss: 0.0099 - F1: 0.5394 - val_loss: 690.2495 - val_F1: 0.9883\n",
      "73/73 [==============================] - 45s 620ms/step - loss: 0.0015 - F1: 0.5406 - val_loss: 686.1610 - val_F1: 0.9756\n",
      "73/73 [==============================] - 45s 620ms/step - loss: 0.0112 - F1: 0.5417 - val_loss: 690.9006 - val_F1: 0.9808\n",
      "73/73 [==============================] - 46s 625ms/step - loss: 0.0081 - F1: 0.5432 - val_loss: 699.3755 - val_F1: 0.9551\n",
      "73/73 [==============================] - 46s 630ms/step - loss: -6.9758e-04 - F1: 0.5443 - val_loss: 682.4898 - val_F1: 1.0000\n",
      "73/73 [==============================] - 45s 622ms/step - loss: 0.0058 - F1: 0.5307 - val_loss: 683.3244 - val_F1: 1.0000\n",
      "71/71 [==============================] - 46s 645ms/step - loss: 0.0038 - F1: 0.5477 - val_loss: 688.7683 - val_F1: 0.9808\n",
      "73/73 [==============================] - 46s 626ms/step - loss: 0.0020 - F1: 0.5471 - val_loss: 686.5041 - val_F1: 0.9827\n",
      "73/73 [==============================] - 45s 622ms/step - loss: 0.0034 - F1: 0.5458 - val_loss: 688.6490 - val_F1: 0.9808\n",
      "73/73 [==============================] - 46s 627ms/step - loss: -0.0053 - F1: 0.5470 - val_loss: 687.7752 - val_F1: 0.9782\n",
      "73/73 [==============================] - 46s 628ms/step - loss: -0.0020 - F1: 0.5482 - val_loss: 688.9995 - val_F1: 0.9883\n",
      "73/73 [==============================] - 46s 624ms/step - loss: -0.0046 - F1: 0.5494 - val_loss: 684.7087 - val_F1: 0.9759\n",
      "73/73 [==============================] - 45s 613ms/step - loss: -0.0043 - F1: 0.5506 - val_loss: 689.4455 - val_F1: 0.9814\n",
      "73/73 [==============================] - 45s 615ms/step - loss: -0.0032 - F1: 0.5520 - val_loss: 698.1664 - val_F1: 0.9551\n",
      "73/73 [==============================] - 45s 623ms/step - loss: 0.0030 - F1: 0.5538 - val_loss: 681.1635 - val_F1: 1.0000\n",
      "73/73 [==============================] - 45s 613ms/step - loss: 0.0128 - F1: 0.5432 - val_loss: 682.0582 - val_F1: 1.0000\n",
      "71/71 [==============================] - 45s 627ms/step - loss: 0.0105 - F1: 0.5573 - val_loss: 687.3828 - val_F1: 0.9814\n",
      "73/73 [==============================] - 44s 602ms/step - loss: -0.0060 - F1: 0.5587 - val_loss: 685.3519 - val_F1: 0.9827\n",
      "73/73 [==============================] - 44s 601ms/step - loss: -0.0037 - F1: 0.5605 - val_loss: 687.7052 - val_F1: 0.9801\n",
      "73/73 [==============================] - 44s 601ms/step - loss: -0.0013 - F1: 0.5622 - val_loss: 686.8893 - val_F1: 0.9772\n",
      "73/73 [==============================] - 45s 617ms/step - loss: 0.0063 - F1: 0.5642 - val_loss: 687.9294 - val_F1: 0.9883\n",
      "73/73 [==============================] - 45s 615ms/step - loss: 0.0073 - F1: 0.5656 - val_loss: 683.3828 - val_F1: 0.9762\n",
      "73/73 [==============================] - 44s 608ms/step - loss: 0.0042 - F1: 0.5681 - val_loss: 688.0596 - val_F1: 0.9824\n",
      "73/73 [==============================] - 45s 614ms/step - loss: -0.0026 - F1: 0.5710 - val_loss: 697.0911 - val_F1: 0.9551\n",
      "73/73 [==============================] - 46s 630ms/step - loss: 0.0068 - F1: 0.5726 - val_loss: 679.9013 - val_F1: 1.0000\n",
      "73/73 [==============================] - 45s 618ms/step - loss: 0.0127 - F1: 0.5647 - val_loss: 680.8155 - val_F1: 1.0000\n",
      "71/71 [==============================] - 45s 635ms/step - loss: 0.0083 - F1: 0.5729 - val_loss: 686.1188 - val_F1: 0.9814\n",
      "73/73 [==============================] - 45s 620ms/step - loss: 0.0025 - F1: 0.5745 - val_loss: 684.1958 - val_F1: 0.9821\n",
      "73/73 [==============================] - 44s 608ms/step - loss: 0.0015 - F1: 0.5769 - val_loss: 686.3813 - val_F1: 0.9801\n",
      "73/73 [==============================] - 45s 612ms/step - loss: -0.0015 - F1: 0.5803 - val_loss: 685.6199 - val_F1: 0.9769\n",
      "73/73 [==============================] - 45s 610ms/step - loss: 7.8621e-04 - F1: 0.5821 - val_loss: 686.7036 - val_F1: 0.9880\n",
      "73/73 [==============================] - 44s 605ms/step - loss: 1.3991e-04 - F1: 0.5823 - val_loss: 681.8027 - val_F1: 0.9762\n",
      "73/73 [==============================] - 45s 610ms/step - loss: -8.8682e-04 - F1: 0.5815 - val_loss: 686.5137 - val_F1: 0.9824\n",
      "73/73 [==============================] - 45s 610ms/step - loss: 0.0022 - F1: 0.5809 - val_loss: 695.3377 - val_F1: 0.9557\n",
      "73/73 [==============================] - 44s 608ms/step - loss: -2.2686e-04 - F1: 0.5801 - val_loss: 678.0254 - val_F1: 1.0000\n",
      "73/73 [==============================] - 45s 621ms/step - loss: 0.0028 - F1: 0.5678 - val_loss: 678.7207 - val_F1: 1.0000\n",
      "71/71 [==============================] - 46s 650ms/step - loss: 0.0013 - F1: 0.5685 - val_loss: 684.4553 - val_F1: 0.9798\n",
      "73/73 [==============================] - 46s 625ms/step - loss: 8.4725e-05 - F1: 0.5704 - val_loss: 682.1983 - val_F1: 0.9821\n",
      "73/73 [==============================] - 46s 625ms/step - loss: 0.0018 - F1: 0.5721 - val_loss: 684.3119 - val_F1: 0.9808\n",
      "73/73 [==============================] - 46s 627ms/step - loss: 4.2000e-04 - F1: 0.5732 - val_loss: 683.7203 - val_F1: 0.9772\n",
      "73/73 [==============================] - 46s 630ms/step - loss: -2.1098e-04 - F1: 0.5740 - val_loss: 684.9629 - val_F1: 0.9886\n",
      "73/73 [==============================] - 45s 622ms/step - loss: 3.8934e-04 - F1: 0.5746 - val_loss: 680.1118 - val_F1: 0.9766\n",
      "73/73 [==============================] - 46s 632ms/step - loss: 0.0058 - F1: 0.5751 - val_loss: 685.8091 - val_F1: 0.9801\n",
      "73/73 [==============================] - 46s 625ms/step - loss: 0.0061 - F1: 0.5715 - val_loss: 694.4238 - val_F1: 0.9544\n",
      "73/73 [==============================] - 46s 626ms/step - loss: 0.0021 - F1: 0.5751 - val_loss: 676.7125 - val_F1: 0.9997\n",
      "73/73 [==============================] - 46s 624ms/step - loss: 0.0218 - F1: 0.5587 - val_loss: 677.0666 - val_F1: 1.0000\n",
      "71/71 [==============================] - 46s 650ms/step - loss: 0.0024 - F1: 0.5742 - val_loss: 682.4160 - val_F1: 0.9811\n",
      "73/73 [==============================] - 45s 622ms/step - loss: 0.0012 - F1: 0.5823 - val_loss: 680.8455 - val_F1: 0.9801\n",
      "73/73 [==============================] - 45s 619ms/step - loss: -7.1904e-05 - F1: 0.5852 - val_loss: 682.3740 - val_F1: 0.9811\n",
      "73/73 [==============================] - 45s 618ms/step - loss: 0.0016 - F1: 0.5821 - val_loss: 681.9663 - val_F1: 0.9762\n",
      "73/73 [==============================] - 46s 626ms/step - loss: 1.7447e-04 - F1: 0.5800 - val_loss: 683.4644 - val_F1: 0.9880\n",
      "73/73 [==============================] - 45s 622ms/step - loss: 5.2423e-04 - F1: 0.5808 - val_loss: 677.7300 - val_F1: 0.9769\n",
      "73/73 [==============================] - 45s 615ms/step - loss: 8.9044e-04 - F1: 0.5823 - val_loss: 683.5696 - val_F1: 0.9814\n",
      "73/73 [==============================] - 46s 628ms/step - loss: 3.1883e-04 - F1: 0.5847 - val_loss: 693.2266 - val_F1: 0.9538\n",
      "73/73 [==============================] - 45s 619ms/step - loss: 4.3031e-04 - F1: 0.5875 - val_loss: 674.8182 - val_F1: 1.0000\n",
      "73/73 [==============================] - 45s 619ms/step - loss: 0.0013 - F1: 0.5780 - val_loss: 676.0641 - val_F1: 1.0000\n",
      "71/71 [==============================] - 45s 635ms/step - loss: 1.0717e-04 - F1: 0.5901 - val_loss: 681.2558 - val_F1: 0.9824\n",
      "73/73 [==============================] - 45s 620ms/step - loss: 0.0010 - F1: 0.5933 - val_loss: 679.6249 - val_F1: 0.9818\n",
      "73/73 [==============================] - 45s 621ms/step - loss: -7.5109e-04 - F1: 0.5950 - val_loss: 681.7619 - val_F1: 0.9808\n",
      "73/73 [==============================] - 45s 620ms/step - loss: 8.8348e-04 - F1: 0.5978 - val_loss: 681.0478 - val_F1: 0.9772\n",
      "73/73 [==============================] - 44s 610ms/step - loss: -5.3845e-04 - F1: 0.6016 - val_loss: 682.8732 - val_F1: 0.9886\n",
      "73/73 [==============================] - 44s 603ms/step - loss: 5.7886e-04 - F1: 0.6067 - val_loss: 677.0876 - val_F1: 0.9762\n",
      "73/73 [==============================] - 45s 613ms/step - loss: 6.4714e-04 - F1: 0.6143 - val_loss: 682.3118 - val_F1: 0.9844\n",
      "73/73 [==============================] - 45s 614ms/step - loss: 4.0579e-04 - F1: 0.6221 - val_loss: 692.7267 - val_F1: 0.9544\n",
      "73/73 [==============================] - 45s 616ms/step - loss: 1.3601e-04 - F1: 0.6334 - val_loss: 674.1448 - val_F1: 1.0000\n",
      "73/73 [==============================] - 45s 614ms/step - loss: 0.0016 - F1: 0.6210 - val_loss: 675.5326 - val_F1: 1.0000\n",
      "71/71 [==============================] - 46s 642ms/step - loss: 3.2266e-04 - F1: 0.6338 - val_loss: 680.4984 - val_F1: 0.9834\n",
      "73/73 [==============================] - 45s 620ms/step - loss: 2.4721e-04 - F1: 0.6335 - val_loss: 679.0730 - val_F1: 0.9818\n",
      "73/73 [==============================] - 45s 618ms/step - loss: 8.9184e-06 - F1: 0.6360 - val_loss: 681.1856 - val_F1: 0.9811\n",
      "73/73 [==============================] - 45s 617ms/step - loss: 5.4346e-04 - F1: 0.6396 - val_loss: 680.6978 - val_F1: 0.9766\n",
      "73/73 [==============================] - 45s 621ms/step - loss: -1.6388e-04 - F1: 0.6440 - val_loss: 683.0264 - val_F1: 0.9873\n",
      "73/73 [==============================] - 45s 612ms/step - loss: 3.1159e-04 - F1: 0.6479 - val_loss: 676.4428 - val_F1: 0.9759\n",
      "73/73 [==============================] - 44s 606ms/step - loss: 7.2685e-04 - F1: 0.6528 - val_loss: 681.9512 - val_F1: 0.9847\n",
      "73/73 [==============================] - 45s 619ms/step - loss: 1.2987e-04 - F1: 0.6610 - val_loss: 692.6948 - val_F1: 0.9544\n",
      "73/73 [==============================] - 46s 632ms/step - loss: 3.0629e-04 - F1: 0.6657 - val_loss: 673.5272 - val_F1: 1.0000\n",
      "73/73 [==============================] - 45s 622ms/step - loss: 6.3320e-04 - F1: 0.6614 - val_loss: 675.0330 - val_F1: 1.0000\n",
      "71/71 [==============================] - 45s 632ms/step - loss: 7.1523e-04 - F1: 0.6791 - val_loss: 680.0813 - val_F1: 0.9831\n",
      "73/73 [==============================] - 45s 618ms/step - loss: -9.7266e-05 - F1: 0.6999 - val_loss: 678.5365 - val_F1: 0.9814\n",
      "73/73 [==============================] - 44s 605ms/step - loss: 3.5674e-04 - F1: 0.7177 - val_loss: 680.2481 - val_F1: 0.9824\n",
      "73/73 [==============================] - 45s 613ms/step - loss: 2.0847e-04 - F1: 0.7298 - val_loss: 680.0716 - val_F1: 0.9762\n",
      "73/73 [==============================] - 44s 604ms/step - loss: 5.5545e-04 - F1: 0.7394 - val_loss: 682.7263 - val_F1: 0.9870\n",
      "73/73 [==============================] - 45s 617ms/step - loss: 6.9981e-04 - F1: 0.7476 - val_loss: 675.2852 - val_F1: 0.9756\n",
      "73/73 [==============================] - 45s 619ms/step - loss: 4.7351e-04 - F1: 0.7550 - val_loss: 681.7527 - val_F1: 0.9827\n",
      "73/73 [==============================] - 45s 622ms/step - loss: -1.1092e-04 - F1: 0.7635 - val_loss: 692.3318 - val_F1: 0.9544\n",
      "73/73 [==============================] - 46s 626ms/step - loss: 9.4173e-04 - F1: 0.7749 - val_loss: 672.3776 - val_F1: 1.0000\n",
      "73/73 [==============================] - 45s 621ms/step - loss: 6.8699e-04 - F1: 0.7300 - val_loss: 673.9802 - val_F1: 1.0000\n",
      "71/71 [==============================] - 46s 647ms/step - loss: -1.6820e-04 - F1: 0.7802 - val_loss: 679.1778 - val_F1: 0.9827\n",
      "73/73 [==============================] - 46s 625ms/step - loss: 4.2362e-05 - F1: 0.7918 - val_loss: 677.2617 - val_F1: 0.9821\n",
      "73/73 [==============================] - 45s 617ms/step - loss: 3.1549e-04 - F1: 0.8061 - val_loss: 679.0801 - val_F1: 0.9831\n",
      "73/73 [==============================] - 45s 620ms/step - loss: 3.3695e-04 - F1: 0.8213 - val_loss: 679.7641 - val_F1: 0.9736\n",
      "73/73 [==============================] - 44s 609ms/step - loss: 8.1659e-05 - F1: 0.8359 - val_loss: 682.4420 - val_F1: 0.9873\n",
      "73/73 [==============================] - 45s 621ms/step - loss: 2.0875e-04 - F1: 0.8498 - val_loss: 673.9872 - val_F1: 0.9753\n",
      "73/73 [==============================] - 45s 621ms/step - loss: 4.5832e-04 - F1: 0.8641 - val_loss: 681.3854 - val_F1: 0.9831\n",
      "73/73 [==============================] - 45s 622ms/step - loss: 6.1955e-04 - F1: 0.8776 - val_loss: 693.0646 - val_F1: 0.9528\n",
      "73/73 [==============================] - 45s 619ms/step - loss: 8.1436e-04 - F1: 0.8909 - val_loss: 671.5233 - val_F1: 1.0000\n",
      "73/73 [==============================] - 46s 626ms/step - loss: 0.0013 - F1: 0.8395 - val_loss: 673.2981 - val_F1: 1.0000\n",
      "71/71 [==============================] - 46s 642ms/step - loss: 7.6079e-04 - F1: 0.9034 - val_loss: 678.8596 - val_F1: 0.9818\n",
      "73/73 [==============================] - 46s 625ms/step - loss: 5.4277e-04 - F1: 0.9173 - val_loss: 676.3456 - val_F1: 0.9821\n",
      "73/73 [==============================] - 45s 623ms/step - loss: 6.7682e-04 - F1: 0.9275 - val_loss: 678.7834 - val_F1: 0.9808\n",
      "73/73 [==============================] - 45s 617ms/step - loss: 7.5402e-04 - F1: 0.9334 - val_loss: 678.7581 - val_F1: 0.9733\n",
      "73/73 [==============================] - 45s 613ms/step - loss: 9.7182e-04 - F1: 0.9405 - val_loss: 682.7855 - val_F1: 0.9860\n",
      "73/73 [==============================] - 46s 624ms/step - loss: 9.5845e-04 - F1: 0.9562 - val_loss: 671.8164 - val_F1: 0.9749\n",
      "73/73 [==============================] - 45s 617ms/step - loss: 6.8518e-04 - F1: 0.9628 - val_loss: 681.9547 - val_F1: 0.9788\n",
      "73/73 [==============================] - 44s 607ms/step - loss: 7.3786e-04 - F1: 0.9682 - val_loss: 693.4468 - val_F1: 0.9528\n",
      "73/73 [==============================] - 45s 618ms/step - loss: 7.5277e-04 - F1: 0.9733 - val_loss: 669.9363 - val_F1: 1.0000\n",
      "73/73 [==============================] - 45s 615ms/step - loss: 6.1397e-04 - F1: 0.9486 - val_loss: 672.0245 - val_F1: 1.0000\n",
      "71/71 [==============================] - 46s 642ms/step - loss: 7.6709e-04 - F1: 0.9750 - val_loss: 678.0046 - val_F1: 0.9818\n",
      "73/73 [==============================] - 46s 624ms/step - loss: 7.5716e-04 - F1: 0.9797 - val_loss: 675.7161 - val_F1: 0.9805\n",
      "73/73 [==============================] - 45s 621ms/step - loss: 5.1190e-04 - F1: 0.9829 - val_loss: 678.4045 - val_F1: 0.9801\n",
      "73/73 [==============================] - 45s 618ms/step - loss: 8.0356e-04 - F1: 0.9843 - val_loss: 678.8412 - val_F1: 0.9717\n",
      "73/73 [==============================] - 46s 627ms/step - loss: 4.7483e-04 - F1: 0.9854 - val_loss: 684.6507 - val_F1: 0.9850\n",
      "73/73 [==============================] - 45s 621ms/step - loss: 6.2718e-04 - F1: 0.9870 - val_loss: 671.1036 - val_F1: 0.9727\n",
      "73/73 [==============================] - 45s 619ms/step - loss: 5.4632e-04 - F1: 0.9888 - val_loss: 684.8524 - val_F1: 0.9736\n",
      "73/73 [==============================] - 45s 622ms/step - loss: 8.6801e-04 - F1: 0.9920 - val_loss: 696.8010 - val_F1: 0.9512\n",
      "73/73 [==============================] - 46s 625ms/step - loss: 8.5978e-04 - F1: 0.5959 - val_loss: 670.0490 - val_F1: 1.0000\n",
      "73/73 [==============================] - 45s 614ms/step - loss: 8.9722e-04 - F1: 0.4884 - val_loss: 673.0705 - val_F1: 1.0000\n",
      "71/71 [==============================] - 45s 640ms/step - loss: 0.0010 - F1: 0.4616 - val_loss: 679.8126 - val_F1: 0.9801\n",
      "73/73 [==============================] - 45s 621ms/step - loss: 0.0011 - F1: 0.4639 - val_loss: 676.8179 - val_F1: 0.9798\n",
      "73/73 [==============================] - 46s 627ms/step - loss: 0.0015 - F1: 0.4654 - val_loss: 679.7563 - val_F1: 0.9795\n",
      "73/73 [==============================] - 45s 623ms/step - loss: 0.0016 - F1: 0.4660 - val_loss: 679.7339 - val_F1: 0.9710\n",
      "73/73 [==============================] - 45s 621ms/step - loss: 0.0018 - F1: 0.4567 - val_loss: 687.9031 - val_F1: 0.9844\n",
      "73/73 [==============================] - 46s 624ms/step - loss: 0.0018 - F1: 0.3288 - val_loss: 669.9117 - val_F1: 0.9687\n",
      "73/73 [==============================] - 45s 620ms/step - loss: 0.0018 - F1: 0.2142 - val_loss: 689.2866 - val_F1: 0.9606\n",
      "73/73 [==============================] - 45s 619ms/step - loss: 0.0020 - F1: 0.1688 - val_loss: 699.3675 - val_F1: 0.9512\n",
      "73/73 [==============================] - 45s 623ms/step - loss: 0.0013 - F1: 0.1571 - val_loss: 667.2464 - val_F1: 1.0000\n",
      "73/73 [==============================] - 45s 620ms/step - loss: 0.0022 - F1: 0.1810 - val_loss: 670.4461 - val_F1: 1.0000\n",
      "71/71 [==============================] - 45s 639ms/step - loss: 0.0019 - F1: 0.0956 - val_loss: 677.5439 - val_F1: 0.9779\n",
      "73/73 [==============================] - 46s 626ms/step - loss: 0.0010 - F1: 0.0604 - val_loss: 672.8058 - val_F1: 0.9785\n",
      "73/73 [==============================] - 44s 608ms/step - loss: 8.5240e-04 - F1: 0.0381 - val_loss: 676.1605 - val_F1: 0.9782\n",
      "73/73 [==============================] - 45s 620ms/step - loss: 0.0017 - F1: 0.0210 - val_loss: 676.3709 - val_F1: 0.9668\n",
      "73/73 [==============================] - 45s 623ms/step - loss: 0.0020 - F1: 0.0131 - val_loss: 687.1890 - val_F1: 0.9837\n",
      "73/73 [==============================] - 45s 619ms/step - loss: 0.0014 - F1: 0.0108 - val_loss: 661.7041 - val_F1: 0.9681\n",
      "73/73 [==============================] - 44s 609ms/step - loss: -5.3232e-05 - F1: 0.0087 - val_loss: 686.6404 - val_F1: 0.9642\n",
      "73/73 [==============================] - 45s 617ms/step - loss: 0.0010 - F1: 0.0066 - val_loss: 701.7994 - val_F1: 0.9489\n",
      "73/73 [==============================] - 45s 614ms/step - loss: 0.0019 - F1: 0.0042 - val_loss: 661.9771 - val_F1: 1.0000\n",
      "73/73 [==============================] - 46s 623ms/step - loss: 7.7311e-04 - F1: 0.0123 - val_loss: 667.2984 - val_F1: 1.0000\n",
      "71/71 [==============================] - 45s 637ms/step - loss: 0.0017 - F1: 0.0023 - val_loss: 676.2064 - val_F1: 0.9756\n",
      "73/73 [==============================] - 46s 625ms/step - loss: 9.1413e-04 - F1: 0.0019 - val_loss: 670.9977 - val_F1: 0.9749\n",
      "73/73 [==============================] - 46s 627ms/step - loss: 6.1927e-04 - F1: 0.0017 - val_loss: 674.4417 - val_F1: 0.9782\n",
      "73/73 [==============================] - 44s 608ms/step - loss: 0.0016 - F1: 7.1304e-04 - val_loss: 675.8694 - val_F1: 0.9609\n",
      "73/73 [==============================] - 44s 608ms/step - loss: 8.4892e-04 - F1: 2.4072e-04 - val_loss: 691.0491 - val_F1: 0.9824\n",
      "73/73 [==============================] - 44s 602ms/step - loss: -8.1993e-04 - F1: 0.0000e+00 - val_loss: 655.4493 - val_F1: 0.9658\n",
      "73/73 [==============================] - 45s 618ms/step - loss: 0.0014 - F1: 0.0000e+00 - val_loss: 690.0371 - val_F1: 0.9606\n",
      "73/73 [==============================] - 45s 621ms/step - loss: 0.0017 - F1: 0.0000e+00 - val_loss: 710.0742 - val_F1: 0.9450\n",
      "73/73 [==============================] - 44s 603ms/step - loss: 3.1883e-04 - F1: 0.0000e+00 - val_loss: 660.3676 - val_F1: 1.0000\n",
      "73/73 [==============================] - 45s 621ms/step - loss: 0.0026 - F1: 0.0028 - val_loss: 668.5792 - val_F1: 1.0000\n",
      "71/71 [==============================] - 46s 641ms/step - loss: 0.0014 - F1: 1.9250e-04 - val_loss: 677.4509 - val_F1: 0.9775\n",
      "73/73 [==============================] - 45s 614ms/step - loss: -3.7123e-04 - F1: 0.0000e+00 - val_loss: 671.6040 - val_F1: 0.9727\n",
      "73/73 [==============================] - 45s 618ms/step - loss: 6.8672e-04 - F1: 0.0000e+00 - val_loss: 676.8640 - val_F1: 0.9749\n",
      "73/73 [==============================] - 46s 624ms/step - loss: -0.0011 - F1: 0.0000e+00 - val_loss: 676.7561 - val_F1: 0.9609\n",
      "73/73 [==============================] - 45s 620ms/step - loss: 0.0031 - F1: 0.0000e+00 - val_loss: 701.6252 - val_F1: 0.9805\n",
      "73/73 [==============================] - 45s 617ms/step - loss: -0.0016 - F1: 0.0000e+00 - val_loss: 653.7844 - val_F1: 0.9561\n",
      "73/73 [==============================] - 46s 627ms/step - loss: -3.8126e-04 - F1: 0.0000e+00 - val_loss: 699.1255 - val_F1: 0.9554\n",
      "73/73 [==============================] - 45s 618ms/step - loss: 5.7412e-04 - F1: 0.0000e+00 - val_loss: 724.8311 - val_F1: 0.9424\n",
      "73/73 [==============================] - 45s 618ms/step - loss: 0.0016 - F1: 0.0000e+00 - val_loss: 662.3068 - val_F1: 1.0000\n",
      "73/73 [==============================] - 45s 617ms/step - loss: -0.0033 - F1: 8.8918e-04 - val_loss: 673.4634 - val_F1: 0.9990\n",
      "71/71 [==============================] - 45s 630ms/step - loss: 0.0221 - F1: 3.7239e-04 - val_loss: 689.9153 - val_F1: 0.9417\n",
      "73/73 [==============================] - 45s 623ms/step - loss: 0.0046 - F1: 0.0000e+00 - val_loss: 673.8431 - val_F1: 0.9476\n",
      "73/73 [==============================] - 45s 620ms/step - loss: 0.0071 - F1: 0.0000e+00 - val_loss: 670.1003 - val_F1: 0.9652\n",
      "73/73 [==============================] - 45s 613ms/step - loss: 0.0106 - F1: 0.0000e+00 - val_loss: 677.3608 - val_F1: 0.8971\n",
      "73/73 [==============================] - 45s 619ms/step - loss: 0.0070 - F1: 0.0000e+00 - val_loss: 695.7075 - val_F1: 0.9476\n",
      "73/73 [==============================] - 45s 619ms/step - loss: 0.0067 - F1: 0.0000e+00 - val_loss: 628.8381 - val_F1: 0.9128\n",
      "73/73 [==============================] - 45s 618ms/step - loss: 0.0036 - F1: 0.0000e+00 - val_loss: 675.1892 - val_F1: 0.9277\n",
      "73/73 [==============================] - 45s 623ms/step - loss: 0.0116 - F1: 0.0000e+00 - val_loss: 693.9874 - val_F1: 0.9235\n",
      "73/73 [==============================] - 44s 609ms/step - loss: 0.0106 - F1: 0.0000e+00 - val_loss: 625.6251 - val_F1: 0.9388\n",
      "73/73 [==============================] - 45s 616ms/step - loss: 1.0715 - F1: 0.0027 - val_loss: 621.1677 - val_F1: 0.9980\n",
      "71/71 [==============================] - 46s 644ms/step - loss: 0.0474 - F1: 2.3815e-04 - val_loss: 632.8836 - val_F1: 0.9717\n",
      "73/73 [==============================] - 45s 620ms/step - loss: 0.0051 - F1: 0.0000e+00 - val_loss: 622.9315 - val_F1: 0.9600\n",
      "73/73 [==============================] - 45s 621ms/step - loss: 8.5839e-04 - F1: 0.0000e+00 - val_loss: 626.1918 - val_F1: 0.9723\n",
      "73/73 [==============================] - 45s 622ms/step - loss: 0.0053 - F1: 0.0000e+00 - val_loss: 623.7464 - val_F1: 0.9593\n",
      "73/73 [==============================] - 45s 621ms/step - loss: -0.0039 - F1: 0.0000e+00 - val_loss: 659.4373 - val_F1: 0.9707\n",
      "73/73 [==============================] - 45s 617ms/step - loss: 0.0034 - F1: 0.0000e+00 - val_loss: 587.1537 - val_F1: 0.9574\n",
      "73/73 [==============================] - 45s 613ms/step - loss: 0.0140 - F1: 0.0000e+00 - val_loss: 644.7959 - val_F1: 0.9622\n",
      "73/73 [==============================] - 45s 610ms/step - loss: -0.0070 - F1: 0.0000e+00 - val_loss: 669.7736 - val_F1: 0.9551\n",
      "73/73 [==============================] - 45s 611ms/step - loss: 0.0167 - F1: 0.0000e+00 - val_loss: 600.0693 - val_F1: 0.9932\n",
      "73/73 [==============================] - 45s 621ms/step - loss: 0.0685 - F1: 6.2405e-05 - val_loss: 609.2242 - val_F1: 1.0000\n",
      "71/71 [==============================] - 46s 646ms/step - loss: 0.0169 - F1: 0.0000e+00 - val_loss: 620.5114 - val_F1: 0.9805\n",
      "73/73 [==============================] - 46s 627ms/step - loss: -0.0286 - F1: 0.0000e+00 - val_loss: 609.7242 - val_F1: 0.9743\n",
      "73/73 [==============================] - 45s 621ms/step - loss: -0.0104 - F1: 0.0000e+00 - val_loss: 615.2538 - val_F1: 0.9795\n",
      "73/73 [==============================] - 46s 627ms/step - loss: 0.0125 - F1: 0.0000e+00 - val_loss: 609.9885 - val_F1: 0.9788\n",
      "73/73 [==============================] - 45s 615ms/step - loss: 0.0112 - F1: 0.0000e+00 - val_loss: 647.5983 - val_F1: 0.9847\n",
      "73/73 [==============================] - 45s 618ms/step - loss: -0.0044 - F1: 0.0000e+00 - val_loss: 576.4059 - val_F1: 0.9704\n",
      "73/73 [==============================] - 45s 621ms/step - loss: 1.8283e-04 - F1: 0.0000e+00 - val_loss: 635.5500 - val_F1: 0.9714\n",
      "73/73 [==============================] - 46s 630ms/step - loss: 0.0129 - F1: 0.0000e+00 - val_loss: 665.5516 - val_F1: 0.9460\n",
      "73/73 [==============================] - 45s 622ms/step - loss: -0.0348 - F1: 0.0000e+00 - val_loss: 592.5578 - val_F1: 1.0000\n",
      "73/73 [==============================] - 46s 625ms/step - loss: 0.0046 - F1: 0.0000e+00 - val_loss: 604.2729 - val_F1: 1.0000\n",
      "71/71 [==============================] - 45s 641ms/step - loss: -0.0132 - F1: 0.0000e+00 - val_loss: 616.9393 - val_F1: 0.9775\n",
      "73/73 [==============================] - 45s 617ms/step - loss: 0.0128 - F1: 0.0000e+00 - val_loss: 608.0698 - val_F1: 0.9642\n",
      "73/73 [==============================] - 45s 614ms/step - loss: 0.0042 - F1: 0.0000e+00 - val_loss: 611.6791 - val_F1: 0.9788\n",
      "73/73 [==============================] - 45s 610ms/step - loss: -0.0160 - F1: 0.0000e+00 - val_loss: 607.5762 - val_F1: 0.9733\n",
      "73/73 [==============================] - 45s 613ms/step - loss: -0.0091 - F1: 0.0000e+00 - val_loss: 644.4355 - val_F1: 0.9831\n",
      "73/73 [==============================] - 45s 614ms/step - loss: 0.0100 - F1: 0.0000e+00 - val_loss: 573.0990 - val_F1: 0.9678\n",
      "73/73 [==============================] - 46s 626ms/step - loss: -0.0031 - F1: 0.0000e+00 - val_loss: 634.3298 - val_F1: 0.9626\n",
      "73/73 [==============================] - 46s 625ms/step - loss: -0.0011 - F1: 0.0000e+00 - val_loss: 664.2048 - val_F1: 0.9391\n",
      "73/73 [==============================] - 45s 621ms/step - loss: -0.0047 - F1: 0.0000e+00 - val_loss: 589.4490 - val_F1: 1.0000\n",
      "73/73 [==============================] - 45s 621ms/step - loss: 0.0196 - F1: 0.0000e+00 - val_loss: 601.1924 - val_F1: 1.0000\n",
      "71/71 [==============================] - 45s 639ms/step - loss: -0.0156 - F1: 0.0000e+00 - val_loss: 614.7731 - val_F1: 0.9740\n",
      "73/73 [==============================] - 46s 627ms/step - loss: 0.0047 - F1: 0.0000e+00 - val_loss: 604.1423 - val_F1: 0.9671\n",
      "73/73 [==============================] - 46s 625ms/step - loss: 1.7614e-04 - F1: 0.0000e+00 - val_loss: 610.0780 - val_F1: 0.9727\n",
      "73/73 [==============================] - 46s 625ms/step - loss: 0.0064 - F1: 0.0000e+00 - val_loss: 606.6103 - val_F1: 0.9645\n",
      "73/73 [==============================] - 45s 622ms/step - loss: -0.0028 - F1: 0.0000e+00 - val_loss: 644.2720 - val_F1: 0.9743\n",
      "73/73 [==============================] - 45s 617ms/step - loss: 0.0122 - F1: 0.0000e+00 - val_loss: 574.2811 - val_F1: 0.9489\n",
      "73/73 [==============================] - 46s 624ms/step - loss: 0.0072 - F1: 0.0000e+00 - val_loss: 634.1894 - val_F1: 0.9518\n",
      "73/73 [==============================] - 45s 617ms/step - loss: 0.0033 - F1: 0.0000e+00 - val_loss: 661.4007 - val_F1: 0.9401\n",
      "73/73 [==============================] - 46s 624ms/step - loss: 0.0084 - F1: 0.0000e+00 - val_loss: 586.2507 - val_F1: 1.0000\n",
      "73/73 [==============================] - 45s 619ms/step - loss: -0.0297 - F1: 0.0000e+00 - val_loss: 597.8586 - val_F1: 1.0000\n",
      "71/71 [==============================] - 46s 651ms/step - loss: 0.0107 - F1: 0.0000e+00 - val_loss: 610.7804 - val_F1: 0.9756\n",
      "73/73 [==============================] - 45s 617ms/step - loss: 9.1413e-04 - F1: 0.0000e+00 - val_loss: 602.8658 - val_F1: 0.9577\n",
      "73/73 [==============================] - 45s 620ms/step - loss: -0.0015 - F1: 0.0000e+00 - val_loss: 607.1101 - val_F1: 0.9707\n",
      "73/73 [==============================] - 45s 619ms/step - loss: -0.0042 - F1: 0.0000e+00 - val_loss: 605.7734 - val_F1: 0.9535\n",
      "73/73 [==============================] - 45s 616ms/step - loss: 0.0012 - F1: 0.0000e+00 - val_loss: 641.7491 - val_F1: 0.9727\n",
      "73/73 [==============================] - 45s 614ms/step - loss: 0.0059 - F1: 0.0000e+00 - val_loss: 571.9319 - val_F1: 0.9417\n",
      "73/73 [==============================] - 46s 624ms/step - loss: 0.0040 - F1: 0.0000e+00 - val_loss: 633.1048 - val_F1: 0.9440\n",
      "73/73 [==============================] - 46s 625ms/step - loss: -0.0102 - F1: 0.0000e+00 - val_loss: 659.2529 - val_F1: 0.9382\n",
      "73/73 [==============================] - 46s 636ms/step - loss: -0.0025 - F1: 0.0000e+00 - val_loss: 583.0787 - val_F1: 1.0000\n",
      "73/73 [==============================] - 45s 619ms/step - loss: -0.0126 - F1: 0.0000e+00 - val_loss: 595.0167 - val_F1: 1.0000\n",
      "71/71 [==============================] - 46s 646ms/step - loss: 0.0252 - F1: 0.0000e+00 - val_loss: 608.3644 - val_F1: 0.9743\n",
      "73/73 [==============================] - 45s 615ms/step - loss: -0.0029 - F1: 0.0000e+00 - val_loss: 597.6692 - val_F1: 0.9665\n",
      "73/73 [==============================] - 45s 613ms/step - loss: -0.0026 - F1: 0.0000e+00 - val_loss: 604.2891 - val_F1: 0.9717\n",
      "73/73 [==============================] - 45s 617ms/step - loss: 0.0030 - F1: 0.0000e+00 - val_loss: 602.1446 - val_F1: 0.9587\n",
      "73/73 [==============================] - 44s 606ms/step - loss: 0.0027 - F1: 0.0000e+00 - val_loss: 638.7278 - val_F1: 0.9795\n",
      "73/73 [==============================] - 44s 607ms/step - loss: 0.0010 - F1: 0.0000e+00 - val_loss: 569.2982 - val_F1: 0.9437\n",
      "73/73 [==============================] - 44s 606ms/step - loss: 9.7656e-04 - F1: 0.0000e+00 - val_loss: 631.4500 - val_F1: 0.9466\n",
      "73/73 [==============================] - 44s 603ms/step - loss: -0.0056 - F1: 0.0000e+00 - val_loss: 658.7812 - val_F1: 0.9398\n",
      "73/73 [==============================] - 45s 611ms/step - loss: 0.0070 - F1: 0.0000e+00 - val_loss: 582.4276 - val_F1: 1.0000\n",
      "73/73 [==============================] - 45s 612ms/step - loss: -0.0051 - F1: 0.0000e+00 - val_loss: 595.4295 - val_F1: 1.0000\n",
      "71/71 [==============================] - 45s 636ms/step - loss: 0.0050 - F1: 0.0000e+00 - val_loss: 611.5831 - val_F1: 0.9655\n",
      "73/73 [==============================] - 45s 617ms/step - loss: -0.0038 - F1: 0.0000e+00 - val_loss: 602.3097 - val_F1: 0.9538\n",
      "73/73 [==============================] - 45s 612ms/step - loss: -8.6731e-04 - F1: 0.0000e+00 - val_loss: 607.2060 - val_F1: 0.9674\n",
      "73/73 [==============================] - 45s 619ms/step - loss: 0.0042 - F1: 0.0000e+00 - val_loss: 607.1778 - val_F1: 0.9466\n",
      "73/73 [==============================] - 46s 625ms/step - loss: 5.7969e-05 - F1: 0.0000e+00 - val_loss: 643.3606 - val_F1: 0.9743\n",
      "73/73 [==============================] - 45s 620ms/step - loss: -0.0020 - F1: 0.0000e+00 - val_loss: 571.5503 - val_F1: 0.9417\n",
      "73/73 [==============================] - 46s 627ms/step - loss: 9.8994e-04 - F1: 0.0000e+00 - val_loss: 636.2032 - val_F1: 0.9427\n",
      "73/73 [==============================] - 46s 631ms/step - loss: 0.0034 - F1: 0.0000e+00 - val_loss: 665.1245 - val_F1: 0.9359\n",
      "73/73 [==============================] - 46s 631ms/step - loss: 0.0027 - F1: 0.0000e+00 - val_loss: 586.5295 - val_F1: 1.0000\n",
      "73/73 [==============================] - 45s 621ms/step - loss: 0.0017 - F1: 0.0000e+00 - val_loss: 600.2864 - val_F1: 1.0000\n"
     ]
    }
   ],
   "source": [
    "for j in range(30):\n",
    "    for i in range(10):\n",
    "        if i == 0:\n",
    "            tt_x = ten_fold_x[0:4500]; td_x = ten_fold_x[4500:];\n",
    "            tt_flb = ten_fold_flb[0:4500]; td_flb = ten_fold_flb[4500:];\n",
    "            tt_steal = ten_fold_steal[0:4500]; td_steal = ten_fold_steal[4500:];\n",
    "            tt_draw = ten_fold_draw[0:4500]; td_draw = ten_fold_draw[4500:];\n",
    "            tt_consume = ten_fold_consume[0:4500]; td_consume = ten_fold_consume[4500:];\n",
    "            tt_sale = ten_fold_sale[0:4500]; td_sale = ten_fold_sale[4500:];\n",
    "            tt_volun = ten_fold_volun[0:4500]; td_volun = ten_fold_volun[4500:];\n",
    "            tt_steal_dis = ten_fold_steal_dis[0:4500]; td_steal_dis = ten_fold_steal_dis[4500:];\n",
    "            tt_draw_dis = ten_fold_draw_dis[0:4500]; td_draw_dis = ten_fold_draw_dis[4500:];\n",
    "            tt_consume_dis = ten_fold_consume_dis[0:4500]; td_consume_dis = ten_fold_consume_dis[4500:];\n",
    "            tt_sale_dis = ten_fold_sale_dis[0:4500]; td_sale_dis = ten_fold_sale_dis[4500:];\n",
    "            tt_volun_dis = ten_fold_volun_dis[0:4500]; td_volun_dis = ten_fold_volun_dis[4500:];\n",
    "            tt_y = ten_fold_y[0:4500]; td_y = ten_fold_y[4500:]\n",
    "            tt_fx = (tt_x, tt_flb, tt_steal, tt_draw, tt_consume, tt_sale, tt_volun,\n",
    "                     tt_steal_dis, tt_draw_dis, tt_consume_dis, tt_sale_dis, tt_volun_dis)\n",
    "            td_fx = (td_x, td_flb, td_steal, td_draw, td_consume, td_sale, td_volun,\n",
    "                     td_steal_dis, td_draw_dis, td_consume_dis, td_sale_dis, td_volun_dis)\n",
    "            model.fit(tt_fx,\n",
    "                      tt_y,\n",
    "                      td_fx,\n",
    "                      td_y,\n",
    "                      epochs=1,\n",
    "                      batch_size=64)\n",
    "        elif i == 1:\n",
    "            tt_x = ten_fold_x[0:4000]+ten_fold_x[4500:]; td_x = ten_fold_x[4000:4500];\n",
    "            tt_flb = ten_fold_flb[0:4000]+ten_fold_flb[4500:]; td_flb = ten_fold_flb[4000:4500];\n",
    "            tt_steal = ten_fold_steal[0:4000]+ten_fold_steal[4500:]; td_steal = ten_fold_steal[4000:4500];\n",
    "            tt_draw = ten_fold_draw[0:4000]+ten_fold_draw[4500:]; td_draw = ten_fold_draw[4000:4500];\n",
    "            tt_consume = ten_fold_consume[0:4000]+ten_fold_consume[4500:]; td_consume = ten_fold_consume[4000:4500];\n",
    "            tt_sale = ten_fold_sale[0:4000]+ten_fold_sale[4500:]; td_sale = ten_fold_sale[4000:4500];\n",
    "            tt_volun = ten_fold_volun[0:4000]+ten_fold_volun[4500:]; td_volun = ten_fold_volun[4000:4500];\n",
    "            tt_steal_dis = ten_fold_steal_dis[0:4000]+ten_fold_steal_dis[4500:]; td_steal_dis = ten_fold_steal_dis[4000:4500];\n",
    "            tt_draw_dis = ten_fold_draw_dis[0:4000]+ten_fold_draw_dis[4500:]; td_draw_dis = ten_fold_draw_dis[4000:4500];\n",
    "            tt_consume_dis = ten_fold_consume_dis[0:4000]+ten_fold_consume_dis[4500:]; td_consume_dis = ten_fold_consume_dis[4000:4500];\n",
    "            tt_sale_dis = ten_fold_sale_dis[0:4000]+ten_fold_sale_dis[4500:]; td_sale_dis = ten_fold_sale_dis[4000:4500];\n",
    "            tt_volun_dis = ten_fold_volun_dis[0:4000]+ten_fold_volun_dis[4500:]; td_volun_dis = ten_fold_volun_dis[4000:4500];\n",
    "            tt_y = ten_fold_y[0:4000]+ten_fold_y[4500:]; td_y = ten_fold_y[4000:4500]\n",
    "            tt_fx = (tt_x, tt_flb, tt_steal, tt_draw, tt_consume, tt_sale, tt_volun,\n",
    "                     tt_steal_dis, tt_draw_dis, tt_consume_dis, tt_sale_dis, tt_volun_dis)\n",
    "            td_fx = (td_x, td_flb, td_steal, td_draw, td_consume, td_sale, td_volun,\n",
    "                     td_steal_dis, td_draw_dis, td_consume_dis, td_sale_dis, td_volun_dis)\n",
    "            model.fit(tt_fx,\n",
    "                      tt_y,\n",
    "                      td_fx,\n",
    "                      td_y,\n",
    "                      epochs=1,\n",
    "                      batch_size=64)\n",
    "        elif i == 2:\n",
    "            tt_x = ten_fold_x[0:3500]+ten_fold_x[4000:]; td_x = ten_fold_x[3500:4000];\n",
    "            tt_flb = ten_fold_flb[0:3500]+ten_fold_flb[4000:]; td_flb = ten_fold_flb[3500:4000];\n",
    "            tt_steal = ten_fold_steal[0:3500]+ten_fold_steal[4000:]; td_steal = ten_fold_steal[3500:4000];\n",
    "            tt_draw = ten_fold_draw[0:3500]+ten_fold_draw[4000:]; td_draw = ten_fold_draw[3500:4000];\n",
    "            tt_consume = ten_fold_consume[0:3500]+ten_fold_consume[4000:]; td_consume = ten_fold_consume[3500:4000];\n",
    "            tt_sale = ten_fold_sale[0:3500]+ten_fold_sale[4000:]; td_sale = ten_fold_sale[3500:4000];\n",
    "            tt_volun = ten_fold_volun[0:3500]+ten_fold_volun[4000:]; td_volun = ten_fold_volun[3500:4000];\n",
    "            tt_steal_dis = ten_fold_steal_dis[0:3500]+ten_fold_steal_dis[4000:]; td_steal_dis = ten_fold_steal_dis[3500:4000];\n",
    "            tt_draw_dis = ten_fold_draw_dis[0:3500]+ten_fold_draw_dis[4000:]; td_draw_dis = ten_fold_draw_dis[3500:4000];\n",
    "            tt_consume_dis = ten_fold_consume_dis[0:3500]+ten_fold_consume_dis[4000:]; td_consume_dis = ten_fold_consume_dis[3500:4000];\n",
    "            tt_sale_dis = ten_fold_sale_dis[0:3500]+ten_fold_sale_dis[4000:]; td_sale_dis = ten_fold_sale_dis[3500:4000];\n",
    "            tt_volun_dis = ten_fold_volun_dis[0:3500]+ten_fold_volun_dis[4000:]; td_volun_dis = ten_fold_volun_dis[3500:4000];\n",
    "            tt_y = ten_fold_y[0:3500]+ten_fold_y[4000:]; td_y = ten_fold_y[3500:4000]\n",
    "            tt_fx = (tt_x, tt_flb, tt_steal, tt_draw, tt_consume, tt_sale, tt_volun,\n",
    "                     tt_steal_dis, tt_draw_dis, tt_consume_dis, tt_sale_dis, tt_volun_dis)\n",
    "            td_fx = (td_x, td_flb, td_steal, td_draw, td_consume, td_sale, td_volun,\n",
    "                     td_steal_dis, td_draw_dis, td_consume_dis, td_sale_dis, td_volun_dis)\n",
    "            model.fit(tt_fx,\n",
    "                      tt_y,\n",
    "                      td_fx,\n",
    "                      td_y,\n",
    "                      epochs=1,\n",
    "                      batch_size=64)\n",
    "        elif i == 3:\n",
    "            tt_x = ten_fold_x[0:3000]+ten_fold_x[3500:]; td_x = ten_fold_x[3000:3500];\n",
    "            tt_flb = ten_fold_flb[0:3000]+ten_fold_flb[3500:]; td_flb = ten_fold_flb[3000:3500];\n",
    "            tt_steal = ten_fold_steal[0:3000]+ten_fold_steal[3500:]; td_steal = ten_fold_steal[3000:3500];\n",
    "            tt_draw = ten_fold_draw[0:3000]+ten_fold_draw[3500:]; td_draw = ten_fold_draw[3000:3500];\n",
    "            tt_consume = ten_fold_consume[0:3000]+ten_fold_consume[3500:]; td_consume = ten_fold_consume[3000:3500];\n",
    "            tt_sale = ten_fold_sale[0:3000]+ten_fold_sale[3500:]; td_sale = ten_fold_sale[3000:3500];\n",
    "            tt_volun = ten_fold_volun[0:3000]+ten_fold_volun[3500:]; td_volun = ten_fold_volun[3000:3500];\n",
    "            tt_steal_dis = ten_fold_steal_dis[0:3000]+ten_fold_steal_dis[3500:]; td_steal_dis = ten_fold_steal_dis[3000:3500];\n",
    "            tt_draw_dis = ten_fold_draw_dis[0:3000]+ten_fold_draw_dis[3500:]; td_draw_dis = ten_fold_draw_dis[3000:3500];\n",
    "            tt_consume_dis = ten_fold_consume_dis[0:3000]+ten_fold_consume_dis[3500:]; td_consume_dis = ten_fold_consume_dis[3000:3500];\n",
    "            tt_sale_dis = ten_fold_sale_dis[0:3000]+ten_fold_sale_dis[3500:]; td_sale_dis = ten_fold_sale_dis[3000:3500];\n",
    "            tt_volun_dis = ten_fold_volun_dis[0:3000]+ten_fold_volun_dis[3500:]; td_volun_dis = ten_fold_volun_dis[3000:3500];\n",
    "            tt_y = ten_fold_y[0:3000]+ten_fold_y[3500:]; td_y = ten_fold_y[3000:3500]\n",
    "            tt_fx = (tt_x, tt_flb, tt_steal, tt_draw, tt_consume, tt_sale, tt_volun,\n",
    "                     tt_steal_dis, tt_draw_dis, tt_consume_dis, tt_sale_dis, tt_volun_dis)\n",
    "            td_fx = (td_x, td_flb, td_steal, td_draw, td_consume, td_sale, td_volun,\n",
    "                     td_steal_dis, td_draw_dis, td_consume_dis, td_sale_dis, td_volun_dis)\n",
    "            model.fit(tt_fx,\n",
    "                      tt_y,\n",
    "                      td_fx,\n",
    "                      td_y,\n",
    "                      epochs=1,\n",
    "                      batch_size=64)\n",
    "        elif i == 4:\n",
    "            tt_x = ten_fold_x[0:2500]+ten_fold_x[3000:]; td_x = ten_fold_x[2500:3000];\n",
    "            tt_flb = ten_fold_flb[0:2500]+ten_fold_flb[3000:]; td_flb = ten_fold_flb[2500:3000];\n",
    "            tt_steal = ten_fold_steal[0:2500]+ten_fold_steal[3000:]; td_steal = ten_fold_steal[2500:3000];\n",
    "            tt_draw = ten_fold_draw[0:2500]+ten_fold_draw[3000:]; td_draw = ten_fold_draw[2500:3000];\n",
    "            tt_consume = ten_fold_consume[0:2500]+ten_fold_consume[3000:]; td_consume = ten_fold_consume[2500:3000];\n",
    "            tt_sale = ten_fold_sale[0:2500]+ten_fold_sale[3000:]; td_sale = ten_fold_sale[2500:3000];\n",
    "            tt_volun = ten_fold_volun[0:2500]+ten_fold_volun[3000:]; td_volun = ten_fold_volun[2500:3000];\n",
    "            tt_steal_dis = ten_fold_steal_dis[0:2500]+ten_fold_steal_dis[3000:]; td_steal_dis = ten_fold_steal_dis[2500:3000];\n",
    "            tt_draw_dis = ten_fold_draw_dis[0:2500]+ten_fold_draw_dis[3000:]; td_draw_dis = ten_fold_draw_dis[2500:3000];\n",
    "            tt_consume_dis = ten_fold_consume_dis[0:2500]+ten_fold_consume_dis[3000:]; td_consume_dis = ten_fold_consume_dis[2500:3000];\n",
    "            tt_sale_dis = ten_fold_sale_dis[0:2500]+ten_fold_sale_dis[3000:]; td_sale_dis = ten_fold_sale_dis[2500:3000];\n",
    "            tt_volun_dis = ten_fold_volun_dis[0:2500]+ten_fold_volun_dis[3000:]; td_volun_dis = ten_fold_volun_dis[2500:3000];\n",
    "            tt_y = ten_fold_y[0:2500]+ten_fold_y[3000:]; td_y = ten_fold_y[2500:3000]\n",
    "            tt_fx = (tt_x, tt_flb, tt_steal, tt_draw, tt_consume, tt_sale, tt_volun,\n",
    "                     tt_steal_dis, tt_draw_dis, tt_consume_dis, tt_sale_dis, tt_volun_dis)\n",
    "            td_fx = (td_x, td_flb, td_steal, td_draw, td_consume, td_sale, td_volun,\n",
    "                     td_steal_dis, td_draw_dis, td_consume_dis, td_sale_dis, td_volun_dis)\n",
    "            model.fit(tt_fx,\n",
    "                      tt_y,\n",
    "                      td_fx,\n",
    "                      td_y,\n",
    "                      epochs=1,\n",
    "                      batch_size=64)\n",
    "        elif i == 5:\n",
    "            tt_x = ten_fold_x[0:2000]+ten_fold_x[2500:]; td_x = ten_fold_x[2000:2500];\n",
    "            tt_flb = ten_fold_flb[0:2000]+ten_fold_flb[2500:]; td_flb = ten_fold_flb[2000:2500];\n",
    "            tt_steal = ten_fold_steal[0:2000]+ten_fold_steal[2500:]; td_steal = ten_fold_steal[2000:2500];\n",
    "            tt_draw = ten_fold_draw[0:2000]+ten_fold_draw[2500:]; td_draw = ten_fold_draw[2000:2500];\n",
    "            tt_consume = ten_fold_consume[0:2000]+ten_fold_consume[2500:]; td_consume = ten_fold_consume[2000:2500];\n",
    "            tt_sale = ten_fold_sale[0:2000]+ten_fold_sale[2500:]; td_sale = ten_fold_sale[2000:2500];\n",
    "            tt_volun = ten_fold_volun[0:2000]+ten_fold_volun[2500:]; td_volun = ten_fold_volun[2000:2500];\n",
    "            tt_steal_dis = ten_fold_steal_dis[0:2000]+ten_fold_steal_dis[2500:]; td_steal_dis = ten_fold_steal_dis[2000:2500];\n",
    "            tt_draw_dis = ten_fold_draw_dis[0:2000]+ten_fold_draw_dis[2500:]; td_draw_dis = ten_fold_draw_dis[2000:2500];\n",
    "            tt_consume_dis = ten_fold_consume_dis[0:2000]+ten_fold_consume_dis[2500:]; td_consume_dis = ten_fold_consume_dis[2000:2500];\n",
    "            tt_sale_dis = ten_fold_sale_dis[0:2000]+ten_fold_sale_dis[2500:]; td_sale_dis = ten_fold_sale_dis[2000:2500];\n",
    "            tt_volun_dis = ten_fold_volun_dis[0:2000]+ten_fold_volun_dis[2500:]; td_volun_dis = ten_fold_volun_dis[2000:2500];\n",
    "            tt_y = ten_fold_y[0:2000]+ten_fold_y[2500:]; td_y = ten_fold_y[2000:2500]\n",
    "            tt_fx = (tt_x, tt_flb, tt_steal, tt_draw, tt_consume, tt_sale, tt_volun,\n",
    "                     tt_steal_dis, tt_draw_dis, tt_consume_dis, tt_sale_dis, tt_volun_dis)\n",
    "            td_fx = (td_x, td_flb, td_steal, td_draw, td_consume, td_sale, td_volun,\n",
    "                     td_steal_dis, td_draw_dis, td_consume_dis, td_sale_dis, td_volun_dis)\n",
    "            model.fit(tt_fx,\n",
    "                      tt_y,\n",
    "                      td_fx,\n",
    "                      td_y,\n",
    "                      epochs=1,\n",
    "                      batch_size=64)\n",
    "        elif i == 6:\n",
    "            tt_x = ten_fold_x[0:1500]+ten_fold_x[2000:]; td_x = ten_fold_x[1500:2000];\n",
    "            tt_flb = ten_fold_flb[0:1500]+ten_fold_flb[2000:]; td_flb = ten_fold_flb[1500:2000];\n",
    "            tt_steal = ten_fold_steal[0:1500]+ten_fold_steal[2000:]; td_steal = ten_fold_steal[1500:2000];\n",
    "            tt_draw = ten_fold_draw[0:1500]+ten_fold_draw[2000:]; td_draw = ten_fold_draw[1500:2000];\n",
    "            tt_consume = ten_fold_consume[0:1500]+ten_fold_consume[2000:]; td_consume = ten_fold_consume[1500:2000];\n",
    "            tt_sale = ten_fold_sale[0:1500]+ten_fold_sale[2000:]; td_sale = ten_fold_sale[1500:2000];\n",
    "            tt_volun = ten_fold_volun[0:1500]+ten_fold_volun[2000:]; td_volun = ten_fold_volun[1500:2000];\n",
    "            tt_steal_dis = ten_fold_steal_dis[0:1500]+ten_fold_steal_dis[2000:]; td_steal_dis = ten_fold_steal_dis[1500:2000];\n",
    "            tt_draw_dis = ten_fold_draw_dis[0:1500]+ten_fold_draw_dis[2000:]; td_draw_dis = ten_fold_draw_dis[1500:2000];\n",
    "            tt_consume_dis = ten_fold_consume_dis[0:1500]+ten_fold_consume_dis[2000:]; td_consume_dis = ten_fold_consume_dis[1500:2000];\n",
    "            tt_sale_dis = ten_fold_sale_dis[0:1500]+ten_fold_sale_dis[2000:]; td_sale_dis = ten_fold_sale_dis[1500:2000];\n",
    "            tt_volun_dis = ten_fold_volun_dis[0:1500]+ten_fold_volun_dis[2000:]; td_volun_dis = ten_fold_volun_dis[1500:2000];\n",
    "            tt_y = ten_fold_y[0:1500]+ten_fold_y[2000:]; td_y = ten_fold_y[1500:2000]\n",
    "            tt_fx = (tt_x, tt_flb, tt_steal, tt_draw, tt_consume, tt_sale, tt_volun,\n",
    "                     tt_steal_dis, tt_draw_dis, tt_consume_dis, tt_sale_dis, tt_volun_dis)\n",
    "            td_fx = (td_x, td_flb, td_steal, td_draw, td_consume, td_sale, td_volun,\n",
    "                     td_steal_dis, td_draw_dis, td_consume_dis, td_sale_dis, td_volun_dis)\n",
    "            model.fit(tt_fx,\n",
    "                      tt_y,\n",
    "                      td_fx,\n",
    "                      td_y,\n",
    "                      epochs=1,\n",
    "                      batch_size=64)\n",
    "        elif i == 7:\n",
    "            tt_x = ten_fold_x[0:1000]+ten_fold_x[1500:]; td_x = ten_fold_x[1000:1500];\n",
    "            tt_flb = ten_fold_flb[0:1000]+ten_fold_flb[1500:]; td_flb = ten_fold_flb[1000:1500];\n",
    "            tt_steal = ten_fold_steal[0:1000]+ten_fold_steal[1500:]; td_steal = ten_fold_steal[1000:1500];\n",
    "            tt_draw = ten_fold_draw[0:1000]+ten_fold_draw[1500:]; td_draw = ten_fold_draw[1000:1500];\n",
    "            tt_consume = ten_fold_consume[0:1000]+ten_fold_consume[1500:]; td_consume = ten_fold_consume[1000:1500];\n",
    "            tt_sale = ten_fold_sale[0:1000]+ten_fold_sale[1500:]; td_sale = ten_fold_sale[1000:1500];\n",
    "            tt_volun = ten_fold_volun[0:1000]+ten_fold_volun[1500:]; td_volun = ten_fold_volun[1000:1500];\n",
    "            tt_steal_dis = ten_fold_steal_dis[0:1000]+ten_fold_steal_dis[1500:]; td_steal_dis = ten_fold_steal_dis[1000:1500];\n",
    "            tt_draw_dis = ten_fold_draw_dis[0:1000]+ten_fold_draw_dis[1500:]; td_draw_dis = ten_fold_draw_dis[1000:1500];\n",
    "            tt_consume_dis = ten_fold_consume_dis[0:1000]+ten_fold_consume_dis[1500:]; td_consume_dis = ten_fold_consume_dis[1000:1500];\n",
    "            tt_sale_dis = ten_fold_sale_dis[0:1000]+ten_fold_sale_dis[1500:]; td_sale_dis = ten_fold_sale_dis[1000:1500];\n",
    "            tt_volun_dis = ten_fold_volun_dis[0:1000]+ten_fold_volun_dis[1500:]; td_volun_dis = ten_fold_volun_dis[1000:1500];\n",
    "            tt_y = ten_fold_y[0:1000]+ten_fold_y[1500:]; td_y = ten_fold_y[1000:1500]\n",
    "            tt_fx = (tt_x, tt_flb, tt_steal, tt_draw, tt_consume, tt_sale, tt_volun,\n",
    "                     tt_steal_dis, tt_draw_dis, tt_consume_dis, tt_sale_dis, tt_volun_dis)\n",
    "            td_fx = (td_x, td_flb, td_steal, td_draw, td_consume, td_sale, td_volun,\n",
    "                     td_steal_dis, td_draw_dis, td_consume_dis, td_sale_dis, td_volun_dis)\n",
    "            model.fit(tt_fx,\n",
    "                      tt_y,\n",
    "                      td_fx,\n",
    "                      td_y,\n",
    "                      epochs=1,\n",
    "                      batch_size=64)\n",
    "        elif i == 8:\n",
    "            tt_x = ten_fold_x[0:500]+ten_fold_x[1000:]; td_x = ten_fold_x[500:1000];\n",
    "            tt_flb = ten_fold_flb[0:500]+ten_fold_flb[1000:]; td_flb = ten_fold_flb[500:1000];\n",
    "            tt_steal = ten_fold_steal[0:500]+ten_fold_steal[1000:]; td_steal = ten_fold_steal[500:1000];\n",
    "            tt_draw = ten_fold_draw[0:500]+ten_fold_draw[1000:]; td_draw = ten_fold_draw[500:1000];\n",
    "            tt_consume = ten_fold_consume[0:500]+ten_fold_consume[1000:]; td_consume = ten_fold_consume[500:1000];\n",
    "            tt_sale = ten_fold_sale[0:500]+ten_fold_sale[1000:]; td_sale = ten_fold_sale[500:1000];\n",
    "            tt_volun = ten_fold_volun[0:500]+ten_fold_volun[1000:]; td_volun = ten_fold_volun[500:1000];\n",
    "            tt_steal_dis = ten_fold_steal_dis[0:500]+ten_fold_steal_dis[1000:]; td_steal_dis = ten_fold_steal_dis[500:1000];\n",
    "            tt_draw_dis = ten_fold_draw_dis[0:500]+ten_fold_draw_dis[1000:]; td_draw_dis = ten_fold_draw_dis[500:1000];\n",
    "            tt_consume_dis = ten_fold_consume_dis[0:500]+ten_fold_consume_dis[1000:]; td_consume_dis = ten_fold_consume_dis[500:1000];\n",
    "            tt_sale_dis = ten_fold_sale_dis[0:500]+ten_fold_sale_dis[1000:]; td_sale_dis = ten_fold_sale_dis[500:1000];\n",
    "            tt_volun_dis = ten_fold_volun_dis[0:500]+ten_fold_volun_dis[1000:]; td_volun_dis = ten_fold_volun_dis[500:1000];\n",
    "            tt_y = ten_fold_y[0:500]+ten_fold_y[1000:]; td_y = ten_fold_y[500:1000]\n",
    "            tt_fx = (tt_x, tt_flb, tt_steal, tt_draw, tt_consume, tt_sale, tt_volun,\n",
    "                     tt_steal_dis, tt_draw_dis, tt_consume_dis, tt_sale_dis, tt_volun_dis)\n",
    "            td_fx = (td_x, td_flb, td_steal, td_draw, td_consume, td_sale, td_volun,\n",
    "                     td_steal_dis, td_draw_dis, td_consume_dis, td_sale_dis, td_volun_dis)\n",
    "            model.fit(tt_fx,\n",
    "                      tt_y,\n",
    "                      td_fx,\n",
    "                      td_y,\n",
    "                      epochs=1,\n",
    "                      batch_size=64)\n",
    "        elif i == 9:\n",
    "            tt_x = ten_fold_x[500:]; td_x = ten_fold_x[0:500];\n",
    "            tt_flb = ten_fold_flb[500:]; td_flb = ten_fold_flb[0:500];\n",
    "            tt_steal = ten_fold_steal[500:]; td_steal = ten_fold_steal[0:500];\n",
    "            tt_draw = ten_fold_draw[500:]; td_draw = ten_fold_draw[0:500];\n",
    "            tt_consume = ten_fold_consume[500:]; td_consume = ten_fold_consume[0:500];\n",
    "            tt_sale = ten_fold_sale[500:]; td_sale = ten_fold_sale[0:500];\n",
    "            tt_volun = ten_fold_volun[500:]; td_volun = ten_fold_volun[0:500];\n",
    "            tt_steal_dis = ten_fold_steal_dis[500:]; td_steal_dis = ten_fold_steal_dis[0:500];\n",
    "            tt_draw_dis = ten_fold_draw_dis[500:]; td_draw_dis = ten_fold_draw_dis[0:500];\n",
    "            tt_consume_dis = ten_fold_consume_dis[500:]; td_consume_dis = ten_fold_consume_dis[0:500];\n",
    "            tt_sale_dis = ten_fold_sale_dis[500:]; td_sale_dis = ten_fold_sale_dis[0:500];\n",
    "            tt_volun_dis = ten_fold_volun_dis[500:]; td_volun_dis = ten_fold_volun_dis[0:500];\n",
    "            tt_y = ten_fold_y[500:]; td_y = ten_fold_y[0:500]\n",
    "            tt_fx = (tt_x, tt_flb, tt_steal, tt_draw, tt_consume, tt_sale, tt_volun,\n",
    "                     tt_steal_dis, tt_draw_dis, tt_consume_dis, tt_sale_dis, tt_volun_dis)\n",
    "            td_fx = (td_x, td_flb, td_steal, td_draw, td_consume, td_sale, td_volun,\n",
    "                     td_steal_dis, td_draw_dis, td_consume_dis, td_sale_dis, td_volun_dis)\n",
    "            model.fit(tt_fx,\n",
    "                      tt_y,\n",
    "                      td_fx,\n",
    "                      td_y,\n",
    "                      epochs=1,\n",
    "                      batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "uuid": "2b5cd603-e772-4f64-bc06-19e284f49e25"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "PersonVolunteer     0.9091    0.1923    0.3175        52\n",
      "     ThiefSteal     0.8707    0.7804    0.8231       906\n",
      "    VictimSteal     0.8585    0.6994    0.7708       928\n",
      "      MoneyDraw     0.7431    0.7085    0.7254       343\n",
      "   LocationDraw     0.5161    0.5590    0.5367       229\n",
      "     CountSteal     0.8868    0.6689    0.7625       761\n",
      "    ThingsSteal     0.7555    0.5234    0.6184      1175\n",
      "       TimeSale     0.0000    0.0000    0.0000        15\n",
      "      TimeSteal     0.5727    0.8131    0.6720       717\n",
      "     MoneySteal     0.1879    0.0793    0.1115       391\n",
      "      MoneySale     0.0000    0.0000    0.0000        30\n",
      "   MoneyConsume     0.0000    0.0000    0.0000        32\n",
      "  LocationSteal     0.4419    0.5069    0.4722       653\n",
      "     PersonDraw     0.7941    0.2000    0.3195       135\n",
      "       TimeDraw     0.3333    0.0794    0.1282       126\n",
      "     ThingsSale     0.5714    0.0833    0.1455        48\n",
      "  PersonConsume     0.0000    0.0000    0.0000        22\n",
      "LocationConsume     0.0000    0.0000    0.0000        21\n",
      "     PersonSale     0.0000    0.0000    0.0000        28\n",
      "    TimeConsume     0.0000    0.0000    0.0000         4\n",
      "\n",
      "      micro avg     0.6429    0.5815    0.6106      6616\n",
      "      macro avg     0.6828    0.5815    0.6131      6616\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'                 precision    recall  f1-score   support\\n\\nPersonVolunteer     0.9091    0.1923    0.3175        52\\n     ThiefSteal     0.8707    0.7804    0.8231       906\\n    VictimSteal     0.8585    0.6994    0.7708       928\\n      MoneyDraw     0.7431    0.7085    0.7254       343\\n   LocationDraw     0.5161    0.5590    0.5367       229\\n     CountSteal     0.8868    0.6689    0.7625       761\\n    ThingsSteal     0.7555    0.5234    0.6184      1175\\n       TimeSale     0.0000    0.0000    0.0000        15\\n      TimeSteal     0.5727    0.8131    0.6720       717\\n     MoneySteal     0.1879    0.0793    0.1115       391\\n      MoneySale     0.0000    0.0000    0.0000        30\\n   MoneyConsume     0.0000    0.0000    0.0000        32\\n  LocationSteal     0.4419    0.5069    0.4722       653\\n     PersonDraw     0.7941    0.2000    0.3195       135\\n       TimeDraw     0.3333    0.0794    0.1282       126\\n     ThingsSale     0.5714    0.0833    0.1455        48\\n  PersonConsume     0.0000    0.0000    0.0000        22\\nLocationConsume     0.0000    0.0000    0.0000        21\\n     PersonSale     0.0000    0.0000    0.0000        28\\n    TimeConsume     0.0000    0.0000    0.0000         4\\n\\n      micro avg     0.6429    0.5815    0.6106      6616\\n      macro avg     0.6828    0.5815    0.6131      6616\\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_fx, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "uuid": "c5a91648-7d20-4bfa-a79e-d27fdc1490f8"
   },
   "outputs": [],
   "source": [
    "model.save('Saved_Model/p2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "uuid": "a50e6d4d-357a-4f47-8710-00e14c351510"
   },
   "outputs": [],
   "source": [
    "model.info()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
