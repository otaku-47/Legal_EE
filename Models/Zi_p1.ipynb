{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "uuid": "35e89b75-f160-490e-ac8c-46950a0bed9a"
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "import kashgari\n",
    "from kashgari.tasks.labeling import BiLSTM_CRF_Model\n",
    "from kashgari.embeddings import BERTEmbedding\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "uuid": "8e20c466-c0fa-4c06-a3fc-5bd9eb82065c"
   },
   "outputs": [],
   "source": [
    "class LossHistory(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = {'batch':[], 'epoch':[]}\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.losses['batch'].append(logs.get('loss'))\n",
    "        \n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        self.losses['epoch'].append(logs.get('loss'))\n",
    "\n",
    "    def loss_plot(self, loss_type):\n",
    "        iters = range(len(self.losses[loss_type]))\n",
    "        plt.figure()\n",
    "        # loss\n",
    "        plt.plot(iters, self.losses[loss_type], 'g', label='train loss')\n",
    "        plt.grid(True)\n",
    "        plt.xlabel(loss_type)\n",
    "        plt.ylabel('loss')\n",
    "        plt.legend(loc=\"upper right\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "uuid": "ac90fea5-59fd-41a4-a1a5-92a8e3346960"
   },
   "outputs": [],
   "source": [
    "# 初始化训练数据\n",
    "train_x = []\n",
    "train_y = []\n",
    "\n",
    "with open('train_zi.txt', 'r', encoding='utf-8') as f:\n",
    "    tmp_x = []\n",
    "    tmp_y = []\n",
    "    for i in f.readlines():\n",
    "        if i == '\\n':\n",
    "            train_x.append(tmp_x)\n",
    "            train_y.append(tmp_y)\n",
    "            tmp_x = []\n",
    "            tmp_y = []\n",
    "        else:\n",
    "            x, y = i.split(' ')\n",
    "            tmp_x.append(x)\n",
    "            tmp_list = y.split('-')\n",
    "            if len(tmp_list) == 1:\n",
    "                tmp_y.append(tmp_list[0].replace('\\n', ''))\n",
    "            else:\n",
    "                tmp_str = tmp_list[0] + '-' + tmp_list[1].replace('\\n', '')\n",
    "                tmp_y.append(tmp_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "uuid": "5425cc15-e9a2-4c65-8c3b-43d067a25283"
   },
   "outputs": [],
   "source": [
    "# 初始化测试数据\n",
    "test_x = []\n",
    "test_y = []\n",
    "\n",
    "with open('test_zi.txt', 'r', encoding='utf-8') as f:\n",
    "    tmp_x = []\n",
    "    tmp_y = []\n",
    "    for i in f.readlines():\n",
    "        if i == '\\n':\n",
    "            test_x.append(tmp_x)\n",
    "            test_y.append(tmp_y)\n",
    "            tmp_x = []\n",
    "            tmp_y = []\n",
    "        else:\n",
    "            x, y = i.split(' ')\n",
    "            tmp_x.append(x)\n",
    "            tmp_list = y.split('-')\n",
    "            if len(tmp_list) == 1:\n",
    "                tmp_y.append(tmp_list[0].replace('\\n', ''))\n",
    "            else:\n",
    "                tmp_str = tmp_list[0] + '-' + tmp_list[1].replace('\\n', '')\n",
    "                tmp_y.append(tmp_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "uuid": "0c8a831d-704a-496d-91ad-89d4a4ab55b2"
   },
   "outputs": [],
   "source": [
    "# 初始化验证数据\n",
    "dev_x = []\n",
    "dev_y = []\n",
    "\n",
    "with open('dev_zi.txt', 'r', encoding='utf-8') as f:\n",
    "    tmp_x = []\n",
    "    tmp_y = []\n",
    "    for i in f.readlines():\n",
    "        if i == '\\n':\n",
    "            dev_x.append(tmp_x)\n",
    "            dev_y.append(tmp_y)\n",
    "            tmp_x = []\n",
    "            tmp_y = []\n",
    "        else:\n",
    "            x, y = i.split(' ')\n",
    "            tmp_x.append(x)\n",
    "            tmp_list = y.split('-')\n",
    "            if len(tmp_list) == 1:\n",
    "                tmp_y.append(tmp_list[0].replace('\\n', ''))\n",
    "            else:\n",
    "                tmp_str = tmp_list[0] + '-' + tmp_list[1].replace('\\n', '')\n",
    "                tmp_y.append(tmp_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "uuid": "8b3a1ae6-cdee-48cf-9303-9f0eb6447886"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7fa60e0ecda0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7fa60e0ecda0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7fa60e819dd8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7fa60e819dd8>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa60d7556d8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa60d7556d8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fa60e7e3a90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fa60e7e3a90>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fa60d63d7b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fa60d63d7b8>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa60d7317f0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa60d7317f0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fa60d684748>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fa60d684748>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa60d43b6d8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa60d43b6d8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fa60d37be80>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fa60d37be80>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fa60d1b4c50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fa60d1b4c50>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa60d3c6cc0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa60d3c6cc0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fa60cfa95c0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fa60cfa95c0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa60ceef1d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa60ceef1d0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fa60cef0fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fa60cef0fd0>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fa60ccd4668>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fa60ccd4668>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa60cdd4ac8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa60cdd4ac8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fa60cd1f358>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fa60cd1f358>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa60cb24a58>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa60cb24a58>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fa60ca33e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fa60ca33e10>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fa60c860a90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fa60c860a90>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa60ca81eb8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa60ca81eb8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fa60c85b048>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fa60c85b048>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa60c8ea860>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa60c8ea860>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fa6097df940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fa6097df940>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fa6099865c0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fa6099865c0>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa6097c3710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa6097c3710>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fa609691a58>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fa609691a58>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa6093299b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa6093299b0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fa6093eae48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fa6093eae48>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fa60916bbe0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fa60916bbe0>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa6093321d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa6093321d0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fa6091cf0f0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fa6091cf0f0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa608f91a58>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa608f91a58>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fa608e931d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fa608e931d0>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fa608cf6c50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fa608cf6c50>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa608ec3748>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa608ec3748>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fa608aea5c0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fa608aea5c0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa608d39160>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa608d39160>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fa608a2fa58>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fa608a2fa58>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fa60881c550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fa60881c550>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa6089d47f0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa6089d47f0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fa608864780>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fa608864780>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa60d4969e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa60d4969e8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fa60856d5f8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fa60856d5f8>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fa6083a8ba8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fa6083a8ba8>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa608498dd8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa608498dd8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fa6081a4748>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fa6081a4748>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa6083ecef0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa6083ecef0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fa6080e4eb8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fa6080e4eb8>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fa608156b70>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fa608156b70>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa6080499e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa6080499e8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fa607f19c18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fa607f19c18>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa607ceebe0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa607ceebe0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fa607c2d8d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fa607c2d8d0>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fa607a72e80>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fa607a72e80>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa607c97518>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa607c97518>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fa607a53128>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fa607a53128>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa607836940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa607836940>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fa6077aa7b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fa6077aa7b8>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fa6075faa20>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fa6075faa20>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa607718630>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa607718630>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fa6073f6518>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fa6073f6518>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa6072deef0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fa6072deef0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:seq_len: 256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method NonMaskingLayer.call of <kashgari.layers.non_masking_layer.NonMaskingLayer object at 0x7fa5efc32048>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method NonMaskingLayer.call of <kashgari.layers.non_masking_layer.NonMaskingLayer object at 0x7fa5efc32048>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    }
   ],
   "source": [
    "bert_embed = BERTEmbedding('chinese_L-12_H-768_A-12',\n",
    "                           task=kashgari.LABELING,\n",
    "                           sequence_length=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "uuid": "895ce1c1-be19-434f-8b72-a118fa78e4a9"
   },
   "outputs": [],
   "source": [
    "model = BiLSTM_CRF_Model(bert_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "uuid": "5c9c3480-3117-4db2-87f7-7320cbc6054c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:CuDNN enabled, this will speed up the training, but will make model incompatible with CPU device.\n"
     ]
    }
   ],
   "source": [
    "kashgari.config.use_cudnn_cell = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "uuid": "05051039-c3d6-4bde-a4ab-ff34ef775271"
   },
   "outputs": [],
   "source": [
    "history = LossHistory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "uuid": "f4e5575c-ec35-41af-b83c-82c9c1913a1b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method CRF.call of <kashgari.layers.crf.CRF object at 0x7fa5efbf4b70>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method CRF.call of <kashgari.layers.crf.CRF object at 0x7fa5efbf4b70>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Input-Token (InputLayer)        [(None, 256)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Input-Segment (InputLayer)      [(None, 256)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Token (TokenEmbedding [(None, 256, 768), ( 16226304    Input-Token[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Segment (Embedding)   (None, 256, 768)     1536        Input-Segment[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Token-Segment (Add)   (None, 256, 768)     0           Embedding-Token[0][0]            \n",
      "                                                                 Embedding-Segment[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Position (PositionEmb (None, 256, 768)     196608      Embedding-Token-Segment[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Dropout (Dropout)     (None, 256, 768)     0           Embedding-Position[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Norm (LayerNormalizat (None, 256, 768)     1536        Embedding-Dropout[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-MultiHeadSelfAttentio (None, 256, 768)     2362368     Embedding-Norm[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-MultiHeadSelfAttentio (None, 256, 768)     0           Encoder-1-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-MultiHeadSelfAttentio (None, 256, 768)     0           Embedding-Norm[0][0]             \n",
      "                                                                 Encoder-1-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-MultiHeadSelfAttentio (None, 256, 768)     1536        Encoder-1-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-FeedForward (FeedForw (None, 256, 768)     4722432     Encoder-1-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-FeedForward-Dropout ( (None, 256, 768)     0           Encoder-1-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-FeedForward-Add (Add) (None, 256, 768)     0           Encoder-1-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-1-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-FeedForward-Norm (Lay (None, 256, 768)     1536        Encoder-1-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-MultiHeadSelfAttentio (None, 256, 768)     2362368     Encoder-1-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-MultiHeadSelfAttentio (None, 256, 768)     0           Encoder-2-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-MultiHeadSelfAttentio (None, 256, 768)     0           Encoder-1-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-2-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-MultiHeadSelfAttentio (None, 256, 768)     1536        Encoder-2-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-FeedForward (FeedForw (None, 256, 768)     4722432     Encoder-2-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-FeedForward-Dropout ( (None, 256, 768)     0           Encoder-2-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-FeedForward-Add (Add) (None, 256, 768)     0           Encoder-2-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-2-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-FeedForward-Norm (Lay (None, 256, 768)     1536        Encoder-2-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-MultiHeadSelfAttentio (None, 256, 768)     2362368     Encoder-2-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-MultiHeadSelfAttentio (None, 256, 768)     0           Encoder-3-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-MultiHeadSelfAttentio (None, 256, 768)     0           Encoder-2-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-3-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-MultiHeadSelfAttentio (None, 256, 768)     1536        Encoder-3-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-FeedForward (FeedForw (None, 256, 768)     4722432     Encoder-3-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-FeedForward-Dropout ( (None, 256, 768)     0           Encoder-3-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-FeedForward-Add (Add) (None, 256, 768)     0           Encoder-3-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-3-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-FeedForward-Norm (Lay (None, 256, 768)     1536        Encoder-3-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-MultiHeadSelfAttentio (None, 256, 768)     2362368     Encoder-3-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-MultiHeadSelfAttentio (None, 256, 768)     0           Encoder-4-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-MultiHeadSelfAttentio (None, 256, 768)     0           Encoder-3-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-4-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-MultiHeadSelfAttentio (None, 256, 768)     1536        Encoder-4-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-FeedForward (FeedForw (None, 256, 768)     4722432     Encoder-4-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-FeedForward-Dropout ( (None, 256, 768)     0           Encoder-4-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-FeedForward-Add (Add) (None, 256, 768)     0           Encoder-4-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-4-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-FeedForward-Norm (Lay (None, 256, 768)     1536        Encoder-4-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-MultiHeadSelfAttentio (None, 256, 768)     2362368     Encoder-4-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-MultiHeadSelfAttentio (None, 256, 768)     0           Encoder-5-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-MultiHeadSelfAttentio (None, 256, 768)     0           Encoder-4-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-5-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-MultiHeadSelfAttentio (None, 256, 768)     1536        Encoder-5-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-FeedForward (FeedForw (None, 256, 768)     4722432     Encoder-5-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-FeedForward-Dropout ( (None, 256, 768)     0           Encoder-5-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-FeedForward-Add (Add) (None, 256, 768)     0           Encoder-5-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-5-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-FeedForward-Norm (Lay (None, 256, 768)     1536        Encoder-5-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-MultiHeadSelfAttentio (None, 256, 768)     2362368     Encoder-5-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-MultiHeadSelfAttentio (None, 256, 768)     0           Encoder-6-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-MultiHeadSelfAttentio (None, 256, 768)     0           Encoder-5-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-6-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-MultiHeadSelfAttentio (None, 256, 768)     1536        Encoder-6-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-FeedForward (FeedForw (None, 256, 768)     4722432     Encoder-6-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-FeedForward-Dropout ( (None, 256, 768)     0           Encoder-6-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-FeedForward-Add (Add) (None, 256, 768)     0           Encoder-6-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-6-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-FeedForward-Norm (Lay (None, 256, 768)     1536        Encoder-6-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-MultiHeadSelfAttentio (None, 256, 768)     2362368     Encoder-6-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-MultiHeadSelfAttentio (None, 256, 768)     0           Encoder-7-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-MultiHeadSelfAttentio (None, 256, 768)     0           Encoder-6-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-7-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-MultiHeadSelfAttentio (None, 256, 768)     1536        Encoder-7-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-FeedForward (FeedForw (None, 256, 768)     4722432     Encoder-7-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-FeedForward-Dropout ( (None, 256, 768)     0           Encoder-7-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-FeedForward-Add (Add) (None, 256, 768)     0           Encoder-7-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-7-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-FeedForward-Norm (Lay (None, 256, 768)     1536        Encoder-7-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-MultiHeadSelfAttentio (None, 256, 768)     2362368     Encoder-7-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-MultiHeadSelfAttentio (None, 256, 768)     0           Encoder-8-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-MultiHeadSelfAttentio (None, 256, 768)     0           Encoder-7-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-8-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-MultiHeadSelfAttentio (None, 256, 768)     1536        Encoder-8-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-FeedForward (FeedForw (None, 256, 768)     4722432     Encoder-8-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-FeedForward-Dropout ( (None, 256, 768)     0           Encoder-8-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-FeedForward-Add (Add) (None, 256, 768)     0           Encoder-8-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-8-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-FeedForward-Norm (Lay (None, 256, 768)     1536        Encoder-8-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-MultiHeadSelfAttentio (None, 256, 768)     2362368     Encoder-8-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-MultiHeadSelfAttentio (None, 256, 768)     0           Encoder-9-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-MultiHeadSelfAttentio (None, 256, 768)     0           Encoder-8-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-9-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-MultiHeadSelfAttentio (None, 256, 768)     1536        Encoder-9-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-FeedForward (FeedForw (None, 256, 768)     4722432     Encoder-9-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-FeedForward-Dropout ( (None, 256, 768)     0           Encoder-9-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-FeedForward-Add (Add) (None, 256, 768)     0           Encoder-9-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-9-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-FeedForward-Norm (Lay (None, 256, 768)     1536        Encoder-9-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-MultiHeadSelfAttenti (None, 256, 768)     2362368     Encoder-9-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-MultiHeadSelfAttenti (None, 256, 768)     0           Encoder-10-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-MultiHeadSelfAttenti (None, 256, 768)     0           Encoder-9-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-10-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-MultiHeadSelfAttenti (None, 256, 768)     1536        Encoder-10-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-FeedForward (FeedFor (None, 256, 768)     4722432     Encoder-10-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-FeedForward-Dropout  (None, 256, 768)     0           Encoder-10-FeedForward[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-FeedForward-Add (Add (None, 256, 768)     0           Encoder-10-MultiHeadSelfAttention\n",
      "                                                                 Encoder-10-FeedForward-Dropout[0]\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-FeedForward-Norm (La (None, 256, 768)     1536        Encoder-10-FeedForward-Add[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-MultiHeadSelfAttenti (None, 256, 768)     2362368     Encoder-10-FeedForward-Norm[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-MultiHeadSelfAttenti (None, 256, 768)     0           Encoder-11-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-MultiHeadSelfAttenti (None, 256, 768)     0           Encoder-10-FeedForward-Norm[0][0]\n",
      "                                                                 Encoder-11-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-MultiHeadSelfAttenti (None, 256, 768)     1536        Encoder-11-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-FeedForward (FeedFor (None, 256, 768)     4722432     Encoder-11-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-FeedForward-Dropout  (None, 256, 768)     0           Encoder-11-FeedForward[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-FeedForward-Add (Add (None, 256, 768)     0           Encoder-11-MultiHeadSelfAttention\n",
      "                                                                 Encoder-11-FeedForward-Dropout[0]\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-FeedForward-Norm (La (None, 256, 768)     1536        Encoder-11-FeedForward-Add[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-MultiHeadSelfAttenti (None, 256, 768)     2362368     Encoder-11-FeedForward-Norm[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-MultiHeadSelfAttenti (None, 256, 768)     0           Encoder-12-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-MultiHeadSelfAttenti (None, 256, 768)     0           Encoder-11-FeedForward-Norm[0][0]\n",
      "                                                                 Encoder-12-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-MultiHeadSelfAttenti (None, 256, 768)     1536        Encoder-12-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-FeedForward (FeedFor (None, 256, 768)     4722432     Encoder-12-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-FeedForward-Dropout  (None, 256, 768)     0           Encoder-12-FeedForward[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-FeedForward-Add (Add (None, 256, 768)     0           Encoder-12-MultiHeadSelfAttention\n",
      "                                                                 Encoder-12-FeedForward-Dropout[0]\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-FeedForward-Norm (La (None, 256, 768)     1536        Encoder-12-FeedForward-Add[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-Output (Concatenate)    (None, 256, 3072)    0           Encoder-9-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-10-FeedForward-Norm[0][0]\n",
      "                                                                 Encoder-11-FeedForward-Norm[0][0]\n",
      "                                                                 Encoder-12-FeedForward-Norm[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "non_masking_layer (NonMaskingLa (None, 256, 3072)    0           Encoder-Output[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "layer_blstm (Bidirectional)     (None, 256, 256)     3278848     non_masking_layer[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_dense (Dense)             (None, 256, 64)      16448       layer_blstm[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "layer_crf_dense (Dense)         (None, 256, 24)      1560        layer_dense[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "layer_crf (CRF)                 (None, 256, 24)      576         layer_crf_dense[0][0]            \n",
      "==================================================================================================\n",
      "Total params: 104,777,880\n",
      "Trainable params: 3,297,432\n",
      "Non-trainable params: 101,480,448\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/1000\n",
      "74/74 [==============================] - 100s 1s/step - loss: 55.1062 - accuracy: 0.9482 - val_loss: 584.0924 - val_accuracy: 0.9826\n",
      "Epoch 2/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 10.6593 - accuracy: 0.9911 - val_loss: 579.4992 - val_accuracy: 0.9890\n",
      "Epoch 3/1000\n",
      "74/74 [==============================] - 93s 1s/step - loss: 7.3316 - accuracy: 0.9938 - val_loss: 576.3784 - val_accuracy: 0.9909\n",
      "Epoch 4/1000\n",
      "74/74 [==============================] - 94s 1s/step - loss: 5.8889 - accuracy: 0.9950 - val_loss: 574.0596 - val_accuracy: 0.9911\n",
      "Epoch 5/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 5.0146 - accuracy: 0.9955 - val_loss: 572.2564 - val_accuracy: 0.9911\n",
      "Epoch 6/1000\n",
      "74/74 [==============================] - 93s 1s/step - loss: 4.3464 - accuracy: 0.9962 - val_loss: 570.0507 - val_accuracy: 0.9917\n",
      "Epoch 7/1000\n",
      "74/74 [==============================] - 94s 1s/step - loss: 3.8574 - accuracy: 0.9966 - val_loss: 568.3616 - val_accuracy: 0.9922\n",
      "Epoch 8/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 3.4801 - accuracy: 0.9970 - val_loss: 566.2150 - val_accuracy: 0.9920\n",
      "Epoch 9/1000\n",
      "74/74 [==============================] - 94s 1s/step - loss: 3.2363 - accuracy: 0.9970 - val_loss: 565.0305 - val_accuracy: 0.9916\n",
      "Epoch 10/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 2.8777 - accuracy: 0.9974 - val_loss: 563.3606 - val_accuracy: 0.9919\n",
      "Epoch 11/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 2.5238 - accuracy: 0.9978 - val_loss: 561.9242 - val_accuracy: 0.9912\n",
      "Epoch 12/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 2.2782 - accuracy: 0.9979 - val_loss: 560.7005 - val_accuracy: 0.9915\n",
      "Epoch 13/1000\n",
      "74/74 [==============================] - 94s 1s/step - loss: 2.2008 - accuracy: 0.9979 - val_loss: 559.3007 - val_accuracy: 0.9920\n",
      "Epoch 14/1000\n",
      "74/74 [==============================] - 94s 1s/step - loss: 2.0042 - accuracy: 0.9980 - val_loss: 557.7404 - val_accuracy: 0.9911\n",
      "Epoch 15/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 1.8787 - accuracy: 0.9981 - val_loss: 556.7770 - val_accuracy: 0.9909\n",
      "Epoch 16/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 1.6772 - accuracy: 0.9984 - val_loss: 555.0271 - val_accuracy: 0.9909\n",
      "Epoch 17/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 1.5668 - accuracy: 0.9985 - val_loss: 554.0764 - val_accuracy: 0.9911\n",
      "Epoch 18/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 1.5861 - accuracy: 0.9984 - val_loss: 552.5472 - val_accuracy: 0.9916\n",
      "Epoch 19/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 1.4131 - accuracy: 0.9985 - val_loss: 551.8819 - val_accuracy: 0.9912\n",
      "Epoch 20/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 1.3849 - accuracy: 0.9985 - val_loss: 550.3282 - val_accuracy: 0.9915\n",
      "Epoch 21/1000\n",
      "74/74 [==============================] - 94s 1s/step - loss: 1.2699 - accuracy: 0.9986 - val_loss: 549.3885 - val_accuracy: 0.9910\n",
      "Epoch 22/1000\n",
      "74/74 [==============================] - 93s 1s/step - loss: 1.2743 - accuracy: 0.9985 - val_loss: 548.1077 - val_accuracy: 0.9911\n",
      "Epoch 23/1000\n",
      "74/74 [==============================] - 93s 1s/step - loss: 1.1675 - accuracy: 0.9987 - val_loss: 547.0505 - val_accuracy: 0.9913\n",
      "Epoch 24/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 1.0191 - accuracy: 0.9989 - val_loss: 546.1872 - val_accuracy: 0.9918\n",
      "Epoch 25/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 1.0270 - accuracy: 0.9987 - val_loss: 544.6943 - val_accuracy: 0.9913\n",
      "Epoch 26/1000\n",
      "74/74 [==============================] - 93s 1s/step - loss: 0.8869 - accuracy: 0.9990 - val_loss: 543.8094 - val_accuracy: 0.9916\n",
      "Epoch 27/1000\n",
      "74/74 [==============================] - 93s 1s/step - loss: 0.8713 - accuracy: 0.9989 - val_loss: 542.5665 - val_accuracy: 0.9913\n",
      "Epoch 28/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.9027 - accuracy: 0.9988 - val_loss: 541.1819 - val_accuracy: 0.9910\n",
      "Epoch 29/1000\n",
      "74/74 [==============================] - 93s 1s/step - loss: 0.8303 - accuracy: 0.9989 - val_loss: 540.3404 - val_accuracy: 0.9906\n",
      "Epoch 30/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.6968 - accuracy: 0.9991 - val_loss: 539.8745 - val_accuracy: 0.9877\n",
      "Epoch 31/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.6934 - accuracy: 0.9991 - val_loss: 538.4146 - val_accuracy: 0.9874\n",
      "Epoch 32/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.7415 - accuracy: 0.9990 - val_loss: 538.2645 - val_accuracy: 0.9867\n",
      "Epoch 33/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.7831 - accuracy: 0.9989 - val_loss: 536.6802 - val_accuracy: 0.9874\n",
      "Epoch 34/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.7012 - accuracy: 0.9990 - val_loss: 535.6680 - val_accuracy: 0.9876\n",
      "Epoch 35/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.6365 - accuracy: 0.9990 - val_loss: 534.9237 - val_accuracy: 0.9876\n",
      "Epoch 36/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.6228 - accuracy: 0.9991 - val_loss: 533.6713 - val_accuracy: 0.9864\n",
      "Epoch 37/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.6552 - accuracy: 0.9991 - val_loss: 532.9722 - val_accuracy: 0.9869\n",
      "Epoch 38/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.6307 - accuracy: 0.9991 - val_loss: 531.9266 - val_accuracy: 0.9869\n",
      "Epoch 39/1000\n",
      "74/74 [==============================] - 93s 1s/step - loss: 0.5142 - accuracy: 0.9992 - val_loss: 530.3238 - val_accuracy: 0.9875\n",
      "Epoch 40/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.4705 - accuracy: 0.9993 - val_loss: 530.2771 - val_accuracy: 0.9869\n",
      "Epoch 41/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.4828 - accuracy: 0.9992 - val_loss: 529.3410 - val_accuracy: 0.9872\n",
      "Epoch 42/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.4488 - accuracy: 0.9993 - val_loss: 528.4261 - val_accuracy: 0.9871\n",
      "Epoch 43/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.4391 - accuracy: 0.9993 - val_loss: 527.7099 - val_accuracy: 0.9863\n",
      "Epoch 44/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.5051 - accuracy: 0.9992 - val_loss: 527.4573 - val_accuracy: 0.9874\n",
      "Epoch 45/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.5407 - accuracy: 0.9991 - val_loss: 526.4019 - val_accuracy: 0.9864\n",
      "Epoch 46/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.4708 - accuracy: 0.9993 - val_loss: 525.0117 - val_accuracy: 0.9879\n",
      "Epoch 47/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.6134 - accuracy: 0.9990 - val_loss: 524.1418 - val_accuracy: 0.9869\n",
      "Epoch 48/1000\n",
      "74/74 [==============================] - 93s 1s/step - loss: 0.5158 - accuracy: 0.9990 - val_loss: 522.7106 - val_accuracy: 0.9867\n",
      "Epoch 49/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.4511 - accuracy: 0.9993 - val_loss: 522.1055 - val_accuracy: 0.9869\n",
      "Epoch 50/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.3503 - accuracy: 0.9994 - val_loss: 521.6812 - val_accuracy: 0.9873\n",
      "Epoch 51/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.3229 - accuracy: 0.9995 - val_loss: 521.2805 - val_accuracy: 0.9871\n",
      "Epoch 52/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.2579 - accuracy: 0.9996 - val_loss: 520.0687 - val_accuracy: 0.9869\n",
      "Epoch 53/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.2720 - accuracy: 0.9995 - val_loss: 518.6190 - val_accuracy: 0.9866\n",
      "Epoch 54/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.2384 - accuracy: 0.9996 - val_loss: 518.8429 - val_accuracy: 0.9872\n",
      "Epoch 55/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.2233 - accuracy: 0.9995 - val_loss: 518.1894 - val_accuracy: 0.9865\n",
      "Epoch 56/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.2537 - accuracy: 0.9995 - val_loss: 518.3475 - val_accuracy: 0.9867\n",
      "Epoch 57/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.2422 - accuracy: 0.9996 - val_loss: 517.0289 - val_accuracy: 0.9872\n",
      "Epoch 58/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.2893 - accuracy: 0.9994 - val_loss: 515.9970 - val_accuracy: 0.9870\n",
      "Epoch 59/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.3236 - accuracy: 0.9994 - val_loss: 517.0056 - val_accuracy: 0.9868\n",
      "Epoch 60/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.3360 - accuracy: 0.9993 - val_loss: 514.1995 - val_accuracy: 0.9872\n",
      "Epoch 61/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.3533 - accuracy: 0.9993 - val_loss: 514.5631 - val_accuracy: 0.9875\n",
      "Epoch 62/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.5443 - accuracy: 0.9990 - val_loss: 513.4742 - val_accuracy: 0.9863\n",
      "Epoch 63/1000\n",
      "74/74 [==============================] - 93s 1s/step - loss: 0.7798 - accuracy: 0.9985 - val_loss: 512.1107 - val_accuracy: 0.9864\n",
      "Epoch 64/1000\n",
      "74/74 [==============================] - 93s 1s/step - loss: 0.6281 - accuracy: 0.9988 - val_loss: 510.7161 - val_accuracy: 0.9867\n",
      "Epoch 65/1000\n",
      "74/74 [==============================] - 93s 1s/step - loss: 0.4742 - accuracy: 0.9991 - val_loss: 509.3366 - val_accuracy: 0.9850\n",
      "Epoch 66/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.3868 - accuracy: 0.9993 - val_loss: 508.6287 - val_accuracy: 0.9852\n",
      "Epoch 67/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.2857 - accuracy: 0.9994 - val_loss: 509.0630 - val_accuracy: 0.9850\n",
      "Epoch 68/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.2506 - accuracy: 0.9994 - val_loss: 506.1949 - val_accuracy: 0.9845\n",
      "Epoch 69/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.2452 - accuracy: 0.9996 - val_loss: 505.9522 - val_accuracy: 0.9824\n",
      "Epoch 70/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.2379 - accuracy: 0.9995 - val_loss: 505.3787 - val_accuracy: 0.9782\n",
      "Epoch 71/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.2242 - accuracy: 0.9996 - val_loss: 505.4128 - val_accuracy: 0.9735\n",
      "Epoch 72/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.2495 - accuracy: 0.9995 - val_loss: 503.6975 - val_accuracy: 0.9740\n",
      "Epoch 73/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.2629 - accuracy: 0.9994 - val_loss: 503.7973 - val_accuracy: 0.9736\n",
      "Epoch 74/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.2392 - accuracy: 0.9995 - val_loss: 503.0811 - val_accuracy: 0.9713\n",
      "Epoch 75/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.2438 - accuracy: 0.9995 - val_loss: 502.7280 - val_accuracy: 0.9711\n",
      "Epoch 76/1000\n",
      "74/74 [==============================] - 93s 1s/step - loss: 0.4456 - accuracy: 0.9990 - val_loss: 501.9513 - val_accuracy: 0.9711\n",
      "Epoch 77/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.2833 - accuracy: 0.9994 - val_loss: 500.6076 - val_accuracy: 0.9711\n",
      "Epoch 78/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.2840 - accuracy: 0.9994 - val_loss: 499.8102 - val_accuracy: 0.9713\n",
      "Epoch 79/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.2129 - accuracy: 0.9996 - val_loss: 498.9029 - val_accuracy: 0.9718\n",
      "Epoch 80/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.2104 - accuracy: 0.9996 - val_loss: 499.8971 - val_accuracy: 0.9706\n",
      "Epoch 81/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.2206 - accuracy: 0.9995 - val_loss: 497.8450 - val_accuracy: 0.9714\n",
      "Epoch 82/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.1561 - accuracy: 0.9997 - val_loss: 497.3749 - val_accuracy: 0.9716\n",
      "Epoch 83/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.1686 - accuracy: 0.9996 - val_loss: 496.3860 - val_accuracy: 0.9710\n",
      "Epoch 84/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.2224 - accuracy: 0.9995 - val_loss: 496.2865 - val_accuracy: 0.9708\n",
      "Epoch 85/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.2338 - accuracy: 0.9995 - val_loss: 494.1398 - val_accuracy: 0.9707\n",
      "Epoch 86/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.2367 - accuracy: 0.9995 - val_loss: 494.9583 - val_accuracy: 0.9700\n",
      "Epoch 87/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.2336 - accuracy: 0.9995 - val_loss: 493.0370 - val_accuracy: 0.9720\n",
      "Epoch 88/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.2062 - accuracy: 0.9995 - val_loss: 493.8680 - val_accuracy: 0.9708\n",
      "Epoch 89/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.2352 - accuracy: 0.9995 - val_loss: 493.0729 - val_accuracy: 0.2940\n",
      "Epoch 90/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.1928 - accuracy: 0.9995 - val_loss: 492.5307 - val_accuracy: 0.2928\n",
      "Epoch 91/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.1783 - accuracy: 0.9996 - val_loss: 490.8254 - val_accuracy: 0.2987\n",
      "Epoch 92/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.2616 - accuracy: 0.9994 - val_loss: 491.4965 - val_accuracy: 0.2870\n",
      "Epoch 93/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.3641 - accuracy: 0.9992 - val_loss: 490.8227 - val_accuracy: 0.2878\n",
      "Epoch 94/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.3280 - accuracy: 0.9992 - val_loss: 488.5091 - val_accuracy: 0.2927\n",
      "Epoch 95/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.3885 - accuracy: 0.9991 - val_loss: 487.4203 - val_accuracy: 0.2917\n",
      "Epoch 96/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.3760 - accuracy: 0.9991 - val_loss: 487.5084 - val_accuracy: 0.2873\n",
      "Epoch 97/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.3879 - accuracy: 0.9991 - val_loss: 485.5127 - val_accuracy: 0.2903\n",
      "Epoch 98/1000\n",
      "74/74 [==============================] - 93s 1s/step - loss: 0.3632 - accuracy: 0.9993 - val_loss: 484.1691 - val_accuracy: 0.2871\n",
      "Epoch 99/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.2507 - accuracy: 0.9994 - val_loss: 483.7292 - val_accuracy: 0.2870\n",
      "Epoch 100/1000\n",
      "74/74 [==============================] - 93s 1s/step - loss: 0.1810 - accuracy: 0.9996 - val_loss: 483.7632 - val_accuracy: 0.2847\n",
      "Epoch 101/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.2010 - accuracy: 0.9995 - val_loss: 481.8286 - val_accuracy: 0.2912\n",
      "Epoch 102/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.1967 - accuracy: 0.9995 - val_loss: 481.3116 - val_accuracy: 0.2902\n",
      "Epoch 103/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.1575 - accuracy: 0.9996 - val_loss: 481.5621 - val_accuracy: 0.2846\n",
      "Epoch 104/1000\n",
      "74/74 [==============================] - 93s 1s/step - loss: 0.1499 - accuracy: 0.9996 - val_loss: 479.4051 - val_accuracy: 0.2945\n",
      "Epoch 105/1000\n",
      "74/74 [==============================] - 93s 1s/step - loss: 0.1991 - accuracy: 0.9995 - val_loss: 480.3643 - val_accuracy: 0.2865\n",
      "Epoch 106/1000\n",
      "74/74 [==============================] - 93s 1s/step - loss: 0.1453 - accuracy: 0.9996 - val_loss: 478.6192 - val_accuracy: 0.2929\n",
      "Epoch 107/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.1318 - accuracy: 0.9996 - val_loss: 478.5250 - val_accuracy: 0.2893\n",
      "Epoch 108/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.1266 - accuracy: 0.9997 - val_loss: 477.5939 - val_accuracy: 0.2914\n",
      "Epoch 109/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.1308 - accuracy: 0.9997 - val_loss: 477.9918 - val_accuracy: 0.2886\n",
      "Epoch 110/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.1544 - accuracy: 0.9996 - val_loss: 477.2074 - val_accuracy: 0.2883\n",
      "Epoch 111/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.2425 - accuracy: 0.9994 - val_loss: 475.5073 - val_accuracy: 0.2925\n",
      "Epoch 112/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.1794 - accuracy: 0.9995 - val_loss: 477.0247 - val_accuracy: 0.2827\n",
      "Epoch 113/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.1583 - accuracy: 0.9996 - val_loss: 475.9360 - val_accuracy: 0.2863\n",
      "Epoch 114/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.1640 - accuracy: 0.9995 - val_loss: 473.7814 - val_accuracy: 0.2905\n",
      "Epoch 115/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.1750 - accuracy: 0.9995 - val_loss: 472.7256 - val_accuracy: 0.2929\n",
      "Epoch 116/1000\n",
      "74/74 [==============================] - 93s 1s/step - loss: 0.1424 - accuracy: 0.9996 - val_loss: 474.5530 - val_accuracy: 0.2827\n",
      "Epoch 117/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.1451 - accuracy: 0.9996 - val_loss: 474.0548 - val_accuracy: 0.2835\n",
      "Epoch 118/1000\n",
      "74/74 [==============================] - 93s 1s/step - loss: 0.1170 - accuracy: 0.9997 - val_loss: 472.4773 - val_accuracy: 0.2868\n",
      "Epoch 119/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.1311 - accuracy: 0.9997 - val_loss: 471.1075 - val_accuracy: 0.2910\n",
      "Epoch 120/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.1559 - accuracy: 0.9996 - val_loss: 472.8032 - val_accuracy: 0.2791\n",
      "Epoch 121/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.1826 - accuracy: 0.9994 - val_loss: 471.1961 - val_accuracy: 0.2839\n",
      "Epoch 122/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.2121 - accuracy: 0.9995 - val_loss: 469.9147 - val_accuracy: 0.2851\n",
      "Epoch 123/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.1914 - accuracy: 0.9995 - val_loss: 469.5631 - val_accuracy: 0.2822\n",
      "Epoch 124/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.2452 - accuracy: 0.9994 - val_loss: 467.8692 - val_accuracy: 0.2849\n",
      "Epoch 125/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.1953 - accuracy: 0.9994 - val_loss: 467.0255 - val_accuracy: 0.2847\n",
      "Epoch 126/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.1711 - accuracy: 0.9996 - val_loss: 466.2605 - val_accuracy: 0.2846\n",
      "Epoch 127/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.1540 - accuracy: 0.9996 - val_loss: 466.4340 - val_accuracy: 0.2803\n",
      "Epoch 128/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.1573 - accuracy: 0.9996 - val_loss: 465.6226 - val_accuracy: 0.2796\n",
      "Epoch 129/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.1761 - accuracy: 0.9996 - val_loss: 464.1528 - val_accuracy: 0.2834\n",
      "Epoch 130/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.1667 - accuracy: 0.9995 - val_loss: 463.4395 - val_accuracy: 0.2835\n",
      "Epoch 131/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.1363 - accuracy: 0.9996 - val_loss: 462.5805 - val_accuracy: 0.2844\n",
      "Epoch 132/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.1183 - accuracy: 0.9997 - val_loss: 460.9849 - val_accuracy: 0.2857\n",
      "Epoch 133/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.1372 - accuracy: 0.9996 - val_loss: 460.5380 - val_accuracy: 0.2859\n",
      "Epoch 134/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.1215 - accuracy: 0.9996 - val_loss: 459.8286 - val_accuracy: 0.2851\n",
      "Epoch 135/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.1268 - accuracy: 0.9996 - val_loss: 460.6670 - val_accuracy: 0.2801\n",
      "Epoch 136/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.1806 - accuracy: 0.9995 - val_loss: 460.4102 - val_accuracy: 0.2751\n",
      "Epoch 137/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.1876 - accuracy: 0.9994 - val_loss: 457.7612 - val_accuracy: 0.2817\n",
      "Epoch 138/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.1878 - accuracy: 0.9994 - val_loss: 455.3177 - val_accuracy: 0.2872\n",
      "Epoch 139/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.2293 - accuracy: 0.9993 - val_loss: 455.9443 - val_accuracy: 0.2820\n",
      "Epoch 140/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.2405 - accuracy: 0.9993 - val_loss: 453.4301 - val_accuracy: 0.2857\n",
      "Epoch 141/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.2701 - accuracy: 0.9992 - val_loss: 451.5548 - val_accuracy: 0.2869\n",
      "Epoch 142/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.2236 - accuracy: 0.9994 - val_loss: 451.5254 - val_accuracy: 0.2830\n",
      "Epoch 143/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.2121 - accuracy: 0.9994 - val_loss: 449.8862 - val_accuracy: 0.2838\n",
      "Epoch 144/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.2322 - accuracy: 0.9994 - val_loss: 448.5981 - val_accuracy: 0.2836\n",
      "Epoch 145/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.1737 - accuracy: 0.9995 - val_loss: 448.0618 - val_accuracy: 0.2817\n",
      "Epoch 146/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.1527 - accuracy: 0.9996 - val_loss: 445.5248 - val_accuracy: 0.2848\n",
      "Epoch 147/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.1587 - accuracy: 0.9996 - val_loss: 445.3575 - val_accuracy: 0.2824\n",
      "Epoch 148/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.1513 - accuracy: 0.9996 - val_loss: 445.8510 - val_accuracy: 0.2773\n",
      "Epoch 149/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.1273 - accuracy: 0.9996 - val_loss: 443.1183 - val_accuracy: 0.2831\n",
      "Epoch 150/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.1533 - accuracy: 0.9995 - val_loss: 441.8269 - val_accuracy: 0.2840\n",
      "Epoch 151/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.1400 - accuracy: 0.9996 - val_loss: 441.0824 - val_accuracy: 0.2823\n",
      "Epoch 152/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.1561 - accuracy: 0.9995 - val_loss: 440.5794 - val_accuracy: 0.2809\n",
      "Epoch 153/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.1450 - accuracy: 0.9996 - val_loss: 439.5815 - val_accuracy: 0.2818\n",
      "Epoch 154/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.1115 - accuracy: 0.9997 - val_loss: 440.2384 - val_accuracy: 0.2747\n",
      "Epoch 155/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.0913 - accuracy: 0.9997 - val_loss: 436.8630 - val_accuracy: 0.2837\n",
      "Epoch 156/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.0948 - accuracy: 0.9997 - val_loss: 436.1391 - val_accuracy: 0.2810\n",
      "Epoch 157/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.1168 - accuracy: 0.9997 - val_loss: 435.4843 - val_accuracy: 0.2808\n",
      "Epoch 158/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.1022 - accuracy: 0.9997 - val_loss: 435.1644 - val_accuracy: 0.2787\n",
      "Epoch 159/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.1099 - accuracy: 0.9997 - val_loss: 432.3999 - val_accuracy: 0.2834\n",
      "Epoch 160/1000\n",
      "74/74 [==============================] - 93s 1s/step - loss: 0.0835 - accuracy: 0.9997 - val_loss: 432.5902 - val_accuracy: 0.2809\n",
      "Epoch 161/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.1005 - accuracy: 0.9997 - val_loss: 430.9234 - val_accuracy: 0.2815\n",
      "Epoch 162/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.1233 - accuracy: 0.9996 - val_loss: 430.2993 - val_accuracy: 0.2814\n",
      "Epoch 163/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.1300 - accuracy: 0.9996 - val_loss: 429.8216 - val_accuracy: 0.2793\n",
      "Epoch 164/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.1269 - accuracy: 0.9996 - val_loss: 427.9506 - val_accuracy: 0.2814\n",
      "Epoch 165/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.1134 - accuracy: 0.9997 - val_loss: 426.7806 - val_accuracy: 0.2799\n",
      "Epoch 166/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.1293 - accuracy: 0.9996 - val_loss: 425.0778 - val_accuracy: 0.2807\n",
      "Epoch 167/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.1329 - accuracy: 0.9996 - val_loss: 424.9982 - val_accuracy: 0.2744\n",
      "Epoch 168/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.1330 - accuracy: 0.9996 - val_loss: 421.4527 - val_accuracy: 0.2855\n",
      "Epoch 169/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.1493 - accuracy: 0.9995 - val_loss: 420.6743 - val_accuracy: 0.2809\n",
      "Epoch 170/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.1583 - accuracy: 0.9994 - val_loss: 418.4461 - val_accuracy: 0.2817\n",
      "Epoch 171/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.1531 - accuracy: 0.9996 - val_loss: 416.6028 - val_accuracy: 0.2832\n",
      "Epoch 172/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.1894 - accuracy: 0.9994 - val_loss: 414.8403 - val_accuracy: 0.2805\n",
      "Epoch 173/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.1939 - accuracy: 0.9994 - val_loss: 412.3720 - val_accuracy: 0.2839\n",
      "Epoch 174/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.1891 - accuracy: 0.9994 - val_loss: 409.6728 - val_accuracy: 0.2835\n",
      "Epoch 175/1000\n",
      "74/74 [==============================] - 93s 1s/step - loss: 0.1570 - accuracy: 0.9995 - val_loss: 409.8039 - val_accuracy: 0.2760\n",
      "Epoch 176/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.1372 - accuracy: 0.9996 - val_loss: 407.3979 - val_accuracy: 0.2772\n",
      "Epoch 177/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.1227 - accuracy: 0.9996 - val_loss: 404.4383 - val_accuracy: 0.2816\n",
      "Epoch 178/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.1247 - accuracy: 0.9996 - val_loss: 402.1388 - val_accuracy: 0.2836\n",
      "Epoch 179/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.1482 - accuracy: 0.9996 - val_loss: 401.6359 - val_accuracy: 0.2796\n",
      "Epoch 180/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.1141 - accuracy: 0.9996 - val_loss: 399.1686 - val_accuracy: 0.2796\n",
      "Epoch 181/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.1242 - accuracy: 0.9996 - val_loss: 396.5408 - val_accuracy: 0.2839\n",
      "Epoch 182/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.1232 - accuracy: 0.9996 - val_loss: 395.3158 - val_accuracy: 0.2783\n",
      "Epoch 183/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.1207 - accuracy: 0.9996 - val_loss: 392.5840 - val_accuracy: 0.2797\n",
      "Epoch 184/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.1374 - accuracy: 0.9995 - val_loss: 390.1943 - val_accuracy: 0.2818\n",
      "Epoch 185/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.1235 - accuracy: 0.9996 - val_loss: 389.1124 - val_accuracy: 0.2788\n",
      "Epoch 186/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.1221 - accuracy: 0.9996 - val_loss: 387.1547 - val_accuracy: 0.2772\n",
      "Epoch 187/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.1412 - accuracy: 0.9995 - val_loss: 383.7115 - val_accuracy: 0.2818\n",
      "Epoch 188/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.1363 - accuracy: 0.9996 - val_loss: 382.8977 - val_accuracy: 0.2773\n",
      "Epoch 189/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.1381 - accuracy: 0.9995 - val_loss: 378.9002 - val_accuracy: 0.2841\n",
      "Epoch 190/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.1620 - accuracy: 0.9995 - val_loss: 377.3313 - val_accuracy: 0.2803\n",
      "Epoch 191/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.1281 - accuracy: 0.9996 - val_loss: 375.3897 - val_accuracy: 0.2785\n",
      "Epoch 192/1000\n",
      "74/74 [==============================] - 93s 1s/step - loss: 0.1127 - accuracy: 0.9996 - val_loss: 372.6409 - val_accuracy: 0.2775\n",
      "Epoch 193/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.0945 - accuracy: 0.9997 - val_loss: 370.3414 - val_accuracy: 0.2782\n",
      "Epoch 194/1000\n",
      "74/74 [==============================] - 93s 1s/step - loss: 0.1006 - accuracy: 0.9997 - val_loss: 368.7579 - val_accuracy: 0.2749\n",
      "Epoch 195/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.1097 - accuracy: 0.9997 - val_loss: 366.0937 - val_accuracy: 0.2741\n",
      "Epoch 196/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.1225 - accuracy: 0.9995 - val_loss: 362.1614 - val_accuracy: 0.2795\n",
      "Epoch 197/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.1097 - accuracy: 0.9996 - val_loss: 359.4068 - val_accuracy: 0.2804\n",
      "Epoch 198/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.1142 - accuracy: 0.9996 - val_loss: 357.0386 - val_accuracy: 0.2768\n",
      "Epoch 199/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.1156 - accuracy: 0.9996 - val_loss: 354.8745 - val_accuracy: 0.2760\n",
      "Epoch 200/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.0993 - accuracy: 0.9996 - val_loss: 352.1032 - val_accuracy: 0.2771\n",
      "Epoch 201/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.1079 - accuracy: 0.9996 - val_loss: 349.4409 - val_accuracy: 0.2764\n",
      "Epoch 202/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.1030 - accuracy: 0.9996 - val_loss: 346.9207 - val_accuracy: 0.2777\n",
      "Epoch 203/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.1125 - accuracy: 0.9996 - val_loss: 342.1758 - val_accuracy: 0.2852\n",
      "Epoch 204/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.1122 - accuracy: 0.9996 - val_loss: 340.3466 - val_accuracy: 0.2783\n",
      "Epoch 205/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0903 - accuracy: 0.9997 - val_loss: 337.0123 - val_accuracy: 0.2743\n",
      "Epoch 206/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.1094 - accuracy: 0.9996 - val_loss: 331.8363 - val_accuracy: 0.2781\n",
      "Epoch 207/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.1183 - accuracy: 0.9996 - val_loss: 328.1173 - val_accuracy: 0.2760\n",
      "Epoch 208/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.1152 - accuracy: 0.9996 - val_loss: 324.3415 - val_accuracy: 0.2738\n",
      "Epoch 209/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.1168 - accuracy: 0.9996 - val_loss: 319.3946 - val_accuracy: 0.2761\n",
      "Epoch 210/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.1470 - accuracy: 0.9995 - val_loss: 313.8668 - val_accuracy: 0.2801\n",
      "Epoch 211/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.1659 - accuracy: 0.9994 - val_loss: 308.4697 - val_accuracy: 0.2770\n",
      "Epoch 212/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.1388 - accuracy: 0.9996 - val_loss: 303.9282 - val_accuracy: 0.2749\n",
      "Epoch 213/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.1178 - accuracy: 0.9996 - val_loss: 298.2189 - val_accuracy: 0.2742\n",
      "Epoch 214/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.1192 - accuracy: 0.9996 - val_loss: 294.7248 - val_accuracy: 0.3019\n",
      "Epoch 215/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.1333 - accuracy: 0.9996 - val_loss: 290.5035 - val_accuracy: 0.9664\n",
      "Epoch 216/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.1143 - accuracy: 0.9996 - val_loss: 287.7418 - val_accuracy: 0.9695\n",
      "Epoch 217/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.1011 - accuracy: 0.9997 - val_loss: 284.1044 - val_accuracy: 0.9691\n",
      "Epoch 218/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.1050 - accuracy: 0.9996 - val_loss: 279.5099 - val_accuracy: 0.9692\n",
      "Epoch 219/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.0766 - accuracy: 0.9998 - val_loss: 275.2224 - val_accuracy: 0.9689\n",
      "Epoch 220/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.0813 - accuracy: 0.9997 - val_loss: 270.3958 - val_accuracy: 0.9690\n",
      "Epoch 221/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.0937 - accuracy: 0.9996 - val_loss: 266.7400 - val_accuracy: 0.9681\n",
      "Epoch 222/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.0877 - accuracy: 0.9997 - val_loss: 260.9922 - val_accuracy: 0.9692\n",
      "Epoch 223/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0948 - accuracy: 0.9997 - val_loss: 257.0483 - val_accuracy: 0.9694\n",
      "Epoch 224/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.1069 - accuracy: 0.9996 - val_loss: 253.1963 - val_accuracy: 0.9716\n",
      "Epoch 225/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.1015 - accuracy: 0.9997 - val_loss: 247.7231 - val_accuracy: 0.9713\n",
      "Epoch 226/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.1050 - accuracy: 0.9996 - val_loss: 242.1325 - val_accuracy: 0.9708\n",
      "Epoch 227/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.1178 - accuracy: 0.9996 - val_loss: 236.5764 - val_accuracy: 0.9719\n",
      "Epoch 228/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.0884 - accuracy: 0.9997 - val_loss: 230.3124 - val_accuracy: 0.9714\n",
      "Epoch 229/1000\n",
      "74/74 [==============================] - 93s 1s/step - loss: 0.0840 - accuracy: 0.9997 - val_loss: 224.7453 - val_accuracy: 0.9707\n",
      "Epoch 230/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0903 - accuracy: 0.9997 - val_loss: 218.7016 - val_accuracy: 0.9708\n",
      "Epoch 231/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.1283 - accuracy: 0.9995 - val_loss: 212.9240 - val_accuracy: 0.9712\n",
      "Epoch 232/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.1068 - accuracy: 0.9996 - val_loss: 206.5447 - val_accuracy: 0.9710\n",
      "Epoch 233/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.1044 - accuracy: 0.9997 - val_loss: 199.9977 - val_accuracy: 0.9715\n",
      "Epoch 234/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.1049 - accuracy: 0.9996 - val_loss: 194.1197 - val_accuracy: 0.9705\n",
      "Epoch 235/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.0831 - accuracy: 0.9997 - val_loss: 187.1003 - val_accuracy: 0.9718\n",
      "Epoch 236/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0731 - accuracy: 0.9997 - val_loss: 180.5340 - val_accuracy: 0.9726\n",
      "Epoch 237/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0771 - accuracy: 0.9998 - val_loss: 173.2637 - val_accuracy: 0.9727\n",
      "Epoch 238/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.0791 - accuracy: 0.9997 - val_loss: 166.2048 - val_accuracy: 0.9714\n",
      "Epoch 239/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.0877 - accuracy: 0.9997 - val_loss: 158.2124 - val_accuracy: 0.9724\n",
      "Epoch 240/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.1292 - accuracy: 0.9996 - val_loss: 150.3402 - val_accuracy: 0.9729\n",
      "Epoch 241/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.1124 - accuracy: 0.9996 - val_loss: 142.1597 - val_accuracy: 0.9723\n",
      "Epoch 242/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.1163 - accuracy: 0.9996 - val_loss: 133.9854 - val_accuracy: 0.9722\n",
      "Epoch 243/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.1034 - accuracy: 0.9996 - val_loss: 124.4062 - val_accuracy: 0.9732\n",
      "Epoch 244/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0902 - accuracy: 0.9997 - val_loss: 118.6655 - val_accuracy: 0.9715\n",
      "Epoch 245/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0904 - accuracy: 0.9996 - val_loss: 110.6894 - val_accuracy: 0.9717\n",
      "Epoch 246/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0899 - accuracy: 0.9997 - val_loss: 103.1813 - val_accuracy: 0.9722\n",
      "Epoch 247/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.1194 - accuracy: 0.9996 - val_loss: 95.2058 - val_accuracy: 0.9722\n",
      "Epoch 248/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.1271 - accuracy: 0.9995 - val_loss: 88.0431 - val_accuracy: 0.9715\n",
      "Epoch 249/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.1977 - accuracy: 0.9994 - val_loss: 83.2193 - val_accuracy: 0.9708\n",
      "Epoch 250/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.1562 - accuracy: 0.9995 - val_loss: 77.8890 - val_accuracy: 0.9710\n",
      "Epoch 251/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.1225 - accuracy: 0.9996 - val_loss: 72.3993 - val_accuracy: 0.9710\n",
      "Epoch 252/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.1076 - accuracy: 0.9996 - val_loss: 68.8758 - val_accuracy: 0.9704\n",
      "Epoch 253/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.0949 - accuracy: 0.9997 - val_loss: 65.7994 - val_accuracy: 0.9710\n",
      "Epoch 254/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.1076 - accuracy: 0.9996 - val_loss: 65.0838 - val_accuracy: 0.9706\n",
      "Epoch 255/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.0831 - accuracy: 0.9997 - val_loss: 63.3564 - val_accuracy: 0.9698\n",
      "Epoch 256/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0741 - accuracy: 0.9997 - val_loss: 61.8986 - val_accuracy: 0.9707\n",
      "Epoch 257/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0741 - accuracy: 0.9997 - val_loss: 61.2254 - val_accuracy: 0.9693\n",
      "Epoch 258/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0791 - accuracy: 0.9997 - val_loss: 60.6485 - val_accuracy: 0.9692\n",
      "Epoch 259/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0642 - accuracy: 0.9998 - val_loss: 59.5585 - val_accuracy: 0.9692\n",
      "Epoch 260/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.0810 - accuracy: 0.9997 - val_loss: 59.5151 - val_accuracy: 0.9675\n",
      "Epoch 261/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.0851 - accuracy: 0.9997 - val_loss: 59.0633 - val_accuracy: 0.9610\n",
      "Epoch 262/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.0824 - accuracy: 0.9997 - val_loss: 58.5527 - val_accuracy: 0.9550\n",
      "Epoch 263/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.0889 - accuracy: 0.9997 - val_loss: 58.0354 - val_accuracy: 0.9407\n",
      "Epoch 264/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.0976 - accuracy: 0.9996 - val_loss: 58.0812 - val_accuracy: 0.9108\n",
      "Epoch 265/1000\n",
      "74/74 [==============================] - 93s 1s/step - loss: 0.1128 - accuracy: 0.9996 - val_loss: 57.5144 - val_accuracy: 0.8548\n",
      "Epoch 266/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.1029 - accuracy: 0.9997 - val_loss: 57.9110 - val_accuracy: 0.8185\n",
      "Epoch 267/1000\n",
      "74/74 [==============================] - 93s 1s/step - loss: 0.1016 - accuracy: 0.9996 - val_loss: 55.9382 - val_accuracy: 0.7912\n",
      "Epoch 268/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.1026 - accuracy: 0.9996 - val_loss: 57.0200 - val_accuracy: 0.7539\n",
      "Epoch 269/1000\n",
      "74/74 [==============================] - 93s 1s/step - loss: 0.0884 - accuracy: 0.9997 - val_loss: 57.1953 - val_accuracy: 0.7266\n",
      "Epoch 270/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.0750 - accuracy: 0.9997 - val_loss: 56.8537 - val_accuracy: 0.7081\n",
      "Epoch 271/1000\n",
      "74/74 [==============================] - 93s 1s/step - loss: 0.0688 - accuracy: 0.9997 - val_loss: 56.6895 - val_accuracy: 0.7033\n",
      "Epoch 272/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.0845 - accuracy: 0.9997 - val_loss: 57.1888 - val_accuracy: 0.6995\n",
      "Epoch 273/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.0927 - accuracy: 0.9996 - val_loss: 56.0243 - val_accuracy: 0.7037\n",
      "Epoch 274/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.0909 - accuracy: 0.9997 - val_loss: 56.2470 - val_accuracy: 0.7008\n",
      "Epoch 275/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.0969 - accuracy: 0.9996 - val_loss: 55.4005 - val_accuracy: 0.7047\n",
      "Epoch 276/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.1011 - accuracy: 0.9996 - val_loss: 56.2695 - val_accuracy: 0.7044\n",
      "Epoch 277/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.0966 - accuracy: 0.9996 - val_loss: 56.6885 - val_accuracy: 0.6983\n",
      "Epoch 278/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.0994 - accuracy: 0.9996 - val_loss: 55.0113 - val_accuracy: 0.7066\n",
      "Epoch 279/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.0958 - accuracy: 0.9996 - val_loss: 55.4513 - val_accuracy: 0.7039\n",
      "Epoch 280/1000\n",
      "74/74 [==============================] - 93s 1s/step - loss: 0.0869 - accuracy: 0.9997 - val_loss: 55.7701 - val_accuracy: 0.7051\n",
      "Epoch 281/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.0888 - accuracy: 0.9997 - val_loss: 55.5384 - val_accuracy: 0.7037\n",
      "Epoch 282/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.0970 - accuracy: 0.9996 - val_loss: 56.1420 - val_accuracy: 0.7002\n",
      "Epoch 283/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.0891 - accuracy: 0.9997 - val_loss: 55.8133 - val_accuracy: 0.7034\n",
      "Epoch 284/1000\n",
      "74/74 [==============================] - 93s 1s/step - loss: 0.1053 - accuracy: 0.9996 - val_loss: 55.2355 - val_accuracy: 0.7039\n",
      "Epoch 285/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.1158 - accuracy: 0.9996 - val_loss: 55.2017 - val_accuracy: 0.7047\n",
      "Epoch 286/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.1080 - accuracy: 0.9996 - val_loss: 55.2962 - val_accuracy: 0.7020\n",
      "Epoch 287/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.0962 - accuracy: 0.9997 - val_loss: 54.6470 - val_accuracy: 0.7067\n",
      "Epoch 288/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.0795 - accuracy: 0.9997 - val_loss: 55.2334 - val_accuracy: 0.7013\n",
      "Epoch 289/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.0831 - accuracy: 0.9997 - val_loss: 54.8181 - val_accuracy: 0.7037\n",
      "Epoch 290/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.0993 - accuracy: 0.9996 - val_loss: 54.2944 - val_accuracy: 0.7054\n",
      "Epoch 291/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.0778 - accuracy: 0.9997 - val_loss: 54.9004 - val_accuracy: 0.7032\n",
      "Epoch 292/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.0852 - accuracy: 0.9997 - val_loss: 54.9787 - val_accuracy: 0.7028\n",
      "Epoch 293/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.0695 - accuracy: 0.9998 - val_loss: 56.1786 - val_accuracy: 0.6973\n",
      "Epoch 294/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.0515 - accuracy: 0.9998 - val_loss: 55.4978 - val_accuracy: 0.7021\n",
      "Epoch 295/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.0638 - accuracy: 0.9998 - val_loss: 54.2712 - val_accuracy: 0.7064\n",
      "Epoch 296/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.0624 - accuracy: 0.9998 - val_loss: 55.3760 - val_accuracy: 0.7015\n",
      "Epoch 297/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.0742 - accuracy: 0.9997 - val_loss: 55.4928 - val_accuracy: 0.7037\n",
      "Epoch 298/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.0923 - accuracy: 0.9996 - val_loss: 55.9203 - val_accuracy: 0.7011\n",
      "Epoch 299/1000\n",
      "74/74 [==============================] - 93s 1s/step - loss: 0.1112 - accuracy: 0.9996 - val_loss: 56.5102 - val_accuracy: 0.6987\n",
      "Epoch 300/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.0778 - accuracy: 0.9997 - val_loss: 54.7909 - val_accuracy: 0.7085\n",
      "Epoch 301/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.0749 - accuracy: 0.9997 - val_loss: 56.3822 - val_accuracy: 0.7026\n",
      "Epoch 302/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.0841 - accuracy: 0.9997 - val_loss: 57.0058 - val_accuracy: 0.7014\n",
      "Epoch 303/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.0634 - accuracy: 0.9997 - val_loss: 57.4323 - val_accuracy: 0.7025\n",
      "Epoch 304/1000\n",
      "74/74 [==============================] - 93s 1s/step - loss: 0.0665 - accuracy: 0.9997 - val_loss: 56.9839 - val_accuracy: 0.7102\n",
      "Epoch 305/1000\n",
      "74/74 [==============================] - 93s 1s/step - loss: 0.0824 - accuracy: 0.9996 - val_loss: 58.7703 - val_accuracy: 0.7037\n",
      "Epoch 306/1000\n",
      "74/74 [==============================] - 93s 1s/step - loss: 0.0690 - accuracy: 0.9998 - val_loss: 58.4745 - val_accuracy: 0.7100\n",
      "Epoch 307/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.0644 - accuracy: 0.9997 - val_loss: 59.9122 - val_accuracy: 0.7069\n",
      "Epoch 308/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.0779 - accuracy: 0.9997 - val_loss: 61.0808 - val_accuracy: 0.7046\n",
      "Epoch 309/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.0845 - accuracy: 0.9997 - val_loss: 60.9916 - val_accuracy: 0.7065\n",
      "Epoch 310/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.0685 - accuracy: 0.9997 - val_loss: 63.3394 - val_accuracy: 0.7039\n",
      "Epoch 311/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.0739 - accuracy: 0.9998 - val_loss: 63.9605 - val_accuracy: 0.7035\n",
      "Epoch 312/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.0741 - accuracy: 0.9997 - val_loss: 67.1385 - val_accuracy: 0.7029\n",
      "Epoch 313/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.1014 - accuracy: 0.9996 - val_loss: 67.2766 - val_accuracy: 0.7037\n",
      "Epoch 314/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.0939 - accuracy: 0.9996 - val_loss: 66.5577 - val_accuracy: 0.7063\n",
      "Epoch 315/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.0901 - accuracy: 0.9997 - val_loss: 66.6336 - val_accuracy: 0.7074\n",
      "Epoch 316/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.0912 - accuracy: 0.9997 - val_loss: 68.0624 - val_accuracy: 0.6992\n",
      "Epoch 317/1000\n",
      "74/74 [==============================] - 93s 1s/step - loss: 0.0874 - accuracy: 0.9997 - val_loss: 66.4206 - val_accuracy: 0.7043\n",
      "Epoch 318/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.0977 - accuracy: 0.9997 - val_loss: 67.5357 - val_accuracy: 0.7023\n",
      "Epoch 319/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.1264 - accuracy: 0.9995 - val_loss: 67.1189 - val_accuracy: 0.7022\n",
      "Epoch 320/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.1077 - accuracy: 0.9996 - val_loss: 67.2484 - val_accuracy: 0.7034\n",
      "Epoch 321/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.0947 - accuracy: 0.9996 - val_loss: 66.6523 - val_accuracy: 0.7037\n",
      "Epoch 322/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.0958 - accuracy: 0.9997 - val_loss: 67.4213 - val_accuracy: 0.7031\n",
      "Epoch 323/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.0765 - accuracy: 0.9998 - val_loss: 67.2178 - val_accuracy: 0.7030\n",
      "Epoch 324/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.0782 - accuracy: 0.9997 - val_loss: 67.9735 - val_accuracy: 0.6994\n",
      "Epoch 325/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.0754 - accuracy: 0.9997 - val_loss: 66.1471 - val_accuracy: 0.7093\n",
      "Epoch 326/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.0738 - accuracy: 0.9997 - val_loss: 66.4291 - val_accuracy: 0.7040\n",
      "Epoch 327/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.0655 - accuracy: 0.9997 - val_loss: 65.7638 - val_accuracy: 0.7138\n",
      "Epoch 328/1000\n",
      "74/74 [==============================] - 93s 1s/step - loss: 0.0583 - accuracy: 0.9998 - val_loss: 67.4803 - val_accuracy: 0.7030\n",
      "Epoch 329/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.0515 - accuracy: 0.9998 - val_loss: 67.1440 - val_accuracy: 0.7037\n",
      "Epoch 330/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.0570 - accuracy: 0.9997 - val_loss: 68.1502 - val_accuracy: 0.7011\n",
      "Epoch 331/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.0646 - accuracy: 0.9998 - val_loss: 66.8159 - val_accuracy: 0.7050\n",
      "Epoch 332/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.0935 - accuracy: 0.9997 - val_loss: 67.5512 - val_accuracy: 0.6998\n",
      "Epoch 333/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.0750 - accuracy: 0.9997 - val_loss: 65.0687 - val_accuracy: 0.7119\n",
      "Epoch 334/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.0892 - accuracy: 0.9997 - val_loss: 66.1807 - val_accuracy: 0.7073\n",
      "Epoch 335/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.0752 - accuracy: 0.9997 - val_loss: 67.9641 - val_accuracy: 0.7003\n",
      "Epoch 336/1000\n",
      "74/74 [==============================] - 93s 1s/step - loss: 0.0998 - accuracy: 0.9996 - val_loss: 67.1169 - val_accuracy: 0.7026\n",
      "Epoch 337/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.0918 - accuracy: 0.9997 - val_loss: 66.5619 - val_accuracy: 0.7037\n",
      "Epoch 338/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.0776 - accuracy: 0.9997 - val_loss: 67.8051 - val_accuracy: 0.6987\n",
      "Epoch 339/1000\n",
      "74/74 [==============================] - 93s 1s/step - loss: 0.0906 - accuracy: 0.9997 - val_loss: 66.7601 - val_accuracy: 0.7040\n",
      "Epoch 340/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.0897 - accuracy: 0.9996 - val_loss: 66.0357 - val_accuracy: 0.7078\n",
      "Epoch 341/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.0891 - accuracy: 0.9997 - val_loss: 67.9546 - val_accuracy: 0.7009\n",
      "Epoch 342/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.1073 - accuracy: 0.9996 - val_loss: 66.6507 - val_accuracy: 0.7064\n",
      "Epoch 343/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.0881 - accuracy: 0.9997 - val_loss: 67.9439 - val_accuracy: 0.7011\n",
      "Epoch 344/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.1131 - accuracy: 0.9996 - val_loss: 68.7048 - val_accuracy: 0.6946\n",
      "Epoch 345/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.0858 - accuracy: 0.9997 - val_loss: 67.1233 - val_accuracy: 0.7037\n",
      "Epoch 346/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.0712 - accuracy: 0.9997 - val_loss: 67.6512 - val_accuracy: 0.7014\n",
      "Epoch 347/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.0593 - accuracy: 0.9998 - val_loss: 68.6362 - val_accuracy: 0.6970\n",
      "Epoch 348/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.0791 - accuracy: 0.9997 - val_loss: 66.0160 - val_accuracy: 0.7090\n",
      "Epoch 349/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.1607 - accuracy: 0.9995 - val_loss: 69.1479 - val_accuracy: 0.6944\n",
      "Epoch 350/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.0852 - accuracy: 0.9997 - val_loss: 67.1723 - val_accuracy: 0.7035\n",
      "Epoch 351/1000\n",
      "74/74 [==============================] - 93s 1s/step - loss: 0.0810 - accuracy: 0.9997 - val_loss: 66.6136 - val_accuracy: 0.7057\n",
      "Epoch 352/1000\n",
      "74/74 [==============================] - 93s 1s/step - loss: 0.0934 - accuracy: 0.9996 - val_loss: 67.2722 - val_accuracy: 0.7063\n",
      "Epoch 353/1000\n",
      "74/74 [==============================] - 93s 1s/step - loss: 0.0729 - accuracy: 0.9997 - val_loss: 68.6574 - val_accuracy: 0.7037\n",
      "Epoch 354/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.0719 - accuracy: 0.9997 - val_loss: 69.3260 - val_accuracy: 0.6986\n",
      "Epoch 355/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.0613 - accuracy: 0.9997 - val_loss: 68.9682 - val_accuracy: 0.7013\n",
      "Epoch 356/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.0586 - accuracy: 0.9998 - val_loss: 68.2807 - val_accuracy: 0.7022\n",
      "Epoch 357/1000\n",
      "74/74 [==============================] - 93s 1s/step - loss: 0.0712 - accuracy: 0.9997 - val_loss: 68.0049 - val_accuracy: 0.7023\n",
      "Epoch 358/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.0585 - accuracy: 0.9998 - val_loss: 68.2873 - val_accuracy: 0.7029\n",
      "Epoch 359/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.0612 - accuracy: 0.9997 - val_loss: 67.5227 - val_accuracy: 0.7064\n",
      "Epoch 360/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.0556 - accuracy: 0.9997 - val_loss: 67.9120 - val_accuracy: 0.7032\n",
      "Epoch 361/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.0592 - accuracy: 0.9998 - val_loss: 67.9869 - val_accuracy: 0.7037\n",
      "Epoch 362/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.0590 - accuracy: 0.9998 - val_loss: 68.7227 - val_accuracy: 0.7012\n",
      "Epoch 363/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.0663 - accuracy: 0.9997 - val_loss: 68.0190 - val_accuracy: 0.7023\n",
      "Epoch 364/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.0624 - accuracy: 0.9997 - val_loss: 68.6032 - val_accuracy: 0.7024\n",
      "Epoch 365/1000\n",
      "74/74 [==============================] - 93s 1s/step - loss: 0.0576 - accuracy: 0.9998 - val_loss: 69.6869 - val_accuracy: 0.6971\n",
      "Epoch 366/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.0519 - accuracy: 0.9998 - val_loss: 66.4875 - val_accuracy: 0.7095\n",
      "Epoch 367/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.0699 - accuracy: 0.9997 - val_loss: 68.1351 - val_accuracy: 0.7035\n",
      "Epoch 368/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.0634 - accuracy: 0.9997 - val_loss: 68.6647 - val_accuracy: 0.7028\n",
      "Epoch 369/1000\n",
      "74/74 [==============================] - 93s 1s/step - loss: 0.0661 - accuracy: 0.9997 - val_loss: 68.2061 - val_accuracy: 0.7037\n",
      "Epoch 370/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.0717 - accuracy: 0.9997 - val_loss: 66.5339 - val_accuracy: 0.7115\n",
      "Epoch 371/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.0715 - accuracy: 0.9997 - val_loss: 68.9067 - val_accuracy: 0.7001\n",
      "Epoch 372/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.0681 - accuracy: 0.9997 - val_loss: 67.1297 - val_accuracy: 0.7086\n",
      "Epoch 373/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.0498 - accuracy: 0.9998 - val_loss: 66.9749 - val_accuracy: 0.7096\n",
      "Epoch 374/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.0536 - accuracy: 0.9998 - val_loss: 67.2191 - val_accuracy: 0.7082\n",
      "Epoch 375/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.0615 - accuracy: 0.9998 - val_loss: 67.2658 - val_accuracy: 0.7064\n",
      "Epoch 376/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.0703 - accuracy: 0.9997 - val_loss: 68.0520 - val_accuracy: 0.7047\n",
      "Epoch 377/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.0657 - accuracy: 0.9997 - val_loss: 68.4625 - val_accuracy: 0.7037\n",
      "Epoch 378/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.0712 - accuracy: 0.9997 - val_loss: 68.1087 - val_accuracy: 0.7044\n",
      "Epoch 379/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.0668 - accuracy: 0.9997 - val_loss: 68.4052 - val_accuracy: 0.7041\n",
      "Epoch 380/1000\n",
      "74/74 [==============================] - 93s 1s/step - loss: 0.0701 - accuracy: 0.9997 - val_loss: 68.4869 - val_accuracy: 0.7038\n",
      "Epoch 381/1000\n",
      "74/74 [==============================] - 93s 1s/step - loss: 0.0805 - accuracy: 0.9997 - val_loss: 69.1545 - val_accuracy: 0.6996\n",
      "Epoch 382/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.0641 - accuracy: 0.9998 - val_loss: 68.3969 - val_accuracy: 0.7047\n",
      "Epoch 383/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.0625 - accuracy: 0.9997 - val_loss: 69.5606 - val_accuracy: 0.6998\n",
      "Epoch 384/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.0661 - accuracy: 0.9998 - val_loss: 69.5655 - val_accuracy: 0.7037\n",
      "Epoch 385/1000\n",
      "74/74 [==============================] - 93s 1s/step - loss: 0.0708 - accuracy: 0.9998 - val_loss: 69.8965 - val_accuracy: 0.7037\n",
      "Epoch 386/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.0789 - accuracy: 0.9997 - val_loss: 69.9996 - val_accuracy: 0.7030\n",
      "Epoch 387/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.0836 - accuracy: 0.9996 - val_loss: 69.8059 - val_accuracy: 0.7022\n",
      "Epoch 388/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.0758 - accuracy: 0.9997 - val_loss: 70.5244 - val_accuracy: 0.7011\n",
      "Epoch 389/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.0902 - accuracy: 0.9997 - val_loss: 72.7352 - val_accuracy: 0.7027\n",
      "Epoch 390/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.0872 - accuracy: 0.9996 - val_loss: 72.6289 - val_accuracy: 0.7040\n",
      "Epoch 391/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.0888 - accuracy: 0.9997 - val_loss: 72.3394 - val_accuracy: 0.7045\n",
      "Epoch 392/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.1120 - accuracy: 0.9996 - val_loss: 73.6176 - val_accuracy: 0.6994\n",
      "Epoch 393/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.1044 - accuracy: 0.9996 - val_loss: 72.4071 - val_accuracy: 0.7037\n",
      "Epoch 394/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.0825 - accuracy: 0.9997 - val_loss: 72.1098 - val_accuracy: 0.7040\n",
      "Epoch 395/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.0733 - accuracy: 0.9997 - val_loss: 73.7868 - val_accuracy: 0.6981\n",
      "Epoch 396/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.0789 - accuracy: 0.9997 - val_loss: 74.6044 - val_accuracy: 0.6953\n",
      "Epoch 397/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.0852 - accuracy: 0.9997 - val_loss: 71.9390 - val_accuracy: 0.7059\n",
      "Epoch 398/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.1070 - accuracy: 0.9996 - val_loss: 72.6184 - val_accuracy: 0.7031\n",
      "Epoch 399/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.0726 - accuracy: 0.9997 - val_loss: 71.2672 - val_accuracy: 0.7097\n",
      "Epoch 400/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.0723 - accuracy: 0.9997 - val_loss: 70.3314 - val_accuracy: 0.7135\n",
      "Epoch 401/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.0625 - accuracy: 0.9998 - val_loss: 72.6551 - val_accuracy: 0.7077\n",
      "Epoch 403/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.0612 - accuracy: 0.9998 - val_loss: 74.2441 - val_accuracy: 0.7012\n",
      "Epoch 404/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.0733 - accuracy: 0.9997 - val_loss: 72.7764 - val_accuracy: 0.7058\n",
      "Epoch 405/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.0645 - accuracy: 0.9997 - val_loss: 73.8804 - val_accuracy: 0.7029\n",
      "Epoch 406/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.0554 - accuracy: 0.9998 - val_loss: 74.6654 - val_accuracy: 0.7010\n",
      "Epoch 407/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.0578 - accuracy: 0.9997 - val_loss: 72.7315 - val_accuracy: 0.7081\n",
      "Epoch 408/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.0612 - accuracy: 0.9998 - val_loss: 74.5371 - val_accuracy: 0.6997\n",
      "Epoch 409/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.0644 - accuracy: 0.9997 - val_loss: 73.7694 - val_accuracy: 0.7037\n",
      "Epoch 410/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.0579 - accuracy: 0.9998 - val_loss: 74.1003 - val_accuracy: 0.6999\n",
      "Epoch 411/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.0700 - accuracy: 0.9998 - val_loss: 73.5589 - val_accuracy: 0.7066\n",
      "Epoch 412/1000\n",
      "74/74 [==============================] - 93s 1s/step - loss: 0.0603 - accuracy: 0.9998 - val_loss: 74.3820 - val_accuracy: 0.7016\n",
      "Epoch 413/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.0519 - accuracy: 0.9998 - val_loss: 73.4081 - val_accuracy: 0.7058\n",
      "Epoch 414/1000\n",
      "74/74 [==============================] - 93s 1s/step - loss: 0.0555 - accuracy: 0.9998 - val_loss: 73.2462 - val_accuracy: 0.7066\n",
      "Epoch 415/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.0682 - accuracy: 0.9997 - val_loss: 74.3578 - val_accuracy: 0.7017\n",
      "Epoch 416/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.0689 - accuracy: 0.9998 - val_loss: 72.5063 - val_accuracy: 0.7110\n",
      "Epoch 417/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.0622 - accuracy: 0.9998 - val_loss: 74.3406 - val_accuracy: 0.7037\n",
      "Epoch 418/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.0537 - accuracy: 0.9998 - val_loss: 74.4681 - val_accuracy: 0.7044\n",
      "Epoch 419/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.0471 - accuracy: 0.9998 - val_loss: 74.7355 - val_accuracy: 0.7026\n",
      "Epoch 420/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.0399 - accuracy: 0.9998 - val_loss: 74.3278 - val_accuracy: 0.7035\n",
      "Epoch 421/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.0461 - accuracy: 0.9998 - val_loss: 75.4069 - val_accuracy: 0.7026\n",
      "Epoch 422/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.0433 - accuracy: 0.9998 - val_loss: 74.6079 - val_accuracy: 0.7053\n",
      "Epoch 423/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.0504 - accuracy: 0.9998 - val_loss: 75.1178 - val_accuracy: 0.7029\n",
      "Epoch 424/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.0715 - accuracy: 0.9997 - val_loss: 77.7151 - val_accuracy: 0.6955\n",
      "Epoch 425/1000\n",
      "74/74 [==============================] - 93s 1s/step - loss: 0.0594 - accuracy: 0.9998 - val_loss: 75.5844 - val_accuracy: 0.7037\n",
      "Epoch 426/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.0551 - accuracy: 0.9998 - val_loss: 75.5592 - val_accuracy: 0.7068\n",
      "Epoch 427/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.0791 - accuracy: 0.9997 - val_loss: 76.8117 - val_accuracy: 0.7019\n",
      "Epoch 428/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.0679 - accuracy: 0.9997 - val_loss: 75.8551 - val_accuracy: 0.7041\n",
      "Epoch 429/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0691 - accuracy: 0.9997 - val_loss: 75.3686 - val_accuracy: 0.7094\n",
      "Epoch 430/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.0592 - accuracy: 0.9998 - val_loss: 75.3264 - val_accuracy: 0.7051\n",
      "Epoch 431/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.0663 - accuracy: 0.9997 - val_loss: 97.4778 - val_accuracy: 0.7015\n",
      "Epoch 550/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0677 - accuracy: 0.9997 - val_loss: 96.7559 - val_accuracy: 0.7039\n",
      "Epoch 551/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0656 - accuracy: 0.9997 - val_loss: 95.5534 - val_accuracy: 0.7080\n",
      "Epoch 552/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0801 - accuracy: 0.9996 - val_loss: 95.4196 - val_accuracy: 0.7102\n",
      "Epoch 553/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0767 - accuracy: 0.9997 - val_loss: 97.0150 - val_accuracy: 0.7037\n",
      "Epoch 554/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.0719 - accuracy: 0.9997 - val_loss: 96.2344 - val_accuracy: 0.7054\n",
      "Epoch 555/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0622 - accuracy: 0.9997 - val_loss: 97.0046 - val_accuracy: 0.7047\n",
      "Epoch 556/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.0496 - accuracy: 0.9998 - val_loss: 96.5129 - val_accuracy: 0.7051\n",
      "Epoch 557/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0424 - accuracy: 0.9998 - val_loss: 94.9094 - val_accuracy: 0.7116\n",
      "Epoch 558/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.0394 - accuracy: 0.9998 - val_loss: 95.9954 - val_accuracy: 0.7082\n",
      "Epoch 559/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0577 - accuracy: 0.9997 - val_loss: 98.6664 - val_accuracy: 0.7008\n",
      "Epoch 560/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0376 - accuracy: 0.9998 - val_loss: 98.8470 - val_accuracy: 0.7012\n",
      "Epoch 561/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0607 - accuracy: 0.9998 - val_loss: 98.2250 - val_accuracy: 0.7037\n",
      "Epoch 562/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.0564 - accuracy: 0.9998 - val_loss: 97.1269 - val_accuracy: 0.7050\n",
      "Epoch 563/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.0505 - accuracy: 0.9998 - val_loss: 97.3778 - val_accuracy: 0.7055\n",
      "Epoch 564/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0439 - accuracy: 0.9998 - val_loss: 98.0084 - val_accuracy: 0.7040\n",
      "Epoch 565/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.0345 - accuracy: 0.9999 - val_loss: 99.5271 - val_accuracy: 0.6993\n",
      "Epoch 566/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.0477 - accuracy: 0.9998 - val_loss: 98.2043 - val_accuracy: 0.7035\n",
      "Epoch 567/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0483 - accuracy: 0.9998 - val_loss: 98.2134 - val_accuracy: 0.7050\n",
      "Epoch 568/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.0488 - accuracy: 0.9998 - val_loss: 98.9638 - val_accuracy: 0.7015\n",
      "Epoch 569/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.0531 - accuracy: 0.9998 - val_loss: 98.3005 - val_accuracy: 0.7037\n",
      "Epoch 570/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.0571 - accuracy: 0.9997 - val_loss: 99.8803 - val_accuracy: 0.7030\n",
      "Epoch 571/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.0584 - accuracy: 0.9998 - val_loss: 99.8162 - val_accuracy: 0.6993\n",
      "Epoch 572/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.0495 - accuracy: 0.9998 - val_loss: 97.8740 - val_accuracy: 0.7062\n",
      "Epoch 573/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.0553 - accuracy: 0.9998 - val_loss: 99.6974 - val_accuracy: 0.7031\n",
      "Epoch 574/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0655 - accuracy: 0.9997 - val_loss: 98.1271 - val_accuracy: 0.7059\n",
      "Epoch 575/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0673 - accuracy: 0.9998 - val_loss: 99.3370 - val_accuracy: 0.7024\n",
      "Epoch 576/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0503 - accuracy: 0.9998 - val_loss: 96.6361 - val_accuracy: 0.7122\n",
      "Epoch 577/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0531 - accuracy: 0.9998 - val_loss: 99.9207 - val_accuracy: 0.7037\n",
      "Epoch 578/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0579 - accuracy: 0.9997 - val_loss: 98.5495 - val_accuracy: 0.7081\n",
      "Epoch 579/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0577 - accuracy: 0.9997 - val_loss: 102.8862 - val_accuracy: 0.7004\n",
      "Epoch 580/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0544 - accuracy: 0.9998 - val_loss: 102.9849 - val_accuracy: 0.7015\n",
      "Epoch 581/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0665 - accuracy: 0.9998 - val_loss: 102.3990 - val_accuracy: 0.7026\n",
      "Epoch 582/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.0532 - accuracy: 0.9998 - val_loss: 101.4550 - val_accuracy: 0.7049\n",
      "Epoch 583/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0592 - accuracy: 0.9998 - val_loss: 102.2965 - val_accuracy: 0.7029\n",
      "Epoch 584/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0737 - accuracy: 0.9997 - val_loss: 102.3866 - val_accuracy: 0.7037\n",
      "Epoch 586/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0539 - accuracy: 0.9998 - val_loss: 102.5156 - val_accuracy: 0.7022\n",
      "Epoch 587/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0802 - accuracy: 0.9998 - val_loss: 103.7481 - val_accuracy: 0.7014\n",
      "Epoch 588/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0611 - accuracy: 0.9997 - val_loss: 102.4161 - val_accuracy: 0.7035\n",
      "Epoch 589/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.0768 - accuracy: 0.9997 - val_loss: 104.9148 - val_accuracy: 0.6982\n",
      "Epoch 590/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0724 - accuracy: 0.9998 - val_loss: 102.5250 - val_accuracy: 0.7060\n",
      "Epoch 591/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.1032 - accuracy: 0.9996 - val_loss: 104.9790 - val_accuracy: 0.7001\n",
      "Epoch 592/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0615 - accuracy: 0.9997 - val_loss: 104.0993 - val_accuracy: 0.7034\n",
      "Epoch 593/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0588 - accuracy: 0.9998 - val_loss: 104.2520 - val_accuracy: 0.7037\n",
      "Epoch 594/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0546 - accuracy: 0.9998 - val_loss: 105.6532 - val_accuracy: 0.7012\n",
      "Epoch 595/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0442 - accuracy: 0.9998 - val_loss: 105.4797 - val_accuracy: 0.7017\n",
      "Epoch 596/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.0627 - accuracy: 0.9997 - val_loss: 105.7674 - val_accuracy: 0.7013\n",
      "Epoch 597/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0544 - accuracy: 0.9998 - val_loss: 104.8874 - val_accuracy: 0.7033\n",
      "Epoch 598/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0487 - accuracy: 0.9998 - val_loss: 104.0185 - val_accuracy: 0.7068\n",
      "Epoch 599/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0434 - accuracy: 0.9998 - val_loss: 106.2277 - val_accuracy: 0.7028\n",
      "Epoch 600/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0371 - accuracy: 0.9998 - val_loss: 107.0301 - val_accuracy: 0.7018\n",
      "Epoch 601/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0484 - accuracy: 0.9998 - val_loss: 106.5858 - val_accuracy: 0.7037\n",
      "Epoch 602/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.0425 - accuracy: 0.9998 - val_loss: 106.7498 - val_accuracy: 0.7024\n",
      "Epoch 603/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0332 - accuracy: 0.9998 - val_loss: 107.3787 - val_accuracy: 0.7044\n",
      "Epoch 604/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0382 - accuracy: 0.9998 - val_loss: 107.6020 - val_accuracy: 0.7028\n",
      "Epoch 605/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.0429 - accuracy: 0.9998 - val_loss: 109.1862 - val_accuracy: 0.6986\n",
      "Epoch 606/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0410 - accuracy: 0.9998 - val_loss: 106.6659 - val_accuracy: 0.7069\n",
      "Epoch 607/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0587 - accuracy: 0.9997 - val_loss: 109.1499 - val_accuracy: 0.7038\n",
      "Epoch 608/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0540 - accuracy: 0.9998 - val_loss: 107.9569 - val_accuracy: 0.7044\n",
      "Epoch 609/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.0397 - accuracy: 0.9998 - val_loss: 109.2700 - val_accuracy: 0.7037\n",
      "Epoch 610/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0534 - accuracy: 0.9997 - val_loss: 107.9636 - val_accuracy: 0.7072\n",
      "Epoch 611/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0421 - accuracy: 0.9998 - val_loss: 109.5840 - val_accuracy: 0.7036\n",
      "Epoch 612/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0554 - accuracy: 0.9998 - val_loss: 110.3045 - val_accuracy: 0.7033\n",
      "Epoch 613/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0456 - accuracy: 0.9998 - val_loss: 109.8984 - val_accuracy: 0.7062\n",
      "Epoch 614/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0508 - accuracy: 0.9998 - val_loss: 122.0847 - val_accuracy: 0.7037\n",
      "Epoch 674/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0577 - accuracy: 0.9997 - val_loss: 120.8406 - val_accuracy: 0.7065\n",
      "Epoch 675/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.0496 - accuracy: 0.9998 - val_loss: 121.7110 - val_accuracy: 0.7052\n",
      "Epoch 676/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0661 - accuracy: 0.9997 - val_loss: 122.0086 - val_accuracy: 0.7044\n",
      "Epoch 677/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0542 - accuracy: 0.9997 - val_loss: 125.6432 - val_accuracy: 0.6964\n",
      "Epoch 678/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0505 - accuracy: 0.9998 - val_loss: 123.8364 - val_accuracy: 0.7017\n",
      "Epoch 679/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.0493 - accuracy: 0.9998 - val_loss: 122.4280 - val_accuracy: 0.7070\n",
      "Epoch 680/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0368 - accuracy: 0.9998 - val_loss: 125.3383 - val_accuracy: 0.6992\n",
      "Epoch 681/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.0414 - accuracy: 0.9998 - val_loss: 124.4305 - val_accuracy: 0.7037\n",
      "Epoch 682/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.0381 - accuracy: 0.9998 - val_loss: 122.9239 - val_accuracy: 0.7063\n",
      "Epoch 683/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0420 - accuracy: 0.9998 - val_loss: 124.0978 - val_accuracy: 0.7044\n",
      "Epoch 684/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0343 - accuracy: 0.9998 - val_loss: 124.5219 - val_accuracy: 0.7050\n",
      "Epoch 685/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0438 - accuracy: 0.9998 - val_loss: 122.8762 - val_accuracy: 0.7086\n",
      "Epoch 686/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0415 - accuracy: 0.9998 - val_loss: 125.6184 - val_accuracy: 0.7019\n",
      "Epoch 687/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.0525 - accuracy: 0.9998 - val_loss: 122.7599 - val_accuracy: 0.7101\n",
      "Epoch 688/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.0488 - accuracy: 0.9998 - val_loss: 125.8403 - val_accuracy: 0.7037\n",
      "Epoch 690/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.0482 - accuracy: 0.9998 - val_loss: 128.0921 - val_accuracy: 0.6989\n",
      "Epoch 691/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.0488 - accuracy: 0.9998 - val_loss: 128.3234 - val_accuracy: 0.7004\n",
      "Epoch 692/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0564 - accuracy: 0.9998 - val_loss: 127.3897 - val_accuracy: 0.7021\n",
      "Epoch 693/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0503 - accuracy: 0.9998 - val_loss: 127.7005 - val_accuracy: 0.7036\n",
      "Epoch 694/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0522 - accuracy: 0.9998 - val_loss: 125.8089 - val_accuracy: 0.7087\n",
      "Epoch 695/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.0530 - accuracy: 0.9998 - val_loss: 130.1718 - val_accuracy: 0.6985\n",
      "Epoch 696/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0555 - accuracy: 0.9998 - val_loss: 128.4070 - val_accuracy: 0.7035\n",
      "Epoch 697/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0440 - accuracy: 0.9998 - val_loss: 128.5251 - val_accuracy: 0.7037\n",
      "Epoch 698/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0436 - accuracy: 0.9998 - val_loss: 128.0327 - val_accuracy: 0.7068\n",
      "Epoch 699/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0469 - accuracy: 0.9998 - val_loss: 132.4334 - val_accuracy: 0.6980\n",
      "Epoch 700/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0398 - accuracy: 0.9998 - val_loss: 131.1098 - val_accuracy: 0.7029\n",
      "Epoch 701/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.0538 - accuracy: 0.9998 - val_loss: 133.0518 - val_accuracy: 0.7008\n",
      "Epoch 702/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0475 - accuracy: 0.9998 - val_loss: 132.1250 - val_accuracy: 0.7054\n",
      "Epoch 703/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0496 - accuracy: 0.9998 - val_loss: 134.1370 - val_accuracy: 0.6994\n",
      "Epoch 704/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0648 - accuracy: 0.9997 - val_loss: 137.5545 - val_accuracy: 0.7006\n",
      "Epoch 705/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0533 - accuracy: 0.9998 - val_loss: 135.2160 - val_accuracy: 0.7037\n",
      "Epoch 706/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0440 - accuracy: 0.9998 - val_loss: 133.5903 - val_accuracy: 0.7084\n",
      "Epoch 707/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.0444 - accuracy: 0.9998 - val_loss: 138.1249 - val_accuracy: 0.6981\n",
      "Epoch 708/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.0555 - accuracy: 0.9998 - val_loss: 135.4671 - val_accuracy: 0.7028\n",
      "Epoch 709/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0493 - accuracy: 0.9998 - val_loss: 130.9668 - val_accuracy: 0.7133\n",
      "Epoch 710/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0529 - accuracy: 0.9998 - val_loss: 134.0502 - val_accuracy: 0.7059\n",
      "Epoch 711/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.0374 - accuracy: 0.9998 - val_loss: 137.2319 - val_accuracy: 0.6978\n",
      "Epoch 720/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0457 - accuracy: 0.9998 - val_loss: 136.5055 - val_accuracy: 0.7022\n",
      "Epoch 721/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.0507 - accuracy: 0.9998 - val_loss: 136.4594 - val_accuracy: 0.7037\n",
      "Epoch 722/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.0486 - accuracy: 0.9998 - val_loss: 139.1342 - val_accuracy: 0.6969\n",
      "Epoch 723/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.0448 - accuracy: 0.9998 - val_loss: 136.1195 - val_accuracy: 0.7046\n",
      "Epoch 724/1000\n",
      "74/74 [==============================] - 93s 1s/step - loss: 0.0471 - accuracy: 0.9998 - val_loss: 137.2969 - val_accuracy: 0.7000\n",
      "Epoch 725/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.0321 - accuracy: 0.9999 - val_loss: 135.0799 - val_accuracy: 0.7063\n",
      "Epoch 726/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.0434 - accuracy: 0.9998 - val_loss: 136.2699 - val_accuracy: 0.7032\n",
      "Epoch 727/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.0550 - accuracy: 0.9998 - val_loss: 136.2182 - val_accuracy: 0.7033\n",
      "Epoch 728/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0506 - accuracy: 0.9998 - val_loss: 139.2580 - val_accuracy: 0.6978\n",
      "Epoch 729/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.0472 - accuracy: 0.9998 - val_loss: 136.4218 - val_accuracy: 0.7037\n",
      "Epoch 730/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0459 - accuracy: 0.9998 - val_loss: 136.5074 - val_accuracy: 0.7022\n",
      "Epoch 731/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0508 - accuracy: 0.9998 - val_loss: 137.0232 - val_accuracy: 0.7036\n",
      "Epoch 732/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0620 - accuracy: 0.9998 - val_loss: 138.1680 - val_accuracy: 0.6995\n",
      "Epoch 733/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0559 - accuracy: 0.9998 - val_loss: 137.7201 - val_accuracy: 0.7016\n",
      "Epoch 734/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.0441 - accuracy: 0.9998 - val_loss: 134.4519 - val_accuracy: 0.7073\n",
      "Epoch 735/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0395 - accuracy: 0.9998 - val_loss: 138.4842 - val_accuracy: 0.6993\n",
      "Epoch 736/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0444 - accuracy: 0.9998 - val_loss: 140.0482 - val_accuracy: 0.6969\n",
      "Epoch 737/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0370 - accuracy: 0.9998 - val_loss: 136.9173 - val_accuracy: 0.7037\n",
      "Epoch 738/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.0426 - accuracy: 0.9998 - val_loss: 137.2814 - val_accuracy: 0.7031\n",
      "Epoch 739/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0419 - accuracy: 0.9998 - val_loss: 136.7686 - val_accuracy: 0.7043\n",
      "Epoch 740/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0438 - accuracy: 0.9998 - val_loss: 138.7553 - val_accuracy: 0.7002\n",
      "Epoch 741/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0404 - accuracy: 0.9998 - val_loss: 138.8771 - val_accuracy: 0.7001\n",
      "Epoch 742/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0477 - accuracy: 0.9997 - val_loss: 138.9293 - val_accuracy: 0.7021\n",
      "Epoch 743/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.0521 - accuracy: 0.9998 - val_loss: 139.5550 - val_accuracy: 0.7005\n",
      "Epoch 744/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0574 - accuracy: 0.9998 - val_loss: 135.5192 - val_accuracy: 0.7096\n",
      "Epoch 745/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.0502 - accuracy: 0.9998 - val_loss: 138.2546 - val_accuracy: 0.7037\n",
      "Epoch 746/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0636 - accuracy: 0.9998 - val_loss: 136.9118 - val_accuracy: 0.7061\n",
      "Epoch 747/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0488 - accuracy: 0.9998 - val_loss: 137.3300 - val_accuracy: 0.7062\n",
      "Epoch 748/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0480 - accuracy: 0.9998 - val_loss: 139.3246 - val_accuracy: 0.7008\n",
      "Epoch 749/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0509 - accuracy: 0.9998 - val_loss: 137.3296 - val_accuracy: 0.7077\n",
      "Epoch 750/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0533 - accuracy: 0.9998 - val_loss: 139.9913 - val_accuracy: 0.7017\n",
      "Epoch 751/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.0532 - accuracy: 0.9998 - val_loss: 140.7916 - val_accuracy: 0.6990\n",
      "Epoch 752/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.0582 - accuracy: 0.9997 - val_loss: 138.4092 - val_accuracy: 0.7054\n",
      "Epoch 753/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.0565 - accuracy: 0.9998 - val_loss: 139.7250 - val_accuracy: 0.7037\n",
      "Epoch 754/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.0437 - accuracy: 0.9998 - val_loss: 138.0905 - val_accuracy: 0.7086\n",
      "Epoch 755/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.0510 - accuracy: 0.9998 - val_loss: 140.4996 - val_accuracy: 0.7019\n",
      "Epoch 756/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.0574 - accuracy: 0.9998 - val_loss: 140.3135 - val_accuracy: 0.7025\n",
      "Epoch 757/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.0454 - accuracy: 0.9998 - val_loss: 139.6538 - val_accuracy: 0.7046\n",
      "Epoch 758/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0447 - accuracy: 0.9998 - val_loss: 140.1292 - val_accuracy: 0.7033\n",
      "Epoch 759/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.0497 - accuracy: 0.9998 - val_loss: 139.8832 - val_accuracy: 0.7065\n",
      "Epoch 760/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0485 - accuracy: 0.9998 - val_loss: 141.0332 - val_accuracy: 0.7037\n",
      "Epoch 761/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.0389 - accuracy: 0.9998 - val_loss: 140.4683 - val_accuracy: 0.7037\n",
      "Epoch 762/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.0528 - accuracy: 0.9997 - val_loss: 144.3025 - val_accuracy: 0.6943\n",
      "Epoch 763/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.0413 - accuracy: 0.9998 - val_loss: 144.6408 - val_accuracy: 0.6954\n",
      "Epoch 764/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0385 - accuracy: 0.9998 - val_loss: 138.2353 - val_accuracy: 0.7109\n",
      "Epoch 765/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.0401 - accuracy: 0.9998 - val_loss: 141.1111 - val_accuracy: 0.7064\n",
      "Epoch 767/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.0412 - accuracy: 0.9998 - val_loss: 140.0158 - val_accuracy: 0.7114\n",
      "Epoch 768/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.0447 - accuracy: 0.9998 - val_loss: 143.2213 - val_accuracy: 0.7035\n",
      "Epoch 769/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.0434 - accuracy: 0.9998 - val_loss: 142.8223 - val_accuracy: 0.7037\n",
      "Epoch 770/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0322 - accuracy: 0.9999 - val_loss: 143.9737 - val_accuracy: 0.7035\n",
      "Epoch 771/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.0366 - accuracy: 0.9998 - val_loss: 144.2368 - val_accuracy: 0.7035\n",
      "Epoch 772/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0392 - accuracy: 0.9998 - val_loss: 143.2105 - val_accuracy: 0.7048\n",
      "Epoch 773/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0364 - accuracy: 0.9998 - val_loss: 146.2736 - val_accuracy: 0.7011\n",
      "Epoch 774/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.0408 - accuracy: 0.9998 - val_loss: 145.4879 - val_accuracy: 0.7046\n",
      "Epoch 775/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0492 - accuracy: 0.9998 - val_loss: 146.3594 - val_accuracy: 0.7027\n",
      "Epoch 776/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0498 - accuracy: 0.9998 - val_loss: 144.1812 - val_accuracy: 0.7077\n",
      "Epoch 777/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0398 - accuracy: 0.9998 - val_loss: 146.3804 - val_accuracy: 0.7037\n",
      "Epoch 778/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0586 - accuracy: 0.9997 - val_loss: 150.0984 - val_accuracy: 0.6973\n",
      "Epoch 779/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0486 - accuracy: 0.9998 - val_loss: 144.4525 - val_accuracy: 0.7096\n",
      "Epoch 780/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.0495 - accuracy: 0.9998 - val_loss: 146.8136 - val_accuracy: 0.7050\n",
      "Epoch 781/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.0481 - accuracy: 0.9998 - val_loss: 147.0586 - val_accuracy: 0.7048\n",
      "Epoch 782/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.0334 - accuracy: 0.9999 - val_loss: 147.7005 - val_accuracy: 0.7039\n",
      "Epoch 783/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.0293 - accuracy: 0.9998 - val_loss: 148.6149 - val_accuracy: 0.7020\n",
      "Epoch 784/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0375 - accuracy: 0.9998 - val_loss: 150.4530 - val_accuracy: 0.7004\n",
      "Epoch 785/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0392 - accuracy: 0.9998 - val_loss: 149.9834 - val_accuracy: 0.7037\n",
      "Epoch 786/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0322 - accuracy: 0.9998 - val_loss: 146.4197 - val_accuracy: 0.7105\n",
      "Epoch 787/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.0368 - accuracy: 0.9998 - val_loss: 150.8195 - val_accuracy: 0.7026\n",
      "Epoch 788/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.0315 - accuracy: 0.9999 - val_loss: 150.3266 - val_accuracy: 0.7057\n",
      "Epoch 789/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0406 - accuracy: 0.9998 - val_loss: 151.8387 - val_accuracy: 0.7032\n",
      "Epoch 790/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0403 - accuracy: 0.9998 - val_loss: 150.7637 - val_accuracy: 0.7049\n",
      "Epoch 791/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0467 - accuracy: 0.9998 - val_loss: 168.0471 - val_accuracy: 0.7062\n",
      "Epoch 823/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0670 - accuracy: 0.9997 - val_loss: 171.1546 - val_accuracy: 0.7060\n",
      "Epoch 824/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0552 - accuracy: 0.9997 - val_loss: 172.8457 - val_accuracy: 0.7016\n",
      "Epoch 825/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.0421 - accuracy: 0.9998 - val_loss: 172.5606 - val_accuracy: 0.7037\n",
      "Epoch 826/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0487 - accuracy: 0.9998 - val_loss: 172.3703 - val_accuracy: 0.7080\n",
      "Epoch 827/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.0462 - accuracy: 0.9998 - val_loss: 174.9692 - val_accuracy: 0.7029\n",
      "Epoch 828/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0634 - accuracy: 0.9998 - val_loss: 172.9369 - val_accuracy: 0.7067\n",
      "Epoch 829/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.0756 - accuracy: 0.9997 - val_loss: 171.1129 - val_accuracy: 0.7088\n",
      "Epoch 830/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0449 - accuracy: 0.9998 - val_loss: 178.6887 - val_accuracy: 0.6985\n",
      "Epoch 831/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0464 - accuracy: 0.9998 - val_loss: 176.7415 - val_accuracy: 0.6985\n",
      "Epoch 832/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.0395 - accuracy: 0.9998 - val_loss: 174.1341 - val_accuracy: 0.7044\n",
      "Epoch 833/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.0438 - accuracy: 0.9998 - val_loss: 173.9975 - val_accuracy: 0.7037\n",
      "Epoch 834/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.0391 - accuracy: 0.9998 - val_loss: 170.9896 - val_accuracy: 0.7089\n",
      "Epoch 835/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.0324 - accuracy: 0.9999 - val_loss: 173.5155 - val_accuracy: 0.7038\n",
      "Epoch 836/1000\n",
      "74/74 [==============================] - 93s 1s/step - loss: 0.0376 - accuracy: 0.9999 - val_loss: 174.7429 - val_accuracy: 0.7023\n",
      "Epoch 837/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.0349 - accuracy: 0.9998 - val_loss: 175.0755 - val_accuracy: 0.7027\n",
      "Epoch 838/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.0520 - accuracy: 0.9998 - val_loss: 174.1900 - val_accuracy: 0.7035\n",
      "Epoch 839/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.0388 - accuracy: 0.9998 - val_loss: 169.1460 - val_accuracy: 0.7114\n",
      "Epoch 840/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0290 - accuracy: 0.9998 - val_loss: 171.9990 - val_accuracy: 0.7072\n",
      "Epoch 841/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0518 - accuracy: 0.9998 - val_loss: 174.8476 - val_accuracy: 0.7037\n",
      "Epoch 842/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0469 - accuracy: 0.9998 - val_loss: 171.2867 - val_accuracy: 0.7098\n",
      "Epoch 843/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.0384 - accuracy: 0.9998 - val_loss: 177.9018 - val_accuracy: 0.6983\n",
      "Epoch 844/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.0386 - accuracy: 0.9998 - val_loss: 172.6640 - val_accuracy: 0.7046\n",
      "Epoch 845/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.0418 - accuracy: 0.9998 - val_loss: 172.1256 - val_accuracy: 0.7079\n",
      "Epoch 846/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0350 - accuracy: 0.9998 - val_loss: 173.3755 - val_accuracy: 0.7061\n",
      "Epoch 847/1000\n",
      "74/74 [==============================] - 93s 1s/step - loss: 0.0437 - accuracy: 0.9998 - val_loss: 178.9946 - val_accuracy: 0.6953\n",
      "Epoch 848/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.0353 - accuracy: 0.9998 - val_loss: 175.2815 - val_accuracy: 0.7033\n",
      "Epoch 849/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0380 - accuracy: 0.9998 - val_loss: 174.3600 - val_accuracy: 0.7037\n",
      "Epoch 850/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.0412 - accuracy: 0.9998 - val_loss: 177.5387 - val_accuracy: 0.6990\n",
      "Epoch 851/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0484 - accuracy: 0.9998 - val_loss: 174.4037 - val_accuracy: 0.7044\n",
      "Epoch 852/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0340 - accuracy: 0.9998 - val_loss: 176.5990 - val_accuracy: 0.7001\n",
      "Epoch 853/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0446 - accuracy: 0.9998 - val_loss: 174.4558 - val_accuracy: 0.7039\n",
      "Epoch 854/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.0418 - accuracy: 0.9998 - val_loss: 174.7255 - val_accuracy: 0.7032\n",
      "Epoch 855/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0351 - accuracy: 0.9998 - val_loss: 179.5550 - val_accuracy: 0.6985\n",
      "Epoch 856/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0342 - accuracy: 0.9998 - val_loss: 176.6891 - val_accuracy: 0.7035\n",
      "Epoch 857/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0398 - accuracy: 0.9998 - val_loss: 177.0276 - val_accuracy: 0.7037\n",
      "Epoch 858/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0356 - accuracy: 0.9998 - val_loss: 179.5331 - val_accuracy: 0.6989\n",
      "Epoch 859/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0312 - accuracy: 0.9998 - val_loss: 179.6717 - val_accuracy: 0.7004\n",
      "Epoch 860/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.0366 - accuracy: 0.9999 - val_loss: 179.5785 - val_accuracy: 0.6979\n",
      "Epoch 861/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.0230 - accuracy: 0.9999 - val_loss: 176.3192 - val_accuracy: 0.7057\n",
      "Epoch 862/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.0412 - accuracy: 0.9998 - val_loss: 175.3271 - val_accuracy: 0.7057\n",
      "Epoch 863/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.0325 - accuracy: 0.9999 - val_loss: 177.9356 - val_accuracy: 0.7022\n",
      "Epoch 864/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.0455 - accuracy: 0.9998 - val_loss: 176.8846 - val_accuracy: 0.7042\n",
      "Epoch 865/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.0419 - accuracy: 0.9998 - val_loss: 176.9606 - val_accuracy: 0.7037\n",
      "Epoch 866/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0410 - accuracy: 0.9998 - val_loss: 178.3404 - val_accuracy: 0.7007\n",
      "Epoch 867/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.0412 - accuracy: 0.9998 - val_loss: 180.4275 - val_accuracy: 0.6993\n",
      "Epoch 868/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.0378 - accuracy: 0.9998 - val_loss: 177.3047 - val_accuracy: 0.7050\n",
      "Epoch 869/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0527 - accuracy: 0.9998 - val_loss: 175.4968 - val_accuracy: 0.7087\n",
      "Epoch 870/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0495 - accuracy: 0.9998 - val_loss: 178.0643 - val_accuracy: 0.7034\n",
      "Epoch 871/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0360 - accuracy: 0.9998 - val_loss: 180.7711 - val_accuracy: 0.6985\n",
      "Epoch 872/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.0509 - accuracy: 0.9998 - val_loss: 175.1003 - val_accuracy: 0.7094\n",
      "Epoch 873/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.0418 - accuracy: 0.9998 - val_loss: 178.9982 - val_accuracy: 0.7037\n",
      "Epoch 874/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.0372 - accuracy: 0.9998 - val_loss: 177.4421 - val_accuracy: 0.7047\n",
      "Epoch 875/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0438 - accuracy: 0.9998 - val_loss: 178.2415 - val_accuracy: 0.7037\n",
      "Epoch 876/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0408 - accuracy: 0.9998 - val_loss: 179.6890 - val_accuracy: 0.7015\n",
      "Epoch 877/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0408 - accuracy: 0.9998 - val_loss: 176.0593 - val_accuracy: 0.7092\n",
      "Epoch 878/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0584 - accuracy: 0.9998 - val_loss: 180.8818 - val_accuracy: 0.7018\n",
      "Epoch 879/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.0547 - accuracy: 0.9998 - val_loss: 183.2522 - val_accuracy: 0.6980\n",
      "Epoch 880/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.0569 - accuracy: 0.9997 - val_loss: 181.4516 - val_accuracy: 0.6998\n",
      "Epoch 881/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.0602 - accuracy: 0.9997 - val_loss: 180.6483 - val_accuracy: 0.7037\n",
      "Epoch 882/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0491 - accuracy: 0.9998 - val_loss: 178.6966 - val_accuracy: 0.7060\n",
      "Epoch 883/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0507 - accuracy: 0.9998 - val_loss: 179.5166 - val_accuracy: 0.7059\n",
      "Epoch 884/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.0658 - accuracy: 0.9998 - val_loss: 180.4965 - val_accuracy: 0.7035\n",
      "Epoch 885/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.0607 - accuracy: 0.9998 - val_loss: 180.9981 - val_accuracy: 0.7037\n",
      "Epoch 886/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0431 - accuracy: 0.9998 - val_loss: 179.1055 - val_accuracy: 0.7066\n",
      "Epoch 887/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.0421 - accuracy: 0.9998 - val_loss: 181.6381 - val_accuracy: 0.7013\n",
      "Epoch 888/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0533 - accuracy: 0.9998 - val_loss: 183.3203 - val_accuracy: 0.7021\n",
      "Epoch 889/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0403 - accuracy: 0.9998 - val_loss: 182.3171 - val_accuracy: 0.7037\n",
      "Epoch 890/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0483 - accuracy: 0.9998 - val_loss: 182.2790 - val_accuracy: 0.7036\n",
      "Epoch 891/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0301 - accuracy: 0.9998 - val_loss: 180.4816 - val_accuracy: 0.7078\n",
      "Epoch 892/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.0317 - accuracy: 0.9998 - val_loss: 183.3146 - val_accuracy: 0.7043\n",
      "Epoch 893/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.0336 - accuracy: 0.9998 - val_loss: 181.4999 - val_accuracy: 0.7087\n",
      "Epoch 894/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.0349 - accuracy: 0.9999 - val_loss: 183.7051 - val_accuracy: 0.7046\n",
      "Epoch 895/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0419 - accuracy: 0.9998 - val_loss: 184.4114 - val_accuracy: 0.7052\n",
      "Epoch 896/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0366 - accuracy: 0.9999 - val_loss: 183.8696 - val_accuracy: 0.7035\n",
      "Epoch 897/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0399 - accuracy: 0.9998 - val_loss: 185.2898 - val_accuracy: 0.7037\n",
      "Epoch 898/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0555 - accuracy: 0.9998 - val_loss: 185.9341 - val_accuracy: 0.7025\n",
      "Epoch 899/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0538 - accuracy: 0.9997 - val_loss: 183.8323 - val_accuracy: 0.7071\n",
      "Epoch 900/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.0380 - accuracy: 0.9998 - val_loss: 186.9043 - val_accuracy: 0.7017\n",
      "Epoch 901/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.0548 - accuracy: 0.9998 - val_loss: 189.6643 - val_accuracy: 0.7005\n",
      "Epoch 902/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0350 - accuracy: 0.9998 - val_loss: 188.0359 - val_accuracy: 0.7048\n",
      "Epoch 903/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0406 - accuracy: 0.9999 - val_loss: 188.2383 - val_accuracy: 0.7044\n",
      "Epoch 904/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.0406 - accuracy: 0.9998 - val_loss: 188.8551 - val_accuracy: 0.7025\n",
      "Epoch 905/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0427 - accuracy: 0.9998 - val_loss: 188.5080 - val_accuracy: 0.7037\n",
      "Epoch 906/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.0437 - accuracy: 0.9998 - val_loss: 188.5484 - val_accuracy: 0.7052\n",
      "Epoch 907/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.0489 - accuracy: 0.9998 - val_loss: 189.4618 - val_accuracy: 0.7055\n",
      "Epoch 908/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0395 - accuracy: 0.9998 - val_loss: 189.2889 - val_accuracy: 0.7050\n",
      "Epoch 909/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0366 - accuracy: 0.9998 - val_loss: 193.2451 - val_accuracy: 0.7010\n",
      "Epoch 910/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0569 - accuracy: 0.9997 - val_loss: 191.2445 - val_accuracy: 0.7037\n",
      "Epoch 911/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0511 - accuracy: 0.9998 - val_loss: 187.6069 - val_accuracy: 0.7100\n",
      "Epoch 912/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0549 - accuracy: 0.9998 - val_loss: 192.2600 - val_accuracy: 0.7026\n",
      "Epoch 913/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0500 - accuracy: 0.9998 - val_loss: 191.3605 - val_accuracy: 0.7037\n",
      "Epoch 914/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0545 - accuracy: 0.9998 - val_loss: 191.8411 - val_accuracy: 0.7044\n",
      "Epoch 915/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0524 - accuracy: 0.9998 - val_loss: 192.5812 - val_accuracy: 0.7042\n",
      "Epoch 916/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0461 - accuracy: 0.9998 - val_loss: 191.2881 - val_accuracy: 0.7074\n",
      "Epoch 917/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0502 - accuracy: 0.9998 - val_loss: 196.1768 - val_accuracy: 0.6999\n",
      "Epoch 918/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0470 - accuracy: 0.9998 - val_loss: 193.3785 - val_accuracy: 0.7049\n",
      "Epoch 919/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0501 - accuracy: 0.9998 - val_loss: 196.3125 - val_accuracy: 0.7018\n",
      "Epoch 920/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0348 - accuracy: 0.9999 - val_loss: 196.3043 - val_accuracy: 0.7018\n",
      "Epoch 921/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.0549 - accuracy: 0.9997 - val_loss: 196.3587 - val_accuracy: 0.7037\n",
      "Epoch 922/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0533 - accuracy: 0.9998 - val_loss: 197.6931 - val_accuracy: 0.7051\n",
      "Epoch 923/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0415 - accuracy: 0.9998 - val_loss: 198.5291 - val_accuracy: 0.7050\n",
      "Epoch 924/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0355 - accuracy: 0.9998 - val_loss: 198.0880 - val_accuracy: 0.7061\n",
      "Epoch 925/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.0317 - accuracy: 0.9998 - val_loss: 204.3921 - val_accuracy: 0.6970\n",
      "Epoch 926/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.0378 - accuracy: 0.9998 - val_loss: 201.0925 - val_accuracy: 0.7011\n",
      "Epoch 927/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.0420 - accuracy: 0.9998 - val_loss: 200.6725 - val_accuracy: 0.7020\n",
      "Epoch 928/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0361 - accuracy: 0.9998 - val_loss: 203.8860 - val_accuracy: 0.6971\n",
      "Epoch 929/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0356 - accuracy: 0.9999 - val_loss: 200.8502 - val_accuracy: 0.7037\n",
      "Epoch 930/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0417 - accuracy: 0.9998 - val_loss: 199.6255 - val_accuracy: 0.7052\n",
      "Epoch 931/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.0379 - accuracy: 0.9998 - val_loss: 199.7528 - val_accuracy: 0.7050\n",
      "Epoch 932/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.0402 - accuracy: 0.9998 - val_loss: 198.7217 - val_accuracy: 0.7073\n",
      "Epoch 933/1000\n",
      "74/74 [==============================] - 93s 1s/step - loss: 0.0452 - accuracy: 0.9998 - val_loss: 203.6018 - val_accuracy: 0.6992\n",
      "Epoch 934/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.0371 - accuracy: 0.9998 - val_loss: 201.6295 - val_accuracy: 0.7036\n",
      "Epoch 935/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0356 - accuracy: 0.9998 - val_loss: 203.3112 - val_accuracy: 0.7037\n",
      "Epoch 936/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0498 - accuracy: 0.9998 - val_loss: 199.7766 - val_accuracy: 0.7082\n",
      "Epoch 937/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0485 - accuracy: 0.9998 - val_loss: 202.2068 - val_accuracy: 0.7037\n",
      "Epoch 938/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0359 - accuracy: 0.9998 - val_loss: 201.1073 - val_accuracy: 0.7068\n",
      "Epoch 939/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.0668 - accuracy: 0.9997 - val_loss: 204.1051 - val_accuracy: 0.7028\n",
      "Epoch 940/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.1008 - accuracy: 0.9996 - val_loss: 204.3155 - val_accuracy: 0.7033\n",
      "Epoch 941/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.0511 - accuracy: 0.9998 - val_loss: 208.1607 - val_accuracy: 0.6989\n",
      "Epoch 942/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0453 - accuracy: 0.9998 - val_loss: 205.8527 - val_accuracy: 0.7018\n",
      "Epoch 943/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0458 - accuracy: 0.9998 - val_loss: 203.9111 - val_accuracy: 0.7052\n",
      "Epoch 944/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.0337 - accuracy: 0.9998 - val_loss: 203.9073 - val_accuracy: 0.7063\n",
      "Epoch 945/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0418 - accuracy: 0.9998 - val_loss: 203.9919 - val_accuracy: 0.7037\n",
      "Epoch 946/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0375 - accuracy: 0.9999 - val_loss: 206.0228 - val_accuracy: 0.7009\n",
      "Epoch 947/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0459 - accuracy: 0.9998 - val_loss: 202.1127 - val_accuracy: 0.7055\n",
      "Epoch 948/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0360 - accuracy: 0.9998 - val_loss: 203.8166 - val_accuracy: 0.7044\n",
      "Epoch 949/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0410 - accuracy: 0.9998 - val_loss: 201.7418 - val_accuracy: 0.7074\n",
      "Epoch 950/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0397 - accuracy: 0.9998 - val_loss: 205.0983 - val_accuracy: 0.7031\n",
      "Epoch 951/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.0401 - accuracy: 0.9998 - val_loss: 209.9283 - val_accuracy: 0.6962\n",
      "Epoch 952/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0426 - accuracy: 0.9998 - val_loss: 207.0034 - val_accuracy: 0.7035\n",
      "Epoch 953/1000\n",
      "74/74 [==============================] - 93s 1s/step - loss: 0.0357 - accuracy: 0.9999 - val_loss: 209.5666 - val_accuracy: 0.7037\n",
      "Epoch 954/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0343 - accuracy: 0.9998 - val_loss: 205.7362 - val_accuracy: 0.7099\n",
      "Epoch 955/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0359 - accuracy: 0.9999 - val_loss: 213.7487 - val_accuracy: 0.7001\n",
      "Epoch 956/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.0354 - accuracy: 0.9998 - val_loss: 207.5711 - val_accuracy: 0.7079\n",
      "Epoch 957/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0431 - accuracy: 0.9998 - val_loss: 215.3515 - val_accuracy: 0.6965\n",
      "Epoch 958/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0370 - accuracy: 0.9998 - val_loss: 210.2473 - val_accuracy: 0.7045\n",
      "Epoch 959/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0308 - accuracy: 0.9998 - val_loss: 208.2816 - val_accuracy: 0.7074\n",
      "Epoch 960/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0324 - accuracy: 0.9998 - val_loss: 207.0821 - val_accuracy: 0.7109\n",
      "Epoch 961/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0389 - accuracy: 0.9998 - val_loss: 211.9090 - val_accuracy: 0.7037\n",
      "Epoch 962/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0291 - accuracy: 0.9998 - val_loss: 210.8613 - val_accuracy: 0.7046\n",
      "Epoch 963/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.0379 - accuracy: 0.9999 - val_loss: 213.6812 - val_accuracy: 0.7015\n",
      "Epoch 964/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.0399 - accuracy: 0.9998 - val_loss: 213.0199 - val_accuracy: 0.6998\n",
      "Epoch 965/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0419 - accuracy: 0.9998 - val_loss: 205.3237 - val_accuracy: 0.7110\n",
      "Epoch 966/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.0362 - accuracy: 0.9999 - val_loss: 210.9579 - val_accuracy: 0.7040\n",
      "Epoch 967/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0389 - accuracy: 0.9998 - val_loss: 211.2359 - val_accuracy: 0.7026\n",
      "Epoch 968/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0458 - accuracy: 0.9998 - val_loss: 213.4247 - val_accuracy: 0.7015\n",
      "Epoch 969/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0327 - accuracy: 0.9998 - val_loss: 210.8824 - val_accuracy: 0.7037\n",
      "Epoch 970/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0359 - accuracy: 0.9998 - val_loss: 209.8559 - val_accuracy: 0.7063\n",
      "Epoch 971/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0376 - accuracy: 0.9998 - val_loss: 211.4847 - val_accuracy: 0.7031\n",
      "Epoch 972/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0407 - accuracy: 0.9999 - val_loss: 214.5616 - val_accuracy: 0.7013\n",
      "Epoch 973/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.0279 - accuracy: 0.9998 - val_loss: 209.6731 - val_accuracy: 0.7060\n",
      "Epoch 974/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.0391 - accuracy: 0.9998 - val_loss: 208.9507 - val_accuracy: 0.7087\n",
      "Epoch 976/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.0432 - accuracy: 0.9998 - val_loss: 208.8495 - val_accuracy: 0.7088\n",
      "Epoch 977/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0365 - accuracy: 0.9998 - val_loss: 212.0375 - val_accuracy: 0.7029\n",
      "Epoch 979/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.0319 - accuracy: 0.9998 - val_loss: 214.7650 - val_accuracy: 0.7002\n",
      "Epoch 980/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.0388 - accuracy: 0.9998 - val_loss: 210.9812 - val_accuracy: 0.7070\n",
      "Epoch 981/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.0432 - accuracy: 0.9998 - val_loss: 217.1231 - val_accuracy: 0.6980\n",
      "Epoch 982/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.0373 - accuracy: 0.9998 - val_loss: 214.7614 - val_accuracy: 0.7024\n",
      "Epoch 983/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.0314 - accuracy: 0.9998 - val_loss: 213.6924 - val_accuracy: 0.7036\n",
      "Epoch 984/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.0389 - accuracy: 0.9998 - val_loss: 215.9875 - val_accuracy: 0.7017\n",
      "Epoch 985/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.0364 - accuracy: 0.9998 - val_loss: 215.6054 - val_accuracy: 0.7037\n",
      "Epoch 986/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.0360 - accuracy: 0.9998 - val_loss: 218.8089 - val_accuracy: 0.7001\n",
      "Epoch 987/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.0308 - accuracy: 0.9998 - val_loss: 214.8768 - val_accuracy: 0.7064\n",
      "Epoch 988/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0403 - accuracy: 0.9998 - val_loss: 215.5292 - val_accuracy: 0.7042\n",
      "Epoch 989/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.0444 - accuracy: 0.9997 - val_loss: 220.1835 - val_accuracy: 0.6994\n",
      "Epoch 990/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.0470 - accuracy: 0.9998 - val_loss: 216.0676 - val_accuracy: 0.7039\n",
      "Epoch 991/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.0423 - accuracy: 0.9998 - val_loss: 218.7883 - val_accuracy: 0.7019\n",
      "Epoch 992/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.0420 - accuracy: 0.9998 - val_loss: 212.1110 - val_accuracy: 0.7097\n",
      "Epoch 993/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.0378 - accuracy: 0.9998 - val_loss: 216.3325 - val_accuracy: 0.7037\n",
      "Epoch 994/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0377 - accuracy: 0.9998 - val_loss: 214.4084 - val_accuracy: 0.7088\n",
      "Epoch 995/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.0348 - accuracy: 0.9998 - val_loss: 218.4127 - val_accuracy: 0.7018\n",
      "Epoch 996/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0358 - accuracy: 0.9999 - val_loss: 218.7763 - val_accuracy: 0.7031\n",
      "Epoch 997/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.0374 - accuracy: 0.9998 - val_loss: 213.7272 - val_accuracy: 0.7110\n",
      "Epoch 998/1000\n",
      "74/74 [==============================] - 92s 1s/step - loss: 0.0363 - accuracy: 0.9998 - val_loss: 216.2175 - val_accuracy: 0.7067\n",
      "Epoch 999/1000\n",
      "74/74 [==============================] - 90s 1s/step - loss: 0.0285 - accuracy: 0.9999 - val_loss: 220.8926 - val_accuracy: 0.7006\n",
      "Epoch 1000/1000\n",
      "74/74 [==============================] - 91s 1s/step - loss: 0.0330 - accuracy: 0.9998 - val_loss: 217.9049 - val_accuracy: 0.7040\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fa5ece03588>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_x,\n",
    "          train_y,\n",
    "          dev_x,\n",
    "          dev_y,\n",
    "          epochs=1000,\n",
    "          batch_size=64,\n",
    "          callbacks=[history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "uuid": "55e76949-2932-40dd-b704-b17bb7db6cdf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           precision    recall  f1-score   support\n",
      "\n",
      "   Things     0.9432    0.9648    0.9539      1223\n",
      "   Person     0.9822    0.9865    0.9843      2071\n",
      "     Time     0.9792    0.9814    0.9803       862\n",
      "    Steal     0.9583    0.9661    0.9622       738\n",
      "    Money     0.9724    0.9749    0.9737       796\n",
      " Location     0.8735    0.8870    0.8802       903\n",
      "    Count     0.9933    0.9803    0.9868       761\n",
      "  Consume     0.9412    0.9756    0.9581        82\n",
      "     Draw     0.9746    0.9777    0.9762       314\n",
      "Volunteer     0.9362    0.9167    0.9263        48\n",
      "     Sale     0.9028    0.9559    0.9286        68\n",
      "\n",
      "micro avg     0.9592    0.9663    0.9628      7866\n",
      "macro avg     0.9595    0.9663    0.9629      7866\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'           precision    recall  f1-score   support\\n\\n   Things     0.9432    0.9648    0.9539      1223\\n   Person     0.9822    0.9865    0.9843      2071\\n     Time     0.9792    0.9814    0.9803       862\\n    Steal     0.9583    0.9661    0.9622       738\\n    Money     0.9724    0.9749    0.9737       796\\n Location     0.8735    0.8870    0.8802       903\\n    Count     0.9933    0.9803    0.9868       761\\n  Consume     0.9412    0.9756    0.9581        82\\n     Draw     0.9746    0.9777    0.9762       314\\nVolunteer     0.9362    0.9167    0.9263        48\\n     Sale     0.9028    0.9559    0.9286        68\\n\\nmicro avg     0.9592    0.9663    0.9628      7866\\nmacro avg     0.9595    0.9663    0.9629      7866\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_x, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "uuid": "c000eae9-4dee-49eb-9efd-13073e297d06"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "history.loss_plot('epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "uuid": "99bf7bc5-ddac-42fa-90f2-1ecbabb726b9"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "iters = range(len(history.losses['epoch'][:150]))\n",
    "plt.figure()\n",
    "# loss\n",
    "plt.plot(iters, history.losses['epoch'][:150], 'b', label='train loss', lw=1, linestyle=':')\n",
    "plt.grid(True)\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.rcParams['savefig.dpi'] = 200\n",
    "plt.savefig('loss.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "uuid": "8fb2d766-abf7-4fa0-90db-9e28f4d4f0ce"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
